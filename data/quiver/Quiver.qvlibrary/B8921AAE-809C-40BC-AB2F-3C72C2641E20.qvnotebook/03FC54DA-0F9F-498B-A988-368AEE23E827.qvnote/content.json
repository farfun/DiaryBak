{"title":"2018-02-21-ch-neural-networks-case-study","cells":[{"type":"markdown","data":"---\ncategories: posts/cs231n-ch\nmath: y\nlayout: page\ntitle: neural-networks-case-study\n---\n\nTable of Contents:\n\n- [Generating some data](#data)\n- [Training a Softmax Linear Classifier](#linear)\n  - [Initialize the parameters](#init)\n  - [Compute the class scores](#scores)\n  - [Compute the loss](#loss)\n  - [Computing the analytic gradient with backpropagation](#grad)\n  - [Performing a parameter update](#update)\n  - [Putting it all together: Training a Softmax Classifier](#together)\n- [Training a Neural Network](#net)\n- [Summary](#summary)\n\nIn this section we'll walk through a complete implementation of a toy Neural Network in 2 dimensions. We'll first implement a simple linear classifier and then extend the code to a 2-layer Neural Network. As we'll see, this extension is surprisingly simple and very few changes are necessary.\n\n<a name='data'></a>\n\n## Generating some data\n\nLets generate a classification dataset that is not easily linearly separable. Our favorite example is the spiral dataset, which can be generated as follows:\n"},{"type":"code","language":"python","data":"N = 100 # number of points per class\nD = 2 # dimensionality\nK = 3 # number of classes\nX = np.zeros((N*K,D)) # data matrix (each row = single example)\ny = np.zeros(N*K, dtype='uint8') # class labels\nfor j in xrange(K):\n  ix = range(N*j,N*(j+1))\n  r = np.linspace(0.0,1,N) # radius\n  t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2 # theta\n  X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n  y[ix] = j\n# lets visualize the data:\nplt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\nplt.show()"},{"type":"markdown","data":"\n<div class=\"fig figcenter fighighlight\">\n  <img src=\"/Diary/assets/images/cs231n/eg/spiral_raw.png\">\n  <div class=\"figcaption\">\n    The toy spiral data consists of three classes (blue, red, yellow) that are not linearly separable.\n  </div>\n</div>\n\nNormally we would want to preprocess the dataset so that each feature has zero mean and unit standard deviation, but in this case the features are already in a nice range from -1 to 1, so we skip this step.\n\n<a name='linear'></a>\n\n## Training a Softmax Linear Classifier\n\n<a name='init'></a>\n\n### Initialize the parameters\n\nLets first train a Softmax classifier on this classification dataset. As we saw in the previous sections, the Softmax classifier has a linear score function and uses the cross-entropy loss. The parameters of the linear classifier consist of a weight matrix `W` and a bias vector `b` for each class. Lets first initialize these parameters to be random numbers:\n"},{"type":"code","language":"python","data":"# initialize parameters randomly\nW = 0.01 * np.random.randn(D,K)\nb = np.zeros((1,K))"},{"type":"markdown","data":"\nRecall that we `D = 2` is the dimensionality and `K = 3` is the number of classes. \n\n<a name='scores'></a>\n\n### Compute the class scores\n\nSince this is a linear classifier, we can compute all class scores very simply in parallel with a single matrix multiplication:\n"},{"type":"code","language":"python","data":"# compute class scores for a linear classifier\nscores = np.dot(X, W) + b"},{"type":"markdown","data":"\nIn this example we have 300 2-D points, so after this multiplication the array `scores` will have size [300 x 3], where each row gives the class scores corresponding to the 3 classes (blue, red, yellow).\n\n<a name='loss'></a>\n\n### Compute the loss\n\nThe second key ingredient we need is a loss function, which is a differentiable objective that quantifies our unhappiness with the computed class scores. Intuitively, we want the correct class to have a higher score than the other classes. When this is the case, the loss should be low and otherwise the loss should be high. There are many ways to quantify this intuition, but in this example lets use the cross-entropy loss that is associated with the Softmax classifier. Recall that if $f$ is the array of class scores for a single example (e.g. array of 3 numbers here), then the Softmax classifier computes the loss for that example as:\n\n$$\nL_i = -\\log\\left(\\frac{e^{f_{y_i}}}{ \\sum_j e^{f_j} }\\right)\n$$\n\nWe can see that the Softmax classifier interprets every element of $f$ as holding the (unnormalized) log probabilities of the three classes. We exponentiate these to get (unnormalized) probabilities, and then normalize them to get probabilites. Therefore, the expression inside the log is the normalized probability of the correct class. Note how this expression works: this quantity is always between 0 and 1. When the probability of the correct class is very small (near 0), the loss will go towards (positive) infinity. Conversely, when the correct class probability goes towards 1, the loss will go towards zero because $log(1) = 0$. Hence, the expression for $L_i$ is low when the correct class probability is high, and it's very high when it is low.\n\nRecall also that the full Softmax classifier loss is then defined as the average cross-entropy loss over the training examples and the regularization:\n\n$$\nL =  \\underbrace{ \\frac{1}{N} \\sum_i L_i }_\\text{data loss} + \\underbrace{ \\frac{1}{2} \\lambda \\sum_k\\sum_l W_{k,l}^2 }_\\text{regularization loss} \\\\\\\\\n$$\n\nGiven the array of `scores` we've computed above, we can compute the loss. First, the way to obtain the probabilities is straight forward:\n"},{"type":"code","language":"python","data":"num_examples = X.shape[0]\n# get unnormalized probabilities\nexp_scores = np.exp(scores)\n# normalize them for each example\nprobs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)"},{"type":"markdown","data":"\nWe now have an array `probs` of size [300 x 3], where each row now contains the class probabilities. In particular, since we've normalized them every row now sums to one. We can now query for the  log probabilities assigned to the correct classes in each example:\n"},{"type":"code","language":"python","data":"corect_logprobs = -np.log(probs[range(num_examples),y])"},{"type":"markdown","data":"\nThe array `correct_logprobs` is a 1D array of just the probabilities assigned to the correct classes for each example. The full loss is then the average of these log probabilities and the regularization loss:\n"},{"type":"code","language":"python","data":"# compute the loss: average cross-entropy loss and regularization\ndata_loss = np.sum(corect_logprobs)/num_examples\nreg_loss = 0.5*reg*np.sum(W*W)\nloss = data_loss + reg_loss"},{"type":"markdown","data":"\nIn this code, the regularization strength $\\lambda$ is stored inside the `reg`. The convenience factor of `0.5` multiplying the regularization will become clear in a second. Evaluating this in the beginning (with random parameters) might give us `loss = 1.1`, which is `np.log(1.0/3)`, since with small initial random weights all probabilities assigned to all classes are about one third. We now want to make the loss as low as possible, with `loss = 0` as the absolute lower bound. But the lower the loss is, the higher are the probabilities assigned to the correct classes for all examples.\n\n<a name='grad'></a>\n\n### Computing the Analytic Gradient with Backpropagation\n\nWe have a way of evaluating the loss, and now we have to minimize it. We'll do so with gradient descent. That is, we start with random parameters (as shown above), and evaluate the gradient of the loss function with respect to the parameters, so that we know how we should change the parameters to decrease the loss. Lets introduce the intermediate variable $p$, which is a vector of the (normalized) probabilities. The loss for one example is:\n\n$$\np_k = \\frac{e^{f_k}}{ \\sum_j e^{f_j} } \\hspace{1in} L_i =-\\log\\left(p_{y_i}\\right)\n$$\n\nWe now wish to understand how the computed scores inside $f$ should change to decrease the loss $L_i$ that this example contributes to the full objective. In other words, we want to derive the gradient $ \\partial L_i / \\partial f_k $. The loss $L_i$ is computed from $p$, which in turn depends on $f$. It's a fun exercise to the reader to use the chain rule to derive the gradient, but it turns out to be extremely simple and interpretible in the end, after a lot of things cancel out:\n\n$$\n\\frac{\\partial L_i }{ \\partial f_k } = p_k - \\mathbb{1}(y_i = k)\n$$\n\nNotice how elegant and simple this expression is. Suppose the probabilities we computed were `p = [0.2, 0.3, 0.5]`, and that the correct class was the middle one (with probability 0.3). According to this derivation the gradient on the scores would be `df = [0.2, -0.7, 0.5]`. Recalling what the interpretation of the gradient, we see that this result is highly intuitive: increasing the first or last element of the score vector `f` (the scores of the incorrect classes) leads to an *increased* loss (due to the positive signs +0.2 and +0.5) - and increasing the loss is bad, as expected. However, increasing the score of the correct class has *negative* influence on the loss. The gradient of -0.7 is telling us that increasing the correct class score would lead to a decrease of the loss $L_i$, which makes sense. \n\nAll of this boils down to the following code. Recall that `probs` stores the probabilities of all classes (as rows) for each example. To get the gradient on the scores, which we call `dscores`, we proceed as follows:\n"},{"type":"code","language":"python","data":"dscores = probs\ndscores[range(num_examples),y] -= 1\ndscores /= num_examples"},{"type":"markdown","data":"\nLastly, we had that `scores = np.dot(X, W) + b`, so armed with the gradient on `scores` (stored in `dscores`), we can now backpropagate into `W` and `b`:\n"},{"type":"code","language":"python","data":"dW = np.dot(X.T, dscores)\ndb = np.sum(dscores, axis=0, keepdims=True)\ndW += reg*W # don't forget the regularization gradient"},{"type":"markdown","data":"\nWhere we see that we have backpropped through the matrix multiply operation, and also added the contribution from the regularization. Note that the regularization gradient has the very simple form `reg*W` since we used the constant `0.5` for its loss contribution (i.e. $\\frac{d}{dw} ( \\frac{1}{2} \\lambda w^2) = \\lambda w$. This is a common convenience trick that simplifies the gradient expression.\n\n<a name='update'></a>\n\n### Performing a parameter update\n\nNow that we've evaluated the gradient we know how every parameter influences the loss function. We will now perform a parameter update in the *negative* gradient direction to *decrease* the loss:\n"},{"type":"code","language":"python","data":"# perform a parameter update\nW += -step_size * dW\nb += -step_size * db"},{"type":"markdown","data":"\n<a name='together'></a>\n\n### Putting it all together: Training a Softmax Classifier\n\nPutting all of this together, here is the full code for training a Softmax classifier with Gradient descent:\n"},{"type":"code","language":"python","data":"#Train a Linear Classifier\n\n# initialize parameters randomly\nW = 0.01 * np.random.randn(D,K)\nb = np.zeros((1,K))\n\n# some hyperparameters\nstep_size = 1e-0\nreg = 1e-3 # regularization strength\n\n# gradient descent loop\nnum_examples = X.shape[0]\nfor i in xrange(200):\n  \n  # evaluate class scores, [N x K]\n  scores = np.dot(X, W) + b \n  \n  # compute the class probabilities\n  exp_scores = np.exp(scores)\n  probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n  \n  # compute the loss: average cross-entropy loss and regularization\n  corect_logprobs = -np.log(probs[range(num_examples),y])\n  data_loss = np.sum(corect_logprobs)/num_examples\n  reg_loss = 0.5*reg*np.sum(W*W)\n  loss = data_loss + reg_loss\n  if i % 10 == 0:\n    print \"iteration %d: loss %f\" % (i, loss)\n  \n  # compute the gradient on scores\n  dscores = probs\n  dscores[range(num_examples),y] -= 1\n  dscores /= num_examples\n  \n  # backpropate the gradient to the parameters (W,b)\n  dW = np.dot(X.T, dscores)\n  db = np.sum(dscores, axis=0, keepdims=True)\n  \n  dW += reg*W # regularization gradient\n  \n  # perform a parameter update\n  W += -step_size * dW\n  b += -step_size * db"},{"type":"markdown","data":"\nRunning this prints the output:\n"},{"type":"code","language":"text","data":"iteration 0: loss 1.096956\niteration 10: loss 0.917265\niteration 20: loss 0.851503\niteration 30: loss 0.822336\niteration 40: loss 0.807586\niteration 50: loss 0.799448\niteration 60: loss 0.794681\niteration 70: loss 0.791764\niteration 80: loss 0.789920\niteration 90: loss 0.788726\niteration 100: loss 0.787938\niteration 110: loss 0.787409\niteration 120: loss 0.787049\niteration 130: loss 0.786803\niteration 140: loss 0.786633\niteration 150: loss 0.786514\niteration 160: loss 0.786431\niteration 170: loss 0.786373\niteration 180: loss 0.786331\niteration 190: loss 0.786302"},{"type":"markdown","data":"\nWe see that we've converged to something after about 190 iterations. We can evaluate the training set accuracy:\n"},{"type":"code","language":"python","data":"# evaluate training set accuracy\nscores = np.dot(X, W) + b\npredicted_class = np.argmax(scores, axis=1)\nprint 'training accuracy: %.2f' % (np.mean(predicted_class == y))"},{"type":"markdown","data":"\nThis prints **49%**. Not very good at all, but also not surprising given that the dataset is constructed so it is not linearly separable. We can also plot the learned decision boundaries:\n\n<div class=\"fig figcenter fighighlight\">\n  <img src=\"/Diary/assets/images/cs231n/eg/spiral_linear.png\">\n  <div class=\"figcaption\">\n    Linear classifier fails to learn the toy spiral dataset.\n  </div>\n</div>\n\n<a name='net'></a>\n\n## Training a Neural Network\n\nClearly, a linear classifier is inadequate for this dataset and we would like to use a Neural Network. One additional hidden layer will suffice for this toy data. We will now need two sets of weights and biases (for the first and second layers):\n"},{"type":"code","language":"python","data":"# initialize parameters randomly\nh = 100 # size of hidden layer\nW = 0.01 * np.random.randn(D,h)\nb = np.zeros((1,h))\nW2 = 0.01 * np.random.randn(h,K)\nb2 = np.zeros((1,K))"},{"type":"markdown","data":"\nThe forward pass to compute scores now changes form:\n"},{"type":"code","language":"python","data":"# evaluate class scores with a 2-layer Neural Network\nhidden_layer = np.maximum(0, np.dot(X, W) + b) # note, ReLU activation\nscores = np.dot(hidden_layer, W2) + b2"},{"type":"markdown","data":"\nNotice that the only change from before is one extra line of code, where we first compute the hidden layer representation and then the scores based on this hidden layer. Crucially, we've also added a non-linearity, which in this case is simple ReLU that thresholds the activations on the hidden layer at zero.\n\nEverything else remains the same. We compute the loss based on the scores exactly as before, and get the gradient for the scores `dscores` exactly as before. However, the way we backpropagate that gradient into the model parameters now changes form, of course. First lets backpropagate the second layer of the Neural Network. This looks identical to the code we had for the Softmax classifier, except we're replacing `X` (the raw data), with the variable `hidden_layer`):\n"},{"type":"code","language":"python","data":"# backpropate the gradient to the parameters\n# first backprop into parameters W2 and b2\ndW2 = np.dot(hidden_layer.T, dscores)\ndb2 = np.sum(dscores, axis=0, keepdims=True)"},{"type":"markdown","data":"\nHowever, unlike before we are not yet done, because `hidden_layer` is itself a function of other parameters and the data! We need to continue backpropagation through this variable. Its gradient can be computed as:\n"},{"type":"code","language":"python","data":"dhidden = np.dot(dscores, W2.T)"},{"type":"markdown","data":"\nNow we have the gradient on the outputs of the hidden layer. Next, we have to backpropagate the ReLU non-linearity. This turns out to be easy because ReLU during the backward pass is effectively a switch. Since $r = max(0, x)$, we have that $\\frac{dr}{dx} = 1(x > 0) $. Combined with the chain rule, we see that the ReLU unit lets the gradient pass through unchanged if its input was greater than 0, but *kills it* if its input was less than zero during the forward pass. Hence, we can backpropagate the ReLU in place simply with:\n"},{"type":"code","language":"python","data":"# backprop the ReLU non-linearity\ndhidden[hidden_layer <= 0] = 0"},{"type":"markdown","data":"\nAnd now we finally continue to the first layer weights and biases:\n"},{"type":"code","language":"python","data":"# finally into W,b\ndW = np.dot(X.T, dhidden)\ndb = np.sum(dhidden, axis=0, keepdims=True)"},{"type":"markdown","data":"\nWe're done! We have the gradients `dW,db,dW2,db2` and can perform the parameter update. Everything else remains unchanged. The full code looks very similar:\n"},{"type":"code","language":"python","data":"# initialize parameters randomly\nh = 100 # size of hidden layer\nW = 0.01 * np.random.randn(D,h)\nb = np.zeros((1,h))\nW2 = 0.01 * np.random.randn(h,K)\nb2 = np.zeros((1,K))\n\n# some hyperparameters\nstep_size = 1e-0\nreg = 1e-3 # regularization strength\n\n# gradient descent loop\nnum_examples = X.shape[0]\nfor i in xrange(10000):\n  \n  # evaluate class scores, [N x K]\n  hidden_layer = np.maximum(0, np.dot(X, W) + b) # note, ReLU activation\n  scores = np.dot(hidden_layer, W2) + b2\n  \n  # compute the class probabilities\n  exp_scores = np.exp(scores)\n  probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n  \n  # compute the loss: average cross-entropy loss and regularization\n  corect_logprobs = -np.log(probs[range(num_examples),y])\n  data_loss = np.sum(corect_logprobs)/num_examples\n  reg_loss = 0.5*reg*np.sum(W*W) + 0.5*reg*np.sum(W2*W2)\n  loss = data_loss + reg_loss\n  if i % 1000 == 0:\n    print \"iteration %d: loss %f\" % (i, loss)\n  \n  # compute the gradient on scores\n  dscores = probs\n  dscores[range(num_examples),y] -= 1\n  dscores /= num_examples\n  \n  # backpropate the gradient to the parameters\n  # first backprop into parameters W2 and b2\n  dW2 = np.dot(hidden_layer.T, dscores)\n  db2 = np.sum(dscores, axis=0, keepdims=True)\n  # next backprop into hidden layer\n  dhidden = np.dot(dscores, W2.T)\n  # backprop the ReLU non-linearity\n  dhidden[hidden_layer <= 0] = 0\n  # finally into W,b\n  dW = np.dot(X.T, dhidden)\n  db = np.sum(dhidden, axis=0, keepdims=True)\n  \n  # add regularization gradient contribution\n  dW2 += reg * W2\n  dW += reg * W\n  \n  # perform a parameter update\n  W += -step_size * dW\n  b += -step_size * db\n  W2 += -step_size * dW2\n  b2 += -step_size * db2"},{"type":"markdown","data":"\nThis prints:\n"},{"type":"code","language":"text","data":"iteration 0: loss 1.098744\niteration 1000: loss 0.294946\niteration 2000: loss 0.259301\niteration 3000: loss 0.248310\niteration 4000: loss 0.246170\niteration 5000: loss 0.245649\niteration 6000: loss 0.245491\niteration 7000: loss 0.245400\niteration 8000: loss 0.245335\niteration 9000: loss 0.245292"},{"type":"markdown","data":"\nThe training accuracy is now:\n"},{"type":"code","language":"python","data":"# evaluate training set accuracy\nhidden_layer = np.maximum(0, np.dot(X, W) + b)\nscores = np.dot(hidden_layer, W2) + b2\npredicted_class = np.argmax(scores, axis=1)\nprint 'training accuracy: %.2f' % (np.mean(predicted_class == y))"},{"type":"markdown","data":"\nWhich prints **98%**!. We can also visualize the decision boundaries:\n\n<div class=\"fig figcenter fighighlight\">\n  <img src=\"/Diary/assets/images/cs231n/eg/spiral_net.png\">\n  <div class=\"figcaption\">\n    Neural Network classifier crushes the spiral dataset.\n  </div>\n</div>\n\n## Summary\n\nWe've worked with a toy 2D dataset and trained both a linear network and a 2-layer Neural Network. We saw that the change from a linear classifier to a Neural Network involves very few changes in the code. The score function changes its form (1 line of code difference), and the backpropagation changes its form (we have to perform one more round of backprop through the hidden layer to the first layer of the network).\n\n- You may want to look at this IPython Notebook code [rendered as HTML](http://cs.stanford.edu/people/karpathy/cs231nfiles/minimal_net.html).\n- Or download the [ipynb file](http://cs.stanford.edu/people/karpathy/cs231nfiles/minimal_net.ipynb)\n\n"}]}
<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        <meta name="author" content="NiuLiangtao">
        <link rel="canonical" href="https://1007530194.github.io/Diary/书籍/机器学习实战/04.朴素贝叶斯/">
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>4.朴素贝叶斯 - Blog of NiuLiangtao</title>
        <link href="../../../css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome-4.5.0.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="../../../css/highlight.css">
        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
        <![endif]-->

        <script src="../../../js/jquery-1.10.2.min.js"></script>
        <script src="../../../js/bootstrap-3.0.3.min.js"></script>
        <script src="../../../js/highlight.pack.js"></script> 
    </head>

    <body>

        <div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">

        <!-- Collapsed navigation -->
        <div class="navbar-header">
            <!-- Expander button -->
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="../../../ToDo/">Blog of NiuLiangtao</a>
        </div>

        <!-- Expanded navigation -->
        <div class="navbar-collapse collapse">
                <!-- Main navigation -->
                <ul class="nav navbar-nav">
                    <li >
                        <a href="../../../ToDo/">Home</a>
                    </li>
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Home <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                            
<li >
    <a href="../../../AboutMe/">About</a>
</li>
                            
<li >
    <a href="../../..">Home</a>
</li>
                            
<li >
    <a href="../../../Nothing/">Nothing</a>
</li>
                            
<li >
    <a href="../../../ToDo/">要做的</a>
</li>
                        </ul>
                    </li>
                    <li class="dropdown active">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">书籍 <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                            
<li >
    <a href="../../">Home</a>
</li>
                            
  <li class="dropdown-submenu">
    <a href="#">机器学习实战</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../00naive-bayes-discuss/">naive-bayes-discuss</a>
</li>
            
<li >
    <a href="../01.机器学习基础/">1.机器学习基础</a>
</li>
            
<li >
    <a href="../02.k-近邻算法/">2.k-近邻算法</a>
</li>
            
<li >
    <a href="../03.决策树/">3.决策树</a>
</li>
            
<li class="active">
    <a href="./">4.朴素贝叶斯</a>
</li>
            
<li >
    <a href="../05.Logistic回归/">05.Logistic回归</a>
</li>
            
<li >
    <a href="../06.0.支持向量机/">06.0.支持向量机</a>
</li>
            
<li >
    <a href="../06.1.支持向量机的几个通俗理解/">06.1.支持向量机的几个通俗理解</a>
</li>
            
<li >
    <a href="../07.集成方法-随机森林和AdaBoost/">07.集成方法 随机森林和AdaBoost</a>
</li>
            
<li >
    <a href="../08.预测数值型数据-回归/">08.预测数值型数据 回归</a>
</li>
            
<li >
    <a href="../09.树回归/">09.树回归</a>
</li>
            
<li >
    <a href="../10.k-means聚类/">10.k means聚类</a>
</li>
            
<li >
    <a href="../11.使用Apriori算法进行关联分析/">11.使用Apriori算法进行关联分析</a>
</li>
            
<li >
    <a href="../12.使用FP-growth算法来高效发现频繁项集/">12.使用FP growth算法来高效发现频繁项集</a>
</li>
            
<li >
    <a href="../13.利用PCA来简化数据/">13.利用PCA来简化数据</a>
</li>
            
<li >
    <a href="../14.利用SVD简化数据/">14.利用SVD简化数据</a>
</li>
            
<li >
    <a href="../15.大数据与MapReduce/">15.大数据与MapReduce</a>
</li>
            
<li >
    <a href="../16.推荐系统/">16.推荐系统</a>
</li>
    </ul>
  </li>
                            
  <li class="dropdown-submenu">
    <a href="#">深度学习</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../深度学习/DeepLearning/">DeepLearning</a>
</li>
    </ul>
  </li>
                        </ul>
                    </li>
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">学习 <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                            
<li >
    <a href="../../../学习/">Home</a>
</li>
                            
  <li class="dropdown-submenu">
    <a href="#">tensorflow</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../../学习/tensorflow/如何选择优化器 optimizer/">如何选择优化器 optimizer</a>
</li>
    </ul>
  </li>
                            
  <li class="dropdown-submenu">
    <a href="#">深度学习</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../../学习/深度学习/主成分分析-2/">主成分分析 2</a>
</li>
            
<li >
    <a href="../../../学习/深度学习/主成分分析/">主成分分析</a>
</li>
            
<li >
    <a href="../../../学习/深度学习/最小二乘法/">最小二乘法</a>
</li>
            
<li >
    <a href="../../../学习/深度学习/矩阵分解/">矩阵分解</a>
</li>
            
<li >
    <a href="../../../学习/深度学习/矩阵求导/">矩阵求导</a>
</li>
    </ul>
  </li>
                            
  <li class="dropdown-submenu">
    <a href="#">转载</a>
    <ul class="dropdown-menu">
            
  <li class="dropdown-submenu">
    <a href="#">cs231n-ch</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../../学习/转载/cs231n-ch/ch-assignment2_google_cloud/">Ch assignment2 google cloud</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-ch/ch-aws-tutorial/">Ch aws tutorial</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-ch/ch-classification/">Ch classification</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-ch/ch-convnet-tips/">Ch convnet tips</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-ch/ch-convolutional-networks/">Ch convolutional networks</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-ch/ch-google_cloud_tutorial/">Ch google cloud tutorial</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-ch/ch-ipython-tutorial/">Ch ipython tutorial</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-ch/ch-linear-classify/">Ch linear classify</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-ch/ch-neural-networks-1/">Ch neural networks 1</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-ch/ch-neural-networks-2/">Ch neural networks 2</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-ch/ch-neural-networks-3/">Ch neural networks 3</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-ch/ch-neural-networks-case-study/">Ch neural networks case study</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-ch/ch-optimization-1/">Ch optimization 1</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-ch/ch-optimization-2/">Ch optimization 2</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-ch/ch-overview/">Ch overview</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-ch/ch-poster/">Ch poster</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-ch/ch-python-numpy-tutorial/">Ch python numpy tutorial</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-ch/ch-Readme/">ch Readme</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-ch/ch-transfer-learning/">Ch transfer learning</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-ch/ch-understanding-cnn/">Ch understanding cnn</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-ch/terminal-tutorial/">Terminal tutorial</a>
</li>
    </ul>
  </li>
            
  <li class="dropdown-submenu">
    <a href="#">cs231n-en</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../../学习/转载/cs231n-en/en-assignment2_google_cloud/">En assignment2 google cloud</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-en/en-aws-tutorial/">En aws tutorial</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-en/en-classification/">En classification</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-en/en-convnet-tips/">En convnet tips</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-en/en-convolutional-networks/">En convolutional networks</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-en/en-google_cloud_tutorial/">En google cloud tutorial</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-en/en-ipython-tutorial/">En ipython tutorial</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-en/en-linear-classify/">En linear classify</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-en/en-neural-networks-1/">En neural networks 1</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-en/en-neural-networks-2/">En neural networks 2</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-en/en-neural-networks-3/">En neural networks 3</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-en/en-neural-networks-case-study/">En neural networks case study</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-en/en-optimization-1/">En optimization 1</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-en/en-optimization-2/">En optimization 2</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-en/en-overview/">En overview</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-en/en-poster/">En poster</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-en/en-python-numpy-tutorial/">En python numpy tutorial</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-en/en-Readme/">en Readme</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-en/en-terminal-tutorial/">En terminal tutorial</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-en/en-transfer-learning/">En transfer learning</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-en/en-understanding-cnn/">En understanding cnn</a>
</li>
    </ul>
  </li>
            
  <li class="dropdown-submenu">
    <a href="#">example</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../../学习/转载/example/a-web-crawer-with-a-asyncio-coroutines-1/">A web crawer with a asyncio coroutines 1</a>
</li>
            
<li >
    <a href="../../../学习/转载/example/a-web-crawer-with-a-asyncio-coroutines-2/">A web crawer with a asyncio coroutines 2</a>
</li>
            
<li >
    <a href="../../../学习/转载/example/bias-variance/">Bias variance</a>
</li>
            
<li >
    <a href="../../../学习/转载/example/bitcoin-explained-1/">Bitcoin explained 1</a>
</li>
            
<li >
    <a href="../../../学习/转载/example/dive-into-gradient-decent/">Dive into gradient decent</a>
</li>
            
<li >
    <a href="../../../学习/转载/example/ethereum-ultimate-guide/">Ethereum ultimate guide</a>
</li>
            
<li >
    <a href="../../../学习/转载/example/fork-exec-source/">Fork exec source</a>
</li>
            
<li >
    <a href="../../../学习/转载/example/latex语法/">Latex语法</a>
</li>
            
<li >
    <a href="../../../学习/转载/example/latex语法1/">Latex语法1</a>
</li>
            
<li >
    <a href="../../../学习/转载/example/nosql-in-python/">Nosql in python</a>
</li>
            
<li >
    <a href="../../../学习/转载/example/perceptron/">Perceptron</a>
</li>
            
<li >
    <a href="../../../学习/转载/example/quick-latex/">Quick latex</a>
</li>
            
<li >
    <a href="../../../学习/转载/example/tendermint/">Tendermint</a>
</li>
    </ul>
  </li>
            
  <li class="dropdown-submenu">
    <a href="#">example2</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../../学习/转载/example2/arrays-similar/">Arrays similar</a>
</li>
            
<li >
    <a href="../../../学习/转载/example2/baidu-ife-1/">Baidu ife 1</a>
</li>
            
<li >
    <a href="../../../学习/转载/example2/create-my-blog-with-jekyll/">Create my blog with jekyll</a>
</li>
            
<li >
    <a href="../../../学习/转载/example2/front-end-tools/">Front end tools</a>
</li>
            
<li >
    <a href="../../../学习/转载/example2/git-clone-not-master-branch/">Git clone not master branch</a>
</li>
            
<li >
    <a href="../../../学习/转载/example2/History-API/">History API</a>
</li>
            
<li >
    <a href="../../../学习/转载/example2/how-to-use-babel/">How to use babel</a>
</li>
            
<li >
    <a href="../../../学习/转载/example2/how-to-write-a-count-down/">How to write a count down</a>
</li>
            
<li >
    <a href="../../../学习/转载/example2/JavaScript-closure/">JavaScript closure</a>
</li>
            
<li >
    <a href="../../../学习/转载/example2/JavaScript-function/">JavaScript function</a>
</li>
            
<li >
    <a href="../../../学习/转载/example2/JavaScript-good-parts-note1/">JavaScript good parts note1</a>
</li>
            
<li >
    <a href="../../../学习/转载/example2/JavaScript-good-parts-note2/">JavaScript good parts note2</a>
</li>
            
<li >
    <a href="../../../学习/转载/example2/JavaScript-good-parts-note3/">JavaScript good parts note3</a>
</li>
            
<li >
    <a href="../../../学习/转载/example2/JavaScript-Net/">JavaScript Net</a>
</li>
            
<li >
    <a href="../../../学习/转载/example2/JavaScript-Object-Oriented/">JavaScript Object Oriented</a>
</li>
            
<li >
    <a href="../../../学习/转载/example2/JavaScript-this/">JavaScript this</a>
</li>
            
<li >
    <a href="../../../学习/转载/example2/jekyll-theme-version-2.0/">Jekyll theme version 2.0</a>
</li>
            
<li >
    <a href="../../../学习/转载/example2/js-create-file-and-download/">Js create file and download</a>
</li>
            
<li >
    <a href="../../../学习/转载/example2/low-IE-click-empty-block-bug/">low IE click empty block bug</a>
</li>
            
<li >
    <a href="../../../学习/转载/example2/regular-expression-group/">Regular expression group</a>
</li>
            
<li >
    <a href="../../../学习/转载/example2/scope/">Scope</a>
</li>
            
<li >
    <a href="../../../学习/转载/example2/shuffle-algorithm/">Shuffle algorithm</a>
</li>
            
<li >
    <a href="../../../学习/转载/example2/sublimeLinter/">sublimeLinter</a>
</li>
            
<li >
    <a href="../../../学习/转载/example2/Syncing-a-fork/">Syncing a fork</a>
</li>
            
<li >
    <a href="../../../学习/转载/example2/teach-girlfriend-html-css/">Teach girlfriend html css</a>
</li>
            
<li >
    <a href="../../../学习/转载/example2/web-app/">Web app</a>
</li>
            
<li >
    <a href="../../../学习/转载/example2/weinre/">Weinre</a>
</li>
    </ul>
  </li>
    </ul>
  </li>
                        </ul>
                    </li>
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">工具 <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                            
<li >
    <a href="../../../工具/About/">About</a>
</li>
                            
<li >
    <a href="../../../工具/Cheatsheet/">Cheatsheet</a>
</li>
                            
<li >
    <a href="../../../工具/">Home</a>
</li>
                            
<li >
    <a href="../../../工具/shell显示时间/">Shell显示时间</a>
</li>
                            
  <li class="dropdown-submenu">
    <a href="#">常用工具</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../../工具/常用工具/10分钟上手Latex/">10分钟上手Latex</a>
</li>
            
<li >
    <a href="../../../工具/常用工具/10分钟上手Pandas/">10分钟上手Pandas</a>
</li>
            
<li >
    <a href="../../../工具/常用工具/10分钟上手Python/">10分钟上手Python</a>
</li>
    </ul>
  </li>
                        </ul>
                    </li>
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">文章 <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                            
<li >
    <a href="../../../文章/">Home</a>
</li>
                        </ul>
                    </li>
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">日常 <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                            
<li >
    <a href="../../../日常/">Home</a>
</li>
                            
  <li class="dropdown-submenu">
    <a href="#">test</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../../日常/test/testtest/">Testtest</a>
</li>
    </ul>
  </li>
                        </ul>
                    </li>
                </ul>

            <ul class="nav navbar-nav navbar-right">
                <li>
                    <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                        <i class="fa fa-search"></i> Search
                    </a>
                </li>
                    <li >
                        <a rel="next" href="../03.决策树/">
                            <i class="fa fa-arrow-left"></i> Previous
                        </a>
                    </li>
                    <li >
                        <a rel="prev" href="../05.Logistic回归/">
                            Next <i class="fa fa-arrow-right"></i>
                        </a>
                    </li>
                    <li>
                        <a href="https://github.com/1007530194/Diary">1007530194/Diary</a>
                    </li>
            </ul>
        </div>
    </div>
</div>

        <div class="container">
                <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
        <li class="main active"><a href="#4">第4章 基于概率论的分类方法：朴素贝叶斯</a></li>
            <li><a href="#_1">朴素贝叶斯 概述</a></li>
            <li><a href="#_2">贝叶斯理论 &amp; 条件概率</a></li>
            <li><a href="#_6">朴素贝叶斯 场景</a></li>
            <li><a href="#_7">朴素贝叶斯 原理</a></li>
            <li><a href="#_11">朴素贝叶斯 项目案例</a></li>
    </ul>
</div></div>
                <div class="col-md-9" role="main">

<h1 id="4">第4章 基于概率论的分类方法：朴素贝叶斯<a class="headerlink" href="#4" title="Permanent link">&para;</a></h1>
<h2 id="_1">朴素贝叶斯 概述<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h2>
<p><code>贝叶斯分类是一类分类算法的总称，这类算法均以贝叶斯定理为基础，故统称为贝叶斯分类。本章首先介绍贝叶斯分类算法的基础——贝叶斯定理。最后，我们通过实例来讨论贝叶斯分类的中最简单的一种: 朴素贝叶斯分类。</code></p>
<h2 id="_2">贝叶斯理论 &amp; 条件概率<a class="headerlink" href="#_2" title="Permanent link">&para;</a></h2>
<h3 id="_3">贝叶斯理论<a class="headerlink" href="#_3" title="Permanent link">&para;</a></h3>
<p>我们现在有一个数据集，它由两类数据组成，数据分布如下图所示：</p>
<p><img alt="朴素贝叶斯示例数据分布" src="../resources/4.NaiveBayesian/朴素贝叶斯示例数据分布.png" title="参数已知的概率分布" /></p>
<p>我们现在用 p1(x,y) 表示数据点 (x,y) 属于类别 1（图中用圆点表示的类别）的概率，用 p2(x,y) 表示数据点 (x,y) 属于类别 2（图中三角形表示的类别）的概率，那么对于一个新数据点 (x,y)，可以用下面的规则来判断它的类别：
* 如果 p1(x,y) &gt; p2(x,y) ，那么类别为1
* 如果 p2(x,y) &gt; p1(x,y) ，那么类别为2</p>
<p>也就是说，我们会选择高概率对应的类别。这就是贝叶斯决策理论的核心思想，即选择具有最高概率的决策。</p>
<h3 id="_4">条件概率<a class="headerlink" href="#_4" title="Permanent link">&para;</a></h3>
<p>如果你对 p(x,y|c1) 符号很熟悉，那么可以跳过本小节。</p>
<p>有一个装了 7 块石头的罐子，其中 3 块是白色的，4 块是黑色的。如果从罐子中随机取出一块石头，那么是白色石头的可能性是多少？由于取石头有 7 种可能，其中 3 种为白色，所以取出白色石头的概率为 3/7 。那么取到黑色石头的概率又是多少呢？很显然，是 4/7 。我们使用 P(white) 来表示取到白色石头的概率，其概率值可以通过白色石头数目除以总的石头数目来得到。</p>
<p><img alt="包含 7 块石头的集合" src="../resources/4.NaiveBayesian/NB_2.png" /></p>
<p>如果这 7 块石头如下图所示，放在两个桶中，那么上述概率应该如何计算？</p>
<p><img alt="7块石头放入两个桶中" src="../resources/4.NaiveBayesian/NB_3.png" /></p>
<p>计算 P(white) 或者 P(black) ，如果事先我们知道石头所在桶的信息是会改变结果的。这就是所谓的条件概率（conditional probablity）。假定计算的是从 B 桶取到白色石头的概率，这个概率可以记作 P(white|bucketB) ，我们称之为“在已知石头出自 B 桶的条件下，取出白色石头的概率”。很容易得到，P(white|bucketA) 值为 2/4 ，P(white|bucketB) 的值为 &#8531; 。</p>
<p>条件概率的计算公式如下：</p>
<p>P(white|bucketB) = P(white and bucketB) / P(bucketB)</p>
<p>首先，我们用 B 桶中白色石头的个数除以两个桶中总的石头数，得到 P(white and bucketB) = 1/7 .其次，由于 B 桶中有 3 块石头，而总石头数为 7 ，于是 P(bucketB) 就等于 3/7 。于是又 P(white|bucketB) = P(white and bucketB) / P(bucketB) = (1/7) / (3/7) = &#8531; 。</p>
<p>另外一种有效计算条件概率的方法称为贝叶斯准则。贝叶斯准则告诉我们如何交换条件概率中的条件与结果，即如果已知 P(x|c)，要求 P(c|x)，那么可以使用下面的计算方法：</p>
<p><img alt="计算p(c|x)的方法" src="../../../images/4.NaiveBayesian/NB_4.png" /></p>
<h3 id="_5">使用条件概率来分类<a class="headerlink" href="#_5" title="Permanent link">&para;</a></h3>
<p>上面我们提到贝叶斯决策理论要求计算两个概率 p1(x, y) 和 p2(x, y):
* 如果 p1(x, y) &gt; p2(x, y), 那么属于类别 1;
* 如果 p2(x, y) &gt; p1(X, y), 那么属于类别 2.</p>
<p>这并不是贝叶斯决策理论的所有内容。使用 p1() 和 p2() 只是为了尽可能简化描述，而真正需要计算和比较的是 p(c1|x, y) 和 p(c2|x, y) .这些符号所代表的具体意义是: 给定某个由 x、y 表示的数据点，那么该数据点来自类别 c1 的概率是多少？数据点来自类别 c2 的概率又是多少？注意这些概率与概率 p(x, y|c1) 并不一样，不过可以使用贝叶斯准则来交换概率中条件与结果。具体地，应用贝叶斯准则得到: </p>
<p><img alt="应用贝叶斯准则" src="../resources/4.NaiveBayesian/NB_5.png" /></p>
<p>使用上面这些定义，可以定义贝叶斯分类准则为:
* 如果 P(c1|x, y) &gt; P(c2|x, y), 那么属于类别 c1;
* 如果 P(c2|x, y) &gt; P(c1|x, y), 那么属于类别 c2.</p>
<p>在文档分类中，整个文档（如一封电子邮件）是实例，而电子邮件中的某些元素则构成特征。我们可以观察文档中出现的词，并把每个词作为一个特征，而每个词的出现或者不出现作为该特征的值，这样得到的特征数目就会跟词汇表中的词的数目一样多。</p>
<p>我们假设特征之间  <strong>相互独立</strong> 。所谓 <b>独立(independence)</b> 指的是统计意义上的独立，即一个特征或者单词出现的可能性与它和其他单词相邻没有关系，比如说，“我们”中的“我”和“们”出现的概率与这两个字相邻没有任何关系。这个假设正是朴素贝叶斯分类器中 朴素(naive) 一词的含义。朴素贝叶斯分类器中的另一个假设是，<b>每个特征同等重要</b>。</p>
<p><b>Note:</b> 朴素贝叶斯分类器通常有两种实现方式: 一种基于伯努利模型实现，一种基于多项式模型实现。这里采用前一种实现方式。该实现方式中并不考虑词在文档中出现的次数，只考虑出不出现，因此在这个意义上相当于假设词是等权重的。</p>
<h2 id="_6">朴素贝叶斯 场景<a class="headerlink" href="#_6" title="Permanent link">&para;</a></h2>
<p>机器学习的一个重要应用就是文档的自动分类。</p>
<p>在文档分类中，整个文档（如一封电子邮件）是实例，而电子邮件中的某些元素则构成特征。我们可以观察文档中出现的词，并把每个词作为一个特征，而每个词的出现或者不出现作为该特征的值，这样得到的特征数目就会跟词汇表中的词的数目一样多。</p>
<p>朴素贝叶斯是上面介绍的贝叶斯分类器的一个扩展，是用于文档分类的常用算法。下面我们会进行一些朴素贝叶斯分类的实践项目。</p>
<h2 id="_7">朴素贝叶斯 原理<a class="headerlink" href="#_7" title="Permanent link">&para;</a></h2>
<h3 id="_8">朴素贝叶斯 工作原理<a class="headerlink" href="#_8" title="Permanent link">&para;</a></h3>
<div class="codehilite"><pre><span></span>提取所有文档中的词条并进行去重
获取文档的所有类别
计算每个类别中的文档数目
对每篇训练文档: 
    对每个类别: 
        如果词条出现在文档中--&gt;增加该词条的计数值（for循环或者矩阵相加）
        增加所有词条的计数值（此类别下词条总数）
对每个类别: 
    对每个词条: 
        将该词条的数目除以总词条数目得到的条件概率（P(词条|类别)）
返回该文档属于每个类别的条件概率（P(类别|文档的所有词条)）
</pre></div>

<h3 id="_9">朴素贝叶斯 开发流程<a class="headerlink" href="#_9" title="Permanent link">&para;</a></h3>
<div class="codehilite"><pre><span></span>收集数据: 可以使用任何方法。
准备数据: 需要数值型或者布尔型数据。
分析数据: 有大量特征时，绘制特征作用不大，此时使用直方图效果更好。
训练算法: 计算不同的独立特征的条件概率。
测试算法: 计算错误率。
使用算法: 一个常见的朴素贝叶斯应用是文档分类。可以在任意的分类场景中使用朴素贝叶斯分类器，不一定非要是文本。
</pre></div>

<h3 id="_10">朴素贝叶斯 算法特点<a class="headerlink" href="#_10" title="Permanent link">&para;</a></h3>
<div class="codehilite"><pre><span></span>优点: 在数据较少的情况下仍然有效，可以处理多类别问题。
缺点: 对于输入数据的准备方式较为敏感。
适用数据类型: 标称型数据。
</pre></div>

<h2 id="_11">朴素贝叶斯 项目案例<a class="headerlink" href="#_11" title="Permanent link">&para;</a></h2>
<h3 id="1">项目案例1: 屏蔽社区留言板的侮辱性言论<a class="headerlink" href="#1" title="Permanent link">&para;</a></h3>
<h4 id="_12">项目概述<a class="headerlink" href="#_12" title="Permanent link">&para;</a></h4>
<p>构建一个快速过滤器来屏蔽在线社区留言板上的侮辱性言论。如果某条留言使用了负面或者侮辱性的语言，那么就将该留言标识为内容不当。对此问题建立两个类别: 侮辱类和非侮辱类，使用 1 和 0 分别表示。</p>
<h4 id="_13">开发流程<a class="headerlink" href="#_13" title="Permanent link">&para;</a></h4>
<div class="codehilite"><pre><span></span>收集数据: 可以使用任何方法
准备数据: 从文本中构建词向量
分析数据: 检查词条确保解析的正确性
训练算法: 从词向量计算概率
测试算法: 根据现实情况修改分类器
使用算法: 对社区留言板言论进行分类
</pre></div>

<blockquote>
<p>收集数据: 可以使用任何方法</p>
</blockquote>
<p>本例是我们自己构造的词表:</p>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">loadDataSet</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    创建数据集</span>
<span class="sd">    :return: 单词列表postingList, 所属类别classVec</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">postingList</span> <span class="o">=</span> <span class="p">[[</span><span class="s1">&#39;my&#39;</span><span class="p">,</span> <span class="s1">&#39;dog&#39;</span><span class="p">,</span> <span class="s1">&#39;has&#39;</span><span class="p">,</span> <span class="s1">&#39;flea&#39;</span><span class="p">,</span> <span class="s1">&#39;problems&#39;</span><span class="p">,</span> <span class="s1">&#39;help&#39;</span><span class="p">,</span> <span class="s1">&#39;please&#39;</span><span class="p">],</span> <span class="c1">#[0,0,1,1,1......]</span>
                   <span class="p">[</span><span class="s1">&#39;maybe&#39;</span><span class="p">,</span> <span class="s1">&#39;not&#39;</span><span class="p">,</span> <span class="s1">&#39;take&#39;</span><span class="p">,</span> <span class="s1">&#39;him&#39;</span><span class="p">,</span> <span class="s1">&#39;to&#39;</span><span class="p">,</span> <span class="s1">&#39;dog&#39;</span><span class="p">,</span> <span class="s1">&#39;park&#39;</span><span class="p">,</span> <span class="s1">&#39;stupid&#39;</span><span class="p">],</span>
                   <span class="p">[</span><span class="s1">&#39;my&#39;</span><span class="p">,</span> <span class="s1">&#39;dalmation&#39;</span><span class="p">,</span> <span class="s1">&#39;is&#39;</span><span class="p">,</span> <span class="s1">&#39;so&#39;</span><span class="p">,</span> <span class="s1">&#39;cute&#39;</span><span class="p">,</span> <span class="s1">&#39;I&#39;</span><span class="p">,</span> <span class="s1">&#39;love&#39;</span><span class="p">,</span> <span class="s1">&#39;him&#39;</span><span class="p">],</span>
                   <span class="p">[</span><span class="s1">&#39;stop&#39;</span><span class="p">,</span> <span class="s1">&#39;posting&#39;</span><span class="p">,</span> <span class="s1">&#39;stupid&#39;</span><span class="p">,</span> <span class="s1">&#39;worthless&#39;</span><span class="p">,</span> <span class="s1">&#39;garbage&#39;</span><span class="p">],</span>
                   <span class="p">[</span><span class="s1">&#39;mr&#39;</span><span class="p">,</span> <span class="s1">&#39;licks&#39;</span><span class="p">,</span> <span class="s1">&#39;ate&#39;</span><span class="p">,</span> <span class="s1">&#39;my&#39;</span><span class="p">,</span> <span class="s1">&#39;steak&#39;</span><span class="p">,</span> <span class="s1">&#39;how&#39;</span><span class="p">,</span> <span class="s1">&#39;to&#39;</span><span class="p">,</span> <span class="s1">&#39;stop&#39;</span><span class="p">,</span> <span class="s1">&#39;him&#39;</span><span class="p">],</span>
                   <span class="p">[</span><span class="s1">&#39;quit&#39;</span><span class="p">,</span> <span class="s1">&#39;buying&#39;</span><span class="p">,</span> <span class="s1">&#39;worthless&#39;</span><span class="p">,</span> <span class="s1">&#39;dog&#39;</span><span class="p">,</span> <span class="s1">&#39;food&#39;</span><span class="p">,</span> <span class="s1">&#39;stupid&#39;</span><span class="p">]]</span>
    <span class="n">classVec</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>  <span class="c1"># 1 is abusive, 0 not</span>
    <span class="k">return</span> <span class="n">postingList</span><span class="p">,</span> <span class="n">classVec</span>
</pre></div>

<blockquote>
<p>准备数据: 从文本中构建词向量</p>
</blockquote>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">createVocabList</span><span class="p">(</span><span class="n">dataSet</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    获取所有单词的集合</span>
<span class="sd">    :param dataSet: 数据集</span>
<span class="sd">    :return: 所有单词的集合(即不含重复元素的单词列表)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">vocabSet</span> <span class="o">=</span> <span class="nb">set</span><span class="p">([])</span>  <span class="c1"># create empty set</span>
    <span class="k">for</span> <span class="n">document</span> <span class="ow">in</span> <span class="n">dataSet</span><span class="p">:</span>
        <span class="c1"># 操作符 | 用于求两个集合的并集</span>
        <span class="n">vocabSet</span> <span class="o">=</span> <span class="n">vocabSet</span> <span class="o">|</span> <span class="nb">set</span><span class="p">(</span><span class="n">document</span><span class="p">)</span>  <span class="c1"># union of the two sets</span>
    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">vocabSet</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">setOfWords2Vec</span><span class="p">(</span><span class="n">vocabList</span><span class="p">,</span> <span class="n">inputSet</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    遍历查看该单词是否出现，出现该单词则将该单词置1</span>
<span class="sd">    :param vocabList: 所有单词集合列表</span>
<span class="sd">    :param inputSet: 输入数据集</span>
<span class="sd">    :return: 匹配列表[0,1,0,1...]，其中 1与0 表示词汇表中的单词是否出现在输入的数据集中</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># 创建一个和词汇表等长的向量，并将其元素都设置为0</span>
    <span class="n">returnVec</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocabList</span><span class="p">)</span><span class="c1"># [0,0......]</span>
    <span class="c1"># 遍历文档中的所有单词，如果出现了词汇表中的单词，则将输出的文档向量中的对应值设为1</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">inputSet</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">vocabList</span><span class="p">:</span>
            <span class="n">returnVec</span><span class="p">[</span><span class="n">vocabList</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">word</span><span class="p">)]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">print</span> <span class="s2">&quot;the word: </span><span class="si">%s</span><span class="s2"> is not in my Vocabulary!&quot;</span> <span class="o">%</span> <span class="n">word</span>
    <span class="k">return</span> <span class="n">returnVec</span>
</pre></div>

<blockquote>
<p>分析数据: 检查词条确保解析的正确性</p>
</blockquote>
<p>检查函数执行情况，检查词表，不出现重复单词，需要的话，可以对其进行排序。</p>
<div class="codehilite"><pre><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">listOPosts</span><span class="p">,</span> <span class="n">listClasses</span> <span class="o">=</span> <span class="n">bayes</span><span class="o">.</span><span class="n">loadDataSet</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">myVocabList</span> <span class="o">=</span> <span class="n">bayes</span><span class="o">.</span><span class="n">createVocabList</span><span class="p">(</span><span class="n">listOPosts</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">myVocabList</span>
<span class="p">[</span><span class="s1">&#39;cute&#39;</span><span class="p">,</span> <span class="s1">&#39;love&#39;</span><span class="p">,</span> <span class="s1">&#39;help&#39;</span><span class="p">,</span> <span class="s1">&#39;garbage&#39;</span><span class="p">,</span> <span class="s1">&#39;quit&#39;</span><span class="p">,</span> <span class="s1">&#39;I&#39;</span><span class="p">,</span> <span class="s1">&#39;problems&#39;</span><span class="p">,</span> <span class="s1">&#39;is&#39;</span><span class="p">,</span> <span class="s1">&#39;park&#39;</span><span class="p">,</span> 
<span class="s1">&#39;stop&#39;</span><span class="p">,</span> <span class="s1">&#39;flea&#39;</span><span class="p">,</span> <span class="s1">&#39;dalmation&#39;</span><span class="p">,</span> <span class="s1">&#39;licks&#39;</span><span class="p">,</span> <span class="s1">&#39;food&#39;</span><span class="p">,</span> <span class="s1">&#39;not&#39;</span><span class="p">,</span> <span class="s1">&#39;him&#39;</span><span class="p">,</span> <span class="s1">&#39;buying&#39;</span><span class="p">,</span> <span class="s1">&#39;posting&#39;</span><span class="p">,</span> <span class="s1">&#39;has&#39;</span><span class="p">,</span> <span class="s1">&#39;worthless&#39;</span><span class="p">,</span> <span class="s1">&#39;ate&#39;</span><span class="p">,</span> <span class="s1">&#39;to&#39;</span><span class="p">,</span> <span class="s1">&#39;maybe&#39;</span><span class="p">,</span> <span class="s1">&#39;please&#39;</span><span class="p">,</span> <span class="s1">&#39;dog&#39;</span><span class="p">,</span> <span class="s1">&#39;how&#39;</span><span class="p">,</span> 
<span class="s1">&#39;stupid&#39;</span><span class="p">,</span> <span class="s1">&#39;so&#39;</span><span class="p">,</span> <span class="s1">&#39;take&#39;</span><span class="p">,</span> <span class="s1">&#39;mr&#39;</span><span class="p">,</span> <span class="s1">&#39;steak&#39;</span><span class="p">,</span> <span class="s1">&#39;my&#39;</span><span class="p">]</span>
</pre></div>

<p>检查函数有效性。例如：myVocabList 中索引为 2 的元素是什么单词？应该是是 help 。该单词在第一篇文档中出现了，现在检查一下看看它是否出现在第四篇文档中。</p>
<div class="codehilite"><pre><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">bayes</span><span class="o">.</span><span class="n">setOfWords2Vec</span><span class="p">(</span><span class="n">myVocabList</span><span class="p">,</span> <span class="n">listOPosts</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">bayes</span><span class="o">.</span><span class="n">setOfWords2Vec</span><span class="p">(</span><span class="n">myVocabList</span><span class="p">,</span> <span class="n">listOPosts</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
<span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
</pre></div>

<blockquote>
<p>训练算法: 从词向量计算概率</p>
</blockquote>
<p>现在已经知道了一个词是否出现在一篇文档中，也知道该文档所属的类别。接下来我们重写贝叶斯准则，将之前的 x, y 替换为 <b>w</b>. 粗体的 <b>w</b> 表示这是一个向量，即它由多个值组成。在这个例子中，数值个数与词汇表中的词个数相同。</p>
<p><img alt="重写贝叶斯准则" src="../resources/4.NaiveBayesian/NB_6.png" /></p>
<p>我们使用上述公式，对每个类计算该值，然后比较这两个概率值的大小。</p>
<p>首先可以通过类别 i (侮辱性留言或者非侮辱性留言)中的文档数除以总的文档数来计算概率 p(ci) 。接下来计算 p(<b>w</b> | ci) ，这里就要用到朴素贝叶斯假设。如果将 w 展开为一个个独立特征，那么就可以将上述概率写作 p(w0, w1, w2...wn | ci) 。这里假设所有词都互相独立，该假设也称作条件独立性假设（例如 A 和 B 两个人抛骰子，概率是互不影响的，也就是相互独立的，A 抛 2点的同时 B 抛 3 点的概率就是 &#8537; * &#8537;），它意味着可以使用 p(w0 | ci)p(w1 | ci)p(w2 | ci)...p(wn | ci) 来计算上述概率，这样就极大地简化了计算的过程。</p>
<p>朴素贝叶斯分类器训练函数</p>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">_trainNB0</span><span class="p">(</span><span class="n">trainMatrix</span><span class="p">,</span> <span class="n">trainCategory</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    训练数据原版</span>
<span class="sd">    :param trainMatrix: 文件单词矩阵 [[1,0,1,1,1....],[],[]...]</span>
<span class="sd">    :param trainCategory: 文件对应的类别[0,1,1,0....]，列表长度等于单词矩阵数，其中的1代表对应的文件是侮辱性文件，0代表不是侮辱性矩阵</span>
<span class="sd">    :return:</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># 文件数</span>
    <span class="n">numTrainDocs</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">trainMatrix</span><span class="p">)</span>
    <span class="c1"># 单词数</span>
    <span class="n">numWords</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">trainMatrix</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="c1"># 侮辱性文件的出现概率，即trainCategory中所有的1的个数，</span>
    <span class="c1"># 代表的就是多少个侮辱性文件，与文件的总数相除就得到了侮辱性文件的出现概率</span>
    <span class="n">pAbusive</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">trainCategory</span><span class="p">)</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">numTrainDocs</span><span class="p">)</span>
    <span class="c1"># 构造单词出现次数列表</span>
    <span class="n">p0Num</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">(</span><span class="n">numWords</span><span class="p">)</span> <span class="c1"># [0,0,0,.....]</span>
    <span class="n">p1Num</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">(</span><span class="n">numWords</span><span class="p">)</span> <span class="c1"># [0,0,0,.....]</span>

    <span class="c1"># 整个数据集单词出现总数</span>
    <span class="n">p0Denom</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">p1Denom</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">numTrainDocs</span><span class="p">):</span>
        <span class="c1"># 是否是侮辱性文件</span>
        <span class="k">if</span> <span class="n">trainCategory</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># 如果是侮辱性文件，对侮辱性文件的向量进行加和</span>
            <span class="n">p1Num</span> <span class="o">+=</span> <span class="n">trainMatrix</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="c1">#[0,1,1,....] + [0,1,1,....]-&gt;[0,2,2,...]</span>
            <span class="c1"># 对向量中的所有元素进行求和，也就是计算所有侮辱性文件中出现的单词总数</span>
            <span class="n">p1Denom</span> <span class="o">+=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">trainMatrix</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">p0Num</span> <span class="o">+=</span> <span class="n">trainMatrix</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">p0Denom</span> <span class="o">+=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">trainMatrix</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="c1"># 类别1，即侮辱性文档的[P(F1|C1),P(F2|C1),P(F3|C1),P(F4|C1),P(F5|C1)....]列表</span>
    <span class="c1"># 即 在1类别下，每个单词出现的概率</span>
    <span class="n">p1Vect</span> <span class="o">=</span> <span class="n">p1Num</span> <span class="o">/</span> <span class="n">p1Denom</span><span class="c1"># [1,2,3,5]/90-&gt;[1/90,...]</span>
    <span class="c1"># 类别0，即正常文档的[P(F1|C0),P(F2|C0),P(F3|C0),P(F4|C0),P(F5|C0)....]列表</span>
    <span class="c1"># 即 在0类别下，每个单词出现的概率</span>
    <span class="n">p0Vect</span> <span class="o">=</span> <span class="n">p0Num</span> <span class="o">/</span> <span class="n">p0Denom</span>
    <span class="k">return</span> <span class="n">p0Vect</span><span class="p">,</span> <span class="n">p1Vect</span><span class="p">,</span> <span class="n">pAbusive</span>
</pre></div>

<blockquote>
<p>测试算法: 根据现实情况修改分类器</p>
</blockquote>
<p>在利用贝叶斯分类器对文档进行分类时，要计算多个概率的乘积以获得文档属于某个类别的概率，即计算 p(w0|1) * p(w1|1) * p(w2|1)。如果其中一个概率值为 0，那么最后的乘积也为 0。为降低这种影响，可以将所有词的出现数初始化为 1，并将分母初始化为 2 （取1 或 2 的目的主要是为了保证分子和分母不为0，大家可以根据业务需求进行更改）。</p>
<p>另一个遇到的问题是下溢出，这是由于太多很小的数相乘造成的。当计算乘积 p(w0|ci) * p(w1|ci) * p(w2|ci)... p(wn|ci) 时，由于大部分因子都非常小，所以程序会下溢出或者得到不正确的答案。（用 Python 尝试相乘许多很小的数，最后四舍五入后会得到 0）。一种解决办法是对乘积取自然对数。在代数中有 ln(a * b) = ln(a) + ln(b), 于是通过求对数可以避免下溢出或者浮点数舍入导致的错误。同时，采用自然对数进行处理不会有任何损失。</p>
<p>下图给出了函数 f(x) 与 ln(f(x)) 的曲线。可以看出，它们在相同区域内同时增加或者减少，并且在相同点上取到极值。它们的取值虽然不同，但不影响最终结果。</p>
<p><img alt="函数图像" src="../resources/4.NaiveBayesian/NB_7.png" /></p>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">trainNB0</span><span class="p">(</span><span class="n">trainMatrix</span><span class="p">,</span> <span class="n">trainCategory</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    训练数据优化版本</span>
<span class="sd">    :param trainMatrix: 文件单词矩阵</span>
<span class="sd">    :param trainCategory: 文件对应的类别</span>
<span class="sd">    :return:</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># 总文件数</span>
    <span class="n">numTrainDocs</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">trainMatrix</span><span class="p">)</span>
    <span class="c1"># 总单词数</span>
    <span class="n">numWords</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">trainMatrix</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="c1"># 侮辱性文件的出现概率</span>
    <span class="n">pAbusive</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">trainCategory</span><span class="p">)</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">numTrainDocs</span><span class="p">)</span>
    <span class="c1"># 构造单词出现次数列表</span>
    <span class="c1"># p0Num 正常的统计</span>
    <span class="c1"># p1Num 侮辱的统计</span>
    <span class="n">p0Num</span> <span class="o">=</span> <span class="n">ones</span><span class="p">(</span><span class="n">numWords</span><span class="p">)</span><span class="c1">#[0,0......]-&gt;[1,1,1,1,1.....]</span>
    <span class="n">p1Num</span> <span class="o">=</span> <span class="n">ones</span><span class="p">(</span><span class="n">numWords</span><span class="p">)</span>

    <span class="c1"># 整个数据集单词出现总数，2.0根据样本/实际调查结果调整分母的值（2主要是避免分母为0，当然值可以调整）</span>
    <span class="c1"># p0Denom 正常的统计</span>
    <span class="c1"># p1Denom 侮辱的统计</span>
    <span class="n">p0Denom</span> <span class="o">=</span> <span class="mf">2.0</span>
    <span class="n">p1Denom</span> <span class="o">=</span> <span class="mf">2.0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">numTrainDocs</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">trainCategory</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># 累加辱骂词的频次</span>
            <span class="n">p1Num</span> <span class="o">+=</span> <span class="n">trainMatrix</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="c1"># 对每篇文章的辱骂的频次 进行统计汇总</span>
            <span class="n">p1Denom</span> <span class="o">+=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">trainMatrix</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">p0Num</span> <span class="o">+=</span> <span class="n">trainMatrix</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">p0Denom</span> <span class="o">+=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">trainMatrix</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="c1"># 类别1，即侮辱性文档的[log(P(F1|C1)),log(P(F2|C1)),log(P(F3|C1)),log(P(F4|C1)),log(P(F5|C1))....]列表</span>
    <span class="n">p1Vect</span> <span class="o">=</span> <span class="n">log</span><span class="p">(</span><span class="n">p1Num</span> <span class="o">/</span> <span class="n">p1Denom</span><span class="p">)</span>
    <span class="c1"># 类别0，即正常文档的[log(P(F1|C0)),log(P(F2|C0)),log(P(F3|C0)),log(P(F4|C0)),log(P(F5|C0))....]列表</span>
    <span class="n">p0Vect</span> <span class="o">=</span> <span class="n">log</span><span class="p">(</span><span class="n">p0Num</span> <span class="o">/</span> <span class="n">p0Denom</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p0Vect</span><span class="p">,</span> <span class="n">p1Vect</span><span class="p">,</span> <span class="n">pAbusive</span>
</pre></div>

<blockquote>
<p>使用算法: 对社区留言板言论进行分类</p>
</blockquote>
<p>朴素贝叶斯分类函数</p>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">classifyNB</span><span class="p">(</span><span class="n">vec2Classify</span><span class="p">,</span> <span class="n">p0Vec</span><span class="p">,</span> <span class="n">p1Vec</span><span class="p">,</span> <span class="n">pClass1</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    使用算法：</span>
<span class="sd">        # 将乘法转换为加法</span>
<span class="sd">        乘法：P(C|F1F2...Fn) = P(F1F2...Fn|C)P(C)/P(F1F2...Fn)</span>
<span class="sd">        加法：P(F1|C)*P(F2|C)....P(Fn|C)P(C) -&gt; log(P(F1|C))+log(P(F2|C))+....+log(P(Fn|C))+log(P(C))</span>
<span class="sd">    :param vec2Classify: 待测数据[0,1,1,1,1...]，即要分类的向量</span>
<span class="sd">    :param p0Vec: 类别0，即正常文档的[log(P(F1|C0)),log(P(F2|C0)),log(P(F3|C0)),log(P(F4|C0)),log(P(F5|C0))....]列表</span>
<span class="sd">    :param p1Vec: 类别1，即侮辱性文档的[log(P(F1|C1)),log(P(F2|C1)),log(P(F3|C1)),log(P(F4|C1)),log(P(F5|C1))....]列表</span>
<span class="sd">    :param pClass1: 类别1，侮辱性文件的出现概率</span>
<span class="sd">    :return: 类别1 or 0</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># 计算公式  log(P(F1|C))+log(P(F2|C))+....+log(P(Fn|C))+log(P(C))</span>
    <span class="c1"># 大家可能会发现，上面的计算公式，没有除以贝叶斯准则的公式的分母，也就是 P(w) （P(w) 指的是此文档在所有的文档中出现的概率）就进行概率大小的比较了，</span>
    <span class="c1"># 因为 P(w) 针对的是包含侮辱和非侮辱的全部文档，所以 P(w) 是相同的。</span>
    <span class="c1"># 使用 NumPy 数组来计算两个向量相乘的结果，这里的相乘是指对应元素相乘，即先将两个向量中的第一个元素相乘，然后将第2个元素相乘，以此类推。</span>
    <span class="c1"># 我的理解是：这里的 vec2Classify * p1Vec 的意思就是将每个词与其对应的概率相关联起来</span>
    <span class="n">p1</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">vec2Classify</span> <span class="o">*</span> <span class="n">p1Vec</span><span class="p">)</span> <span class="o">+</span> <span class="n">log</span><span class="p">(</span><span class="n">pClass1</span><span class="p">)</span> <span class="c1"># P(w|c1) * P(c1) ，即贝叶斯准则的分子</span>
    <span class="n">p0</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">vec2Classify</span> <span class="o">*</span> <span class="n">p0Vec</span><span class="p">)</span> <span class="o">+</span> <span class="n">log</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">pClass1</span><span class="p">)</span> <span class="c1"># P(w|c0) * P(c0) ，即贝叶斯准则的分子·</span>
    <span class="k">if</span> <span class="n">p1</span> <span class="o">&gt;</span> <span class="n">p0</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">0</span>


<span class="k">def</span> <span class="nf">testingNB</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    测试朴素贝叶斯算法</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># 1. 加载数据集</span>
    <span class="n">listOPosts</span><span class="p">,</span> <span class="n">listClasses</span> <span class="o">=</span> <span class="n">loadDataSet</span><span class="p">()</span>
    <span class="c1"># 2. 创建单词集合</span>
    <span class="n">myVocabList</span> <span class="o">=</span> <span class="n">createVocabList</span><span class="p">(</span><span class="n">listOPosts</span><span class="p">)</span>
    <span class="c1"># 3. 计算单词是否出现并创建数据矩阵</span>
    <span class="n">trainMat</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">postinDoc</span> <span class="ow">in</span> <span class="n">listOPosts</span><span class="p">:</span>
        <span class="c1"># 返回m*len(myVocabList)的矩阵， 记录的都是0，1信息</span>
        <span class="n">trainMat</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">setOfWords2Vec</span><span class="p">(</span><span class="n">myVocabList</span><span class="p">,</span> <span class="n">postinDoc</span><span class="p">))</span>
    <span class="c1"># 4. 训练数据</span>
    <span class="n">p0V</span><span class="p">,</span> <span class="n">p1V</span><span class="p">,</span> <span class="n">pAb</span> <span class="o">=</span> <span class="n">trainNB0</span><span class="p">(</span><span class="n">array</span><span class="p">(</span><span class="n">trainMat</span><span class="p">),</span> <span class="n">array</span><span class="p">(</span><span class="n">listClasses</span><span class="p">))</span>
    <span class="c1"># 5. 测试数据</span>
    <span class="n">testEntry</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;love&#39;</span><span class="p">,</span> <span class="s1">&#39;my&#39;</span><span class="p">,</span> <span class="s1">&#39;dalmation&#39;</span><span class="p">]</span>
    <span class="n">thisDoc</span> <span class="o">=</span> <span class="n">array</span><span class="p">(</span><span class="n">setOfWords2Vec</span><span class="p">(</span><span class="n">myVocabList</span><span class="p">,</span> <span class="n">testEntry</span><span class="p">))</span>
    <span class="k">print</span> <span class="n">testEntry</span><span class="p">,</span> <span class="s1">&#39;classified as: &#39;</span><span class="p">,</span> <span class="n">classifyNB</span><span class="p">(</span><span class="n">thisDoc</span><span class="p">,</span> <span class="n">p0V</span><span class="p">,</span> <span class="n">p1V</span><span class="p">,</span> <span class="n">pAb</span><span class="p">)</span>
    <span class="n">testEntry</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;stupid&#39;</span><span class="p">,</span> <span class="s1">&#39;garbage&#39;</span><span class="p">]</span>
    <span class="n">thisDoc</span> <span class="o">=</span> <span class="n">array</span><span class="p">(</span><span class="n">setOfWords2Vec</span><span class="p">(</span><span class="n">myVocabList</span><span class="p">,</span> <span class="n">testEntry</span><span class="p">))</span>
    <span class="k">print</span> <span class="n">testEntry</span><span class="p">,</span> <span class="s1">&#39;classified as: &#39;</span><span class="p">,</span> <span class="n">classifyNB</span><span class="p">(</span><span class="n">thisDoc</span><span class="p">,</span> <span class="n">p0V</span><span class="p">,</span> <span class="n">p1V</span><span class="p">,</span> <span class="n">pAb</span><span class="p">)</span>
</pre></div>

<p><a href="https://github.com/apachecn/MachineLearning/blob/master/src/python/4.NaiveBayes/bayes.py">完整代码地址</a>: <a href="https://github.com/apachecn/MachineLearning/blob/master/src/python/4.NaiveBayes/bayes.py">https://github.com/apachecn/MachineLearning/blob/master/src/python/4.NaiveBayes/bayes.py</a></p>
<h3 id="2">项目案例2: 使用朴素贝叶斯过滤垃圾邮件<a class="headerlink" href="#2" title="Permanent link">&para;</a></h3>
<h4 id="_14">项目概述<a class="headerlink" href="#_14" title="Permanent link">&para;</a></h4>
<p>完成朴素贝叶斯的一个最著名的应用: 电子邮件垃圾过滤。</p>
<h4 id="_15">开发流程<a class="headerlink" href="#_15" title="Permanent link">&para;</a></h4>
<p>使用朴素贝叶斯对电子邮件进行分类</p>
<div class="codehilite"><pre><span></span>收集数据: 提供文本文件
准备数据: 将文本文件解析成词条向量
分析数据: 检查词条确保解析的正确性
训练算法: 使用我们之前建立的 trainNB() 函数
测试算法: 使用朴素贝叶斯进行交叉验证
使用算法: 构建一个完整的程序对一组文档进行分类，将错分的文档输出到屏幕上
</pre></div>

<blockquote>
<p>收集数据: 提供文本文件</p>
</blockquote>
<p>文本文件内容如下: </p>
<div class="codehilite"><pre><span></span>Hi Peter,

With Jose out of town, do you want to
meet once in a while to keep things
going and do some interesting stuff?

Let me know
Eugene
</pre></div>

<blockquote>
<p>准备数据: 将文本文件解析成词条向量</p>
</blockquote>
<p>使用正则表达式来切分文本</p>
<div class="codehilite"><pre><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">mySent</span> <span class="o">=</span> <span class="s1">&#39;This book is the best book on Python or M.L. I have ever laid eyes upon.&#39;</span>
<span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">re</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">regEx</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\\</span><span class="s1">W*&#39;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">listOfTokens</span> <span class="o">=</span> <span class="n">regEx</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">mySent</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">listOfTokens</span>
<span class="p">[</span><span class="s1">&#39;This&#39;</span><span class="p">,</span> <span class="s1">&#39;book&#39;</span><span class="p">,</span> <span class="s1">&#39;is&#39;</span><span class="p">,</span> <span class="s1">&#39;the&#39;</span><span class="p">,</span> <span class="s1">&#39;best&#39;</span><span class="p">,</span> <span class="s1">&#39;book&#39;</span><span class="p">,</span> <span class="s1">&#39;on&#39;</span><span class="p">,</span> <span class="s1">&#39;Python&#39;</span><span class="p">,</span> <span class="s1">&#39;or&#39;</span><span class="p">,</span> <span class="s1">&#39;M.L.&#39;</span><span class="p">,</span> <span class="s1">&#39;I&#39;</span><span class="p">,</span> <span class="s1">&#39;have&#39;</span><span class="p">,</span> <span class="s1">&#39;ever&#39;</span><span class="p">,</span> <span class="s1">&#39;laid&#39;</span><span class="p">,</span> <span class="s1">&#39;eyes&#39;</span><span class="p">,</span> <span class="s1">&#39;upon&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">]</span>
</pre></div>

<blockquote>
<p>分析数据: 检查词条确保解析的正确性</p>
<p>训练算法: 使用我们之前建立的 trainNB0() 函数</p>
</blockquote>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">trainNB0</span><span class="p">(</span><span class="n">trainMatrix</span><span class="p">,</span> <span class="n">trainCategory</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    训练数据优化版本</span>
<span class="sd">    :param trainMatrix: 文件单词矩阵</span>
<span class="sd">    :param trainCategory: 文件对应的类别</span>
<span class="sd">    :return:</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># 总文件数</span>
    <span class="n">numTrainDocs</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">trainMatrix</span><span class="p">)</span>
    <span class="c1"># 总单词数</span>
    <span class="n">numWords</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">trainMatrix</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="c1"># 侮辱性文件的出现概率</span>
    <span class="n">pAbusive</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">trainCategory</span><span class="p">)</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">numTrainDocs</span><span class="p">)</span>
    <span class="c1"># 构造单词出现次数列表</span>
    <span class="c1"># p0Num 正常的统计</span>
    <span class="c1"># p1Num 侮辱的统计</span>
    <span class="n">p0Num</span> <span class="o">=</span> <span class="n">ones</span><span class="p">(</span><span class="n">numWords</span><span class="p">)</span><span class="c1">#[0,0......]-&gt;[1,1,1,1,1.....]</span>
    <span class="n">p1Num</span> <span class="o">=</span> <span class="n">ones</span><span class="p">(</span><span class="n">numWords</span><span class="p">)</span>

    <span class="c1"># 整个数据集单词出现总数，2.0根据样本/实际调查结果调整分母的值（2主要是避免分母为0，当然值可以调整）</span>
    <span class="c1"># p0Denom 正常的统计</span>
    <span class="c1"># p1Denom 侮辱的统计</span>
    <span class="n">p0Denom</span> <span class="o">=</span> <span class="mf">2.0</span>
    <span class="n">p1Denom</span> <span class="o">=</span> <span class="mf">2.0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">numTrainDocs</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">trainCategory</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># 累加辱骂词的频次</span>
            <span class="n">p1Num</span> <span class="o">+=</span> <span class="n">trainMatrix</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="c1"># 对每篇文章的辱骂的频次 进行统计汇总</span>
            <span class="n">p1Denom</span> <span class="o">+=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">trainMatrix</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">p0Num</span> <span class="o">+=</span> <span class="n">trainMatrix</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">p0Denom</span> <span class="o">+=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">trainMatrix</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="c1"># 类别1，即侮辱性文档的[log(P(F1|C1)),log(P(F2|C1)),log(P(F3|C1)),log(P(F4|C1)),log(P(F5|C1))....]列表</span>
    <span class="n">p1Vect</span> <span class="o">=</span> <span class="n">log</span><span class="p">(</span><span class="n">p1Num</span> <span class="o">/</span> <span class="n">p1Denom</span><span class="p">)</span>
    <span class="c1"># 类别0，即正常文档的[log(P(F1|C0)),log(P(F2|C0)),log(P(F3|C0)),log(P(F4|C0)),log(P(F5|C0))....]列表</span>
    <span class="n">p0Vect</span> <span class="o">=</span> <span class="n">log</span><span class="p">(</span><span class="n">p0Num</span> <span class="o">/</span> <span class="n">p0Denom</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p0Vect</span><span class="p">,</span> <span class="n">p1Vect</span><span class="p">,</span> <span class="n">pAbusive</span>
</pre></div>

<blockquote>
<p>测试算法: 使用朴素贝叶斯进行交叉验证</p>
</blockquote>
<p>文件解析及完整的垃圾邮件测试函数</p>
<div class="codehilite"><pre><span></span><span class="c1"># 切分文本</span>
<span class="k">def</span> <span class="nf">textParse</span><span class="p">(</span><span class="n">bigString</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Desc:</span>
<span class="sd">        接收一个大字符串并将其解析为字符串列表</span>
<span class="sd">    Args:</span>
<span class="sd">        bigString -- 大字符串</span>
<span class="sd">    Returns:</span>
<span class="sd">        去掉少于 2 个字符的字符串，并将所有字符串转换为小写，返回字符串列表</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="kn">import</span> <span class="nn">re</span>
    <span class="c1"># 使用正则表达式来切分句子，其中分隔符是除单词、数字外的任意字符串</span>
    <span class="n">listOfTokens</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;\W*&#39;</span><span class="p">,</span> <span class="n">bigString</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">tok</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">tok</span> <span class="ow">in</span> <span class="n">listOfTokens</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">tok</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">spamTest</span><span class="p">():</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Desc:</span>
<span class="sd">        对贝叶斯垃圾邮件分类器进行自动化处理。</span>
<span class="sd">    Args:</span>
<span class="sd">        none</span>
<span class="sd">    Returns:</span>
<span class="sd">        对测试集中的每封邮件进行分类，若邮件分类错误，则错误数加 1，最后返回总的错误百分比。</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">docList</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">classList</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">fullText</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">26</span><span class="p">):</span>
        <span class="c1"># 切分，解析数据，并归类为 1 类别</span>
        <span class="n">wordList</span> <span class="o">=</span> <span class="n">textParse</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="s1">&#39;input/4.NaiveBayes/email/spam/</span><span class="si">%d</span><span class="s1">.txt&#39;</span> <span class="o">%</span> <span class="n">i</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">())</span>
        <span class="n">docList</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">wordList</span><span class="p">)</span>
        <span class="n">classList</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># 切分，解析数据，并归类为 0 类别</span>
        <span class="n">wordList</span> <span class="o">=</span> <span class="n">textParse</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="s1">&#39;input/4.NaiveBayes/email/ham/</span><span class="si">%d</span><span class="s1">.txt&#39;</span> <span class="o">%</span> <span class="n">i</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">())</span>
        <span class="n">docList</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">wordList</span><span class="p">)</span>
        <span class="n">fullText</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">wordList</span><span class="p">)</span>
        <span class="n">classList</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="c1"># 创建词汇表    </span>
    <span class="n">vocabList</span> <span class="o">=</span> <span class="n">createVocabList</span><span class="p">(</span><span class="n">docList</span><span class="p">)</span>
    <span class="n">trainingSet</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>
    <span class="n">testSet</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># 随机取 10 个邮件用来测试</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="c1"># random.uniform(x, y) 随机生成一个范围为 x - y 的实数</span>
        <span class="n">randIndex</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">trainingSet</span><span class="p">)))</span>
        <span class="n">testSet</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">trainingSet</span><span class="p">[</span><span class="n">randIndex</span><span class="p">])</span>
        <span class="k">del</span><span class="p">(</span><span class="n">trainingSet</span><span class="p">[</span><span class="n">randIndex</span><span class="p">])</span>
    <span class="n">trainMat</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">trainClasses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">docIndex</span> <span class="ow">in</span> <span class="n">trainingSet</span><span class="p">:</span>
        <span class="n">trainMat</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">setOfWords2Vec</span><span class="p">(</span><span class="n">vocabList</span><span class="p">,</span> <span class="n">docList</span><span class="p">[</span><span class="n">docIndex</span><span class="p">]))</span>
        <span class="n">trainClasses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">classList</span><span class="p">[</span><span class="n">docIndex</span><span class="p">])</span>
    <span class="n">p0V</span><span class="p">,</span> <span class="n">p1V</span><span class="p">,</span> <span class="n">pSpam</span> <span class="o">=</span> <span class="n">trainNB0</span><span class="p">(</span><span class="n">array</span><span class="p">(</span><span class="n">trainMat</span><span class="p">),</span> <span class="n">array</span><span class="p">(</span><span class="n">trainClasses</span><span class="p">))</span>
    <span class="n">errorCount</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">docIndex</span> <span class="ow">in</span> <span class="n">testSet</span><span class="p">:</span>
        <span class="n">wordVector</span> <span class="o">=</span> <span class="n">setOfWords2Vec</span><span class="p">(</span><span class="n">vocabList</span><span class="p">,</span> <span class="n">docList</span><span class="p">[</span><span class="n">docIndex</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">classifyNB</span><span class="p">(</span><span class="n">array</span><span class="p">(</span><span class="n">wordVector</span><span class="p">),</span> <span class="n">p0V</span><span class="p">,</span> <span class="n">p1V</span><span class="p">,</span> <span class="n">pSpam</span><span class="p">)</span> <span class="o">!=</span> <span class="n">classList</span><span class="p">[</span><span class="n">docIndex</span><span class="p">]:</span>
            <span class="n">errorCount</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">print</span> <span class="s1">&#39;the errorCount is: &#39;</span><span class="p">,</span> <span class="n">errorCount</span>
    <span class="k">print</span> <span class="s1">&#39;the testSet length is :&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">testSet</span><span class="p">)</span>
    <span class="k">print</span> <span class="s1">&#39;the error rate is :&#39;</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="n">errorCount</span><span class="p">)</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">testSet</span><span class="p">)</span>
</pre></div>

<blockquote>
<p>使用算法: 构建一个完整的程序对一组文档进行分类，将错分的文档输出到屏幕上</p>
</blockquote>
<p><a href="https://github.com/apachecn/MachineLearning/blob/master/src/python/4.NaiveBayes/bayes.py">完整代码地址</a>: <a href="https://github.com/apachecn/MachineLearning/blob/master/src/python/4.NaiveBayes/bayes.py">https://github.com/apachecn/MachineLearning/blob/master/src/python/4.NaiveBayes/bayes.py</a></p>
<h3 id="3">项目案例3: 使用朴素贝叶斯分类器从个人广告中获取区域倾向<a class="headerlink" href="#3" title="Permanent link">&para;</a></h3>
<h4 id="_16">项目概述<a class="headerlink" href="#_16" title="Permanent link">&para;</a></h4>
<p>广告商往往想知道关于一个人的一些特定人口统计信息，以便能更好地定向推销广告。</p>
<p>我们将分别从美国的两个城市中选取一些人，通过分析这些人发布的信息，来比较这两个城市的人们在广告用词上是否不同。如果结论确实不同，那么他们各自常用的词是那些，从人们的用词当中，我们能否对不同城市的人所关心的内容有所了解。</p>
<h4 id="_17">开发流程<a class="headerlink" href="#_17" title="Permanent link">&para;</a></h4>
<div class="codehilite"><pre><span></span>收集数据: 从 RSS 源收集内容，这里需要对 RSS 源构建一个接口
准备数据: 将文本文件解析成词条向量
分析数据: 检查词条确保解析的正确性
训练算法: 使用我们之前建立的 trainNB0() 函数
测试算法: 观察错误率，确保分类器可用。可以修改切分程序，以降低错误率，提高分类结果
使用算法: 构建一个完整的程序，封装所有内容。给定两个 RSS 源，改程序会显示最常用的公共词
</pre></div>

<blockquote>
<p>收集数据: 从 RSS 源收集内容，这里需要对 RSS 源构建一个接口</p>
</blockquote>
<p>也就是导入 RSS 源，我们使用 python 下载文本，在http://code.google.com/p/feedparser/ 下浏览相关文档，安装 feedparse，首先解压下载的包，并将当前目录切换到解压文件所在的文件夹，然后在 python 提示符下输入：</p>
<div class="codehilite"><pre><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">python</span> <span class="n">setup</span><span class="o">.</span><span class="n">py</span> <span class="n">install</span>
</pre></div>

<blockquote>
<p>准备数据: 将文本文件解析成词条向量</p>
</blockquote>
<p>文档词袋模型</p>
<p>我们将每个词的出现与否作为一个特征，这可以被描述为 <b>词集模型(set-of-words model)</b>。如果一个词在文档中出现不止一次，这可能意味着包含该词是否出现在文档中所不能表达的某种信息，这种方法被称为 <b>词袋模型(bag-of-words model)</b>。在词袋中，每个单词可以出现多次，而在词集中，每个词只能出现一次。为适应词袋模型，需要对函数 setOfWords2Vec() 稍加修改，修改后的函数为 bagOfWords2Vec() 。</p>
<p>如下给出了基于词袋模型的朴素贝叶斯代码。它与函数 setOfWords2Vec() 几乎完全相同，唯一不同的是每当遇到一个单词时，它会增加词向量中的对应值，而不只是将对应的数值设为 1 。</p>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">bagOfWords2VecMN</span><span class="p">(</span><span class="n">vocaList</span><span class="p">,</span> <span class="n">inputSet</span><span class="p">):</span>
    <span class="n">returnVec</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocabList</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">inputSet</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">inputSet</span><span class="p">:</span>
            <span class="n">returnVec</span><span class="p">[</span><span class="n">vocabList</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">word</span><span class="p">)]</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">returnVec</span>
</pre></div>

<div class="codehilite"><pre><span></span><span class="c1">#创建一个包含在所有文档中出现的不重复词的列表</span>
<span class="k">def</span> <span class="nf">createVocabList</span><span class="p">(</span><span class="n">dataSet</span><span class="p">):</span>
    <span class="n">vocabSet</span><span class="o">=</span><span class="nb">set</span><span class="p">([])</span>    <span class="c1">#创建一个空集</span>
    <span class="k">for</span> <span class="n">document</span> <span class="ow">in</span> <span class="n">dataSet</span><span class="p">:</span>
        <span class="n">vocabSet</span><span class="o">=</span><span class="n">vocabSet</span><span class="o">|</span><span class="nb">set</span><span class="p">(</span><span class="n">document</span><span class="p">)</span>   <span class="c1">#创建两个集合的并集</span>
    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">vocabSet</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">setOfWords2VecMN</span><span class="p">(</span><span class="n">vocabList</span><span class="p">,</span><span class="n">inputSet</span><span class="p">):</span>
    <span class="n">returnVec</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">vocabList</span><span class="p">)</span>  <span class="c1">#创建一个其中所含元素都为0的向量</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">inputSet</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">vocabList</span><span class="p">:</span>
                <span class="n">returnVec</span><span class="p">[</span><span class="n">vocabList</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">word</span><span class="p">)]</span><span class="o">+=</span><span class="mi">1</span>
    <span class="k">return</span> <span class="n">returnVec</span>

<span class="c1">#文件解析</span>
<span class="k">def</span> <span class="nf">textParse</span><span class="p">(</span><span class="n">bigString</span><span class="p">):</span>
    <span class="kn">import</span> <span class="nn">re</span>
    <span class="n">listOfTokens</span><span class="o">=</span><span class="n">re</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;\W*&#39;</span><span class="p">,</span><span class="n">bigString</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">tok</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">tok</span> <span class="ow">in</span> <span class="n">listOfTokens</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">tok</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">2</span><span class="p">]</span>
</pre></div>

<blockquote>
<p>分析数据: 检查词条确保解析的正确性</p>
<p>训练算法: 使用我们之前建立的 trainNB0() 函数</p>
</blockquote>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">trainNB0</span><span class="p">(</span><span class="n">trainMatrix</span><span class="p">,</span> <span class="n">trainCategory</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    训练数据优化版本</span>
<span class="sd">    :param trainMatrix: 文件单词矩阵</span>
<span class="sd">    :param trainCategory: 文件对应的类别</span>
<span class="sd">    :return:</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># 总文件数</span>
    <span class="n">numTrainDocs</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">trainMatrix</span><span class="p">)</span>
    <span class="c1"># 总单词数</span>
    <span class="n">numWords</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">trainMatrix</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="c1"># 侮辱性文件的出现概率</span>
    <span class="n">pAbusive</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">trainCategory</span><span class="p">)</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">numTrainDocs</span><span class="p">)</span>
    <span class="c1"># 构造单词出现次数列表</span>
    <span class="c1"># p0Num 正常的统计</span>
    <span class="c1"># p1Num 侮辱的统计 </span>
    <span class="c1"># 避免单词列表中的任何一个单词为0，而导致最后的乘积为0，所以将每个单词的出现次数初始化为 1</span>
    <span class="n">p0Num</span> <span class="o">=</span> <span class="n">ones</span><span class="p">(</span><span class="n">numWords</span><span class="p">)</span><span class="c1">#[0,0......]-&gt;[1,1,1,1,1.....]</span>
    <span class="n">p1Num</span> <span class="o">=</span> <span class="n">ones</span><span class="p">(</span><span class="n">numWords</span><span class="p">)</span>

    <span class="c1"># 整个数据集单词出现总数，2.0根据样本/实际调查结果调整分母的值（2主要是避免分母为0，当然值可以调整）</span>
    <span class="c1"># p0Denom 正常的统计</span>
    <span class="c1"># p1Denom 侮辱的统计</span>
    <span class="n">p0Denom</span> <span class="o">=</span> <span class="mf">2.0</span>
    <span class="n">p1Denom</span> <span class="o">=</span> <span class="mf">2.0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">numTrainDocs</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">trainCategory</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># 累加辱骂词的频次</span>
            <span class="n">p1Num</span> <span class="o">+=</span> <span class="n">trainMatrix</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="c1"># 对每篇文章的辱骂的频次 进行统计汇总</span>
            <span class="n">p1Denom</span> <span class="o">+=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">trainMatrix</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">p0Num</span> <span class="o">+=</span> <span class="n">trainMatrix</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">p0Denom</span> <span class="o">+=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">trainMatrix</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="c1"># 类别1，即侮辱性文档的[log(P(F1|C1)),log(P(F2|C1)),log(P(F3|C1)),log(P(F4|C1)),log(P(F5|C1))....]列表</span>
    <span class="n">p1Vect</span> <span class="o">=</span> <span class="n">log</span><span class="p">(</span><span class="n">p1Num</span> <span class="o">/</span> <span class="n">p1Denom</span><span class="p">)</span>
    <span class="c1"># 类别0，即正常文档的[log(P(F1|C0)),log(P(F2|C0)),log(P(F3|C0)),log(P(F4|C0)),log(P(F5|C0))....]列表</span>
    <span class="n">p0Vect</span> <span class="o">=</span> <span class="n">log</span><span class="p">(</span><span class="n">p0Num</span> <span class="o">/</span> <span class="n">p0Denom</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p0Vect</span><span class="p">,</span> <span class="n">p1Vect</span><span class="p">,</span> <span class="n">pAbusive</span>
</pre></div>

<blockquote>
<p>测试算法: 观察错误率，确保分类器可用。可以修改切分程序，以降低错误率，提高分类结果</p>
</blockquote>
<div class="codehilite"><pre><span></span><span class="c1">#RSS源分类器及高频词去除函数</span>
<span class="k">def</span> <span class="nf">calcMostFreq</span><span class="p">(</span><span class="n">vocabList</span><span class="p">,</span><span class="n">fullText</span><span class="p">):</span>
    <span class="kn">import</span> <span class="nn">operator</span>
    <span class="n">freqDict</span><span class="o">=</span><span class="p">{}</span>
    <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">vocabList</span><span class="p">:</span>  <span class="c1">#遍历词汇表中的每个词</span>
        <span class="n">freqDict</span><span class="p">[</span><span class="n">token</span><span class="p">]</span><span class="o">=</span><span class="n">fullText</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>  <span class="c1">#统计每个词在文本中出现的次数</span>
    <span class="n">sortedFreq</span><span class="o">=</span><span class="nb">sorted</span><span class="p">(</span><span class="n">freqDict</span><span class="o">.</span><span class="n">iteritems</span><span class="p">(),</span><span class="n">key</span><span class="o">=</span><span class="n">operator</span><span class="o">.</span><span class="n">itemgetter</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span><span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>  <span class="c1">#根据每个词出现的次数从高到底对字典进行排序</span>
    <span class="k">return</span> <span class="n">sortedFreq</span><span class="p">[:</span><span class="mi">30</span><span class="p">]</span>   <span class="c1">#返回出现次数最高的30个单词</span>
<span class="k">def</span> <span class="nf">localWords</span><span class="p">(</span><span class="n">feed1</span><span class="p">,</span><span class="n">feed0</span><span class="p">):</span>
    <span class="kn">import</span> <span class="nn">feedparser</span>
    <span class="n">docList</span><span class="o">=</span><span class="p">[];</span><span class="n">classList</span><span class="o">=</span><span class="p">[];</span><span class="n">fullText</span><span class="o">=</span><span class="p">[]</span>
    <span class="n">minLen</span><span class="o">=</span><span class="nb">min</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">feed1</span><span class="p">[</span><span class="s1">&#39;entries&#39;</span><span class="p">]),</span><span class="nb">len</span><span class="p">(</span><span class="n">feed0</span><span class="p">[</span><span class="s1">&#39;entries&#39;</span><span class="p">]))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">minLen</span><span class="p">):</span>
        <span class="n">wordList</span><span class="o">=</span><span class="n">textParse</span><span class="p">(</span><span class="n">feed1</span><span class="p">[</span><span class="s1">&#39;entries&#39;</span><span class="p">][</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;summary&#39;</span><span class="p">])</span>   <span class="c1">#每次访问一条RSS源</span>
        <span class="n">docList</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">wordList</span><span class="p">)</span>
        <span class="n">fullText</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">wordList</span><span class="p">)</span>
        <span class="n">classList</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">wordList</span><span class="o">=</span><span class="n">textParse</span><span class="p">(</span><span class="n">feed0</span><span class="p">[</span><span class="s1">&#39;entries&#39;</span><span class="p">][</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;summary&#39;</span><span class="p">])</span>
        <span class="n">docList</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">wordList</span><span class="p">)</span>
        <span class="n">fullText</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">wordList</span><span class="p">)</span>
        <span class="n">classList</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">vocabList</span><span class="o">=</span><span class="n">createVocabList</span><span class="p">(</span><span class="n">docList</span><span class="p">)</span>
    <span class="n">top30Words</span><span class="o">=</span><span class="n">calcMostFreq</span><span class="p">(</span><span class="n">vocabList</span><span class="p">,</span><span class="n">fullText</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">pairW</span> <span class="ow">in</span> <span class="n">top30Words</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">pairW</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">in</span> <span class="n">vocabList</span><span class="p">:</span><span class="n">vocabList</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">pairW</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>    <span class="c1">#去掉出现次数最高的那些词</span>
    <span class="n">trainingSet</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">minLen</span><span class="p">);</span><span class="n">testSet</span><span class="o">=</span><span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
        <span class="n">randIndex</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">trainingSet</span><span class="p">)))</span>
        <span class="n">testSet</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">trainingSet</span><span class="p">[</span><span class="n">randIndex</span><span class="p">])</span>
        <span class="k">del</span><span class="p">(</span><span class="n">trainingSet</span><span class="p">[</span><span class="n">randIndex</span><span class="p">])</span>
    <span class="n">trainMat</span><span class="o">=</span><span class="p">[];</span><span class="n">trainClasses</span><span class="o">=</span><span class="p">[]</span>
    <span class="k">for</span> <span class="n">docIndex</span> <span class="ow">in</span> <span class="n">trainingSet</span><span class="p">:</span>
        <span class="n">trainMat</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">bagOfWords2VecMN</span><span class="p">(</span><span class="n">vocabList</span><span class="p">,</span><span class="n">docList</span><span class="p">[</span><span class="n">docIndex</span><span class="p">]))</span>
        <span class="n">trainClasses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">classList</span><span class="p">[</span><span class="n">docIndex</span><span class="p">])</span>
    <span class="n">p0V</span><span class="p">,</span><span class="n">p1V</span><span class="p">,</span><span class="n">pSpam</span><span class="o">=</span><span class="n">trainNBO</span><span class="p">(</span><span class="n">array</span><span class="p">(</span><span class="n">trainMat</span><span class="p">),</span><span class="n">array</span><span class="p">(</span><span class="n">trainClasses</span><span class="p">))</span>
    <span class="n">errorCount</span><span class="o">=</span><span class="mi">0</span>
    <span class="k">for</span> <span class="n">docIndex</span> <span class="ow">in</span> <span class="n">testSet</span><span class="p">:</span>
        <span class="n">wordVector</span><span class="o">=</span><span class="n">bagOfWords2VecMN</span><span class="p">(</span><span class="n">vocabList</span><span class="p">,</span><span class="n">docList</span><span class="p">[</span><span class="n">docIndex</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">classifyNB</span><span class="p">(</span><span class="n">array</span><span class="p">(</span><span class="n">wordVector</span><span class="p">),</span><span class="n">p0V</span><span class="p">,</span><span class="n">p1V</span><span class="p">,</span><span class="n">pSpam</span><span class="p">)</span><span class="o">!=</span><span class="n">classList</span><span class="p">[</span><span class="n">docIndex</span><span class="p">]:</span>
            <span class="n">errorCount</span><span class="o">+=</span><span class="mi">1</span>
    <span class="k">print</span> <span class="s1">&#39;the error rate is:&#39;</span><span class="p">,</span><span class="nb">float</span><span class="p">(</span><span class="n">errorCount</span><span class="p">)</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">testSet</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">vocabList</span><span class="p">,</span><span class="n">p0V</span><span class="p">,</span><span class="n">p1V</span>

<span class="c1">#朴素贝叶斯分类函数</span>
<span class="k">def</span> <span class="nf">classifyNB</span><span class="p">(</span><span class="n">vec2Classify</span><span class="p">,</span><span class="n">p0Vec</span><span class="p">,</span><span class="n">p1Vec</span><span class="p">,</span><span class="n">pClass1</span><span class="p">):</span>
    <span class="n">p1</span><span class="o">=</span><span class="nb">sum</span><span class="p">(</span><span class="n">vec2Classify</span><span class="o">*</span><span class="n">p1Vec</span><span class="p">)</span><span class="o">+</span><span class="n">log</span><span class="p">(</span><span class="n">pClass1</span><span class="p">)</span>
    <span class="n">p0</span><span class="o">=</span><span class="nb">sum</span><span class="p">(</span><span class="n">vec2Classify</span><span class="o">*</span><span class="n">p0Vec</span><span class="p">)</span><span class="o">+</span><span class="n">log</span><span class="p">(</span><span class="mf">1.0</span><span class="o">-</span><span class="n">pClass1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p1</span><span class="o">&gt;</span><span class="n">p0</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">0</span>
</pre></div>

<blockquote>
<p>使用算法: 构建一个完整的程序，封装所有内容。给定两个 RSS 源，改程序会显示最常用的公共词</p>
</blockquote>
<p>函数 localWords() 使用了两个 RSS 源作为参数，RSS 源要在函数外导入，这样做的原因是 RSS 源会随时间而改变，重新加载 RSS 源就会得到新的数据</p>
<p><div class="codehilite"><pre><span></span><span class="o">&gt;&gt;&gt;</span> <span class="nb">reload</span><span class="p">(</span><span class="n">bayes</span><span class="p">)</span>
<span class="o">&lt;</span><span class="n">module</span> <span class="s1">&#39;bayes&#39;</span> <span class="kn">from</span> <span class="s1">&#39;bayes.pyc&#39;</span><span class="o">&gt;</span>
<span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">feedparser</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">ny</span><span class="o">=</span><span class="n">feedparser</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="s1">&#39;http://newyork.craigslist.org/stp/index.rss&#39;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">sy</span><span class="o">=</span><span class="n">feedparser</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="s1">&#39;http://sfbay.craigslist.org/stp/index.rss&#39;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">vocabList</span><span class="p">,</span><span class="n">pSF</span><span class="p">,</span><span class="n">pNY</span><span class="o">=</span><span class="n">bayes</span><span class="o">.</span><span class="n">localWords</span><span class="p">(</span><span class="n">ny</span><span class="p">,</span><span class="n">sf</span><span class="p">)</span>
<span class="n">the</span> <span class="n">error</span> <span class="n">rate</span> <span class="ow">is</span><span class="p">:</span> <span class="mf">0.2</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">vocabList</span><span class="p">,</span><span class="n">pSF</span><span class="p">,</span><span class="n">pNY</span><span class="o">=</span><span class="n">bayes</span><span class="o">.</span><span class="n">localWords</span><span class="p">(</span><span class="n">ny</span><span class="p">,</span><span class="n">sf</span><span class="p">)</span>
<span class="n">the</span> <span class="n">error</span> <span class="n">rate</span> <span class="ow">is</span><span class="p">:</span> <span class="mf">0.3</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">vocabList</span><span class="p">,</span><span class="n">pSF</span><span class="p">,</span><span class="n">pNY</span><span class="o">=</span><span class="n">bayes</span><span class="o">.</span><span class="n">localWords</span><span class="p">(</span><span class="n">ny</span><span class="p">,</span><span class="n">sf</span><span class="p">)</span>
<span class="n">the</span> <span class="n">error</span> <span class="n">rate</span> <span class="ow">is</span><span class="p">:</span> <span class="mf">0.55</span>
</pre></div>
为了得到错误率的精确估计，应该多次进行上述实验，然后取平均值</p>
<p>接下来，我们要分析一下数据，显示地域相关的用词</p>
<p>可以先对向量pSF与pNY进行排序，然后按照顺序打印出来，将下面的代码添加到文件中：</p>
<div class="codehilite"><pre><span></span><span class="c1">#最具表征性的词汇显示函数</span>
<span class="k">def</span> <span class="nf">getTopWords</span><span class="p">(</span><span class="n">ny</span><span class="p">,</span><span class="n">sf</span><span class="p">):</span>
    <span class="kn">import</span> <span class="nn">operator</span>
    <span class="n">vocabList</span><span class="p">,</span><span class="n">p0V</span><span class="p">,</span><span class="n">p1V</span><span class="o">=</span><span class="n">localWords</span><span class="p">(</span><span class="n">ny</span><span class="p">,</span><span class="n">sf</span><span class="p">)</span>
    <span class="n">topNY</span><span class="o">=</span><span class="p">[];</span><span class="n">topSF</span><span class="o">=</span><span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">p0V</span><span class="p">)):</span>
        <span class="k">if</span> <span class="n">p0V</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">&gt;-</span><span class="mf">6.0</span><span class="p">:</span><span class="n">topSF</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">vocabList</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">p0V</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
        <span class="k">if</span> <span class="n">p1V</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">&gt;-</span><span class="mf">6.0</span><span class="p">:</span><span class="n">topNY</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">vocabList</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">p1V</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
    <span class="n">sortedSF</span><span class="o">=</span><span class="nb">sorted</span><span class="p">(</span><span class="n">topSF</span><span class="p">,</span><span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">pair</span><span class="p">:</span><span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="k">print</span> <span class="s2">&quot;SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**&quot;</span>
    <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">sortedSF</span><span class="p">:</span>
        <span class="k">print</span> <span class="n">item</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">sortedNY</span><span class="o">=</span><span class="nb">sorted</span><span class="p">(</span><span class="n">topNY</span><span class="p">,</span><span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">pair</span><span class="p">:</span><span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="k">print</span> <span class="s2">&quot;NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**&quot;</span>
    <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">sortedNY</span><span class="p">:</span>
        <span class="k">print</span> <span class="n">item</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>

<p>函数 getTopWords() 使用两个 RSS 源作为输入，然后训练并测试朴素贝叶斯分类器，返回使用的概率值。然后创建两个列表用于元组的存储，与之前返回排名最高的 X 个单词不同，这里可以返回大于某个阈值的所有词，这些元组会按照它们的条件概率进行排序。</p>
<p>保存 bayes.py 文件，在python提示符下输入：</p>
<div class="codehilite"><pre><span></span><span class="o">&gt;&gt;&gt;</span> <span class="nb">reload</span><span class="p">(</span><span class="n">bayes</span><span class="p">)</span>
<span class="o">&lt;</span><span class="n">module</span> <span class="s1">&#39;bayes&#39;</span> <span class="kn">from</span> <span class="s1">&#39;bayes.pyc&#39;</span><span class="o">&gt;</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">bayes</span><span class="o">.</span><span class="n">getTopWords</span><span class="p">(</span><span class="n">ny</span><span class="p">,</span><span class="n">sf</span><span class="p">)</span>
<span class="n">the</span> <span class="n">error</span> <span class="n">rate</span> <span class="ow">is</span><span class="p">:</span> <span class="mf">0.55</span>
<span class="n">SF</span><span class="o">**</span><span class="n">SF</span><span class="o">**</span><span class="n">SF</span><span class="o">**</span><span class="n">SF</span><span class="o">**</span><span class="n">SF</span><span class="o">**</span><span class="n">SF</span><span class="o">**</span><span class="n">SF</span><span class="o">**</span><span class="n">SF</span><span class="o">**</span><span class="n">SF</span><span class="o">**</span><span class="n">SF</span><span class="o">**</span><span class="n">SF</span><span class="o">**</span><span class="n">SF</span><span class="o">**</span><span class="n">SF</span><span class="o">**</span><span class="n">SF</span><span class="o">**</span>
<span class="n">how</span>
<span class="n">last</span>
<span class="n">man</span>
<span class="o">...</span>
<span class="n">veteran</span>
<span class="n">still</span>
<span class="n">ends</span>
<span class="n">late</span>
<span class="n">off</span>
<span class="n">own</span>
<span class="n">know</span>
<span class="n">NY</span><span class="o">**</span><span class="n">NY</span><span class="o">**</span><span class="n">NY</span><span class="o">**</span><span class="n">NY</span><span class="o">**</span><span class="n">NY</span><span class="o">**</span><span class="n">NY</span><span class="o">**</span><span class="n">NY</span><span class="o">**</span><span class="n">NY</span><span class="o">**</span><span class="n">NY</span><span class="o">**</span><span class="n">NY</span><span class="o">**</span><span class="n">NY</span><span class="o">**</span><span class="n">NY</span><span class="o">**</span><span class="n">NY</span><span class="o">**</span><span class="n">NY</span><span class="o">**</span>
<span class="n">someone</span>
<span class="n">meet</span>
<span class="o">...</span>
<span class="n">apparel</span>
<span class="n">recalled</span>
<span class="n">starting</span>
<span class="n">strings</span>
</pre></div>

<p>当注释掉用于移除高频词的三行代码，然后比较注释前后的分类性能，去掉这几行代码之后，错误率为54%，，而保留这些代码得到的错误率为70%。这里观察到，这些留言中出现次数最多的前30个词涵盖了所有用词的30%，vocabList的大小约为3000个词，也就是说，词汇表中的一小部分单词却占据了所有文本用词的一大部分。产生这种现象的原因是因为语言中大部分都是冗余和结构辅助性内容。另一个常用的方法是不仅移除高频词，同时从某个预定高频词中移除结构上的辅助词，该词表称为停用词表。</p>
<p>最后输出的单词，可以看出程序输出了大量的停用词，可以移除固定的停用词看看结果如何，这样做的花，分类错误率也会降低。</p>
<p><a href="https://github.com/apachecn/MachineLearning/blob/master/src/python/4.NaiveBayes/bayes.py">完整代码地址</a>: <a href="https://github.com/apachecn/MachineLearning/blob/master/src/python/4.NaiveBayes/bayes.py">https://github.com/apachecn/MachineLearning/blob/master/src/python/4.NaiveBayes/bayes.py</a></p>
<hr />
<ul>
<li><strong>作者：<a href="http://www.apache.wiki/display/~xuxin">羊三</a> <a href="http://www.apache.wiki/display/~chenyao">小瑶</a></strong></li>
<li><a href="https://github.com/apachecn/MachineLearning">GitHub地址</a>: <a href="https://github.com/apachecn/MachineLearning">https://github.com/apachecn/MachineLearning</a></li>
<li><strong>版权声明：欢迎转载学习 =&gt; 请标注信息来源于 <a href="http://www.apachecn.org/">ApacheCN</a></strong></li>
</ul></div>
        </div>

        <footer class="col-md-12">
            <hr>
                <p>Copyright &copy; 2016 - 2018 Martin Donath</p>
            <p>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>var base_url = '../../..';</script>
        <script src="../../../js/base.js"></script>
        <script src="../../../javascripts/extra.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
        <script src="../../../search/require.js"></script>
        <script src="../../../search/search.js"></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="Search Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Search</h4>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form role="form">
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="Keyboard Shortcuts Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Keyboard Shortcuts</h4>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td><kbd>&larr;</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td><kbd>&rarr;</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>


    </body>
</html>

[参考网址](http://blog.csdn.net/u014595019/article/details/52562159)
## 损失函数

在之前的内容中，我们用的损失函数都是平方差函数，即 

$$C=\frac{1}{2}(a−y)^2$$

其中y是我们期望的输出，$a$为神经元的实际输出$a=\sigma(Wx+b)$。也就是说，当神经元的实际输出与我们的期望输出差距越大，代价就越高。想法非常的好，然而在实际应用中，我们知道参数的修正是与$\frac{\partial{C}}{\partial{W}}$和$\frac{\partial{C}}{\partial{b}}$成正比的，而根据 

$$
\frac{\partial{C}}{\partial{W}}=(a-y)\sigma'(a)x^T\\
\frac{\partial{C}}{\partial{b}}=(a-y)\sigma'(a)
$$

我们发现其中都有$\sigma'(a)$这一项。因为$sigmoid$函数的性质，导致$\sigma'(z)$在$z$取大部分值时会造成饱和现象，从而使得参数的更新速度非常慢，甚至会造成离期望值越远，更新越慢的现象。那么怎么克服这个问题呢？我们想到了交叉熵函数。我们知道，熵的计算公式是 

$$H(y)=- \sum_i {y_{i} log{y_i}}$$

而在实际操作中，我们并不知道y的分布，只能对y的分布做一个估计，也就是算得的a值, 这样我们就能够得到用a来表示y的交叉熵 

$$H(y,a)=- \sum_i {y_{i} log{a_i}}$$

如果有多个样本，则整个样本的平均交叉熵为 

$$H(y,a)=- \frac{1}{n}\sum_n \sum_i {y_{i,n} \log{a_{i,n}}}$$

其中$n$表示样本编号,$i$表示类别编。 如果用于logistic分类，则上式可以简化成 

$$H(y,a)=- \frac{1}{n}\sum_n{y\log{a}}+(1-y)\log{(1-a)}$$

与平方损失函数相比，交叉熵函数有个非常好的特质， 

$$H'=\frac{1}{n}\sum{(a_n-y_n)}=\frac{1}{n}\sum{(\sigma(z_n)-y_n)}$$

可以看到其中没有了$\sigma'$这一项，这样一来也就不会受到饱和性的影响了。当误差大的时候，权重更新就快，当误差小的时候，权重的更新就慢。这是一个很好的性质。
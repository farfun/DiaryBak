<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        <meta name="author" content="NiuLiangtao">
        <link rel="canonical" href="https://1007530194.github.io/Diary/学习/转载/cs231n-en/en-optimization-2/">
        <link rel="shortcut icon" href="../../../../img/favicon.ico">
        <title>optimization-2 - Blog of NiuLiangtao</title>
        <link href="../../../../css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="../../../../css/font-awesome-4.5.0.css" rel="stylesheet">
        <link href="../../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="../../../../css/highlight.css">
        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
        <![endif]-->

        <script src="../../../../js/jquery-1.10.2.min.js"></script>
        <script src="../../../../js/bootstrap-3.0.3.min.js"></script>
        <script src="../../../../js/highlight.pack.js"></script> 
    </head>

    <body>

        <div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">

        <!-- Collapsed navigation -->
        <div class="navbar-header">
            <!-- Expander button -->
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="../../../../ToDo/">Blog of NiuLiangtao</a>
        </div>

        <!-- Expanded navigation -->
        <div class="navbar-collapse collapse">
                <!-- Main navigation -->
                <ul class="nav navbar-nav">
                    <li >
                        <a href="../../../../ToDo/">Home</a>
                    </li>
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Home <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                            
<li >
    <a href="../../../../AboutMe/">About</a>
</li>
                            
<li >
    <a href="../../../..">Home</a>
</li>
                            
<li >
    <a href="../../../../Nothing/">Nothing</a>
</li>
                            
<li >
    <a href="../../../../ToDo/">要做的</a>
</li>
                        </ul>
                    </li>
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">书籍 <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                            
<li >
    <a href="../../../../书籍/">Home</a>
</li>
                            
  <li class="dropdown-submenu">
    <a href="#">机器学习实战</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../../../书籍/机器学习实战/00naive-bayes-discuss/">naive-bayes-discuss</a>
</li>
            
<li >
    <a href="../../../../书籍/机器学习实战/01.机器学习基础/">1.机器学习基础</a>
</li>
            
<li >
    <a href="../../../../书籍/机器学习实战/02.k-近邻算法/">2.k-近邻算法</a>
</li>
            
<li >
    <a href="../../../../书籍/机器学习实战/03.决策树/">3.决策树</a>
</li>
            
<li >
    <a href="../../../../书籍/机器学习实战/04.朴素贝叶斯/">4.朴素贝叶斯</a>
</li>
            
<li >
    <a href="../../../../书籍/机器学习实战/05.Logistic回归/">5.Logistic回归</a>
</li>
            
<li >
    <a href="../../../../书籍/机器学习实战/06.0.支持向量机/">6.支持向量机</a>
</li>
            
<li >
    <a href="../../../../书籍/机器学习实战/06.1.支持向量机的几个通俗理解/">6.1.支持向量机的几个通俗理解</a>
</li>
            
<li >
    <a href="../../../../书籍/机器学习实战/07.集成方法-随机森林和AdaBoost/">7.集成方法-随机森林和AdaBoost</a>
</li>
            
<li >
    <a href="../../../../书籍/机器学习实战/08.预测数值型数据-回归/">8.预测数值型数据：回归</a>
</li>
            
<li >
    <a href="../../../../书籍/机器学习实战/09.树回归/">9.树回归</a>
</li>
            
<li >
    <a href="../../../../书籍/机器学习实战/10.k-means聚类/">10.k-means聚类</a>
</li>
            
<li >
    <a href="../../../../书籍/机器学习实战/11.使用Apriori算法进行关联分析/">11.使用Apriori算法进行关联分析</a>
</li>
            
<li >
    <a href="../../../../书籍/机器学习实战/12.使用FP-growth算法来高效发现频繁项集/">12.使用FP-growth算法来高效发现频繁项集</a>
</li>
            
<li >
    <a href="../../../../书籍/机器学习实战/13.利用PCA来简化数据/">13.利用PCA来简化数据</a>
</li>
            
<li >
    <a href="../../../../书籍/机器学习实战/14.利用SVD简化数据/">14.利用SVD简化数据</a>
</li>
            
<li >
    <a href="../../../../书籍/机器学习实战/15.大数据与MapReduce/">15.大数据与MapReduce</a>
</li>
            
<li >
    <a href="../../../../书籍/机器学习实战/16.推荐系统/">16.推荐系统</a>
</li>
    </ul>
  </li>
                            
  <li class="dropdown-submenu">
    <a href="#">深度学习</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../../../书籍/深度学习/DeepLearning/">深度学习中文版</a>
</li>
    </ul>
  </li>
                        </ul>
                    </li>
                    <li class="dropdown active">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">学习 <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                            
<li >
    <a href="../../../">Home</a>
</li>
                            
  <li class="dropdown-submenu">
    <a href="#">tensorflow</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../../tensorflow/如何选择优化器 optimizer/">如何选择优化器 optimizer</a>
</li>
    </ul>
  </li>
                            
  <li class="dropdown-submenu">
    <a href="#">深度学习</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../../深度学习/主成分分析-2/">主成分分析 (2)</a>
</li>
            
<li >
    <a href="../../../深度学习/主成分分析/">主成分分析</a>
</li>
            
<li >
    <a href="../../../深度学习/最小二乘法/">最小二乘法</a>
</li>
            
<li >
    <a href="../../../深度学习/矩阵分解/">矩阵分解</a>
</li>
            
<li >
    <a href="../../../深度学习/矩阵求导/">矩阵求导</a>
</li>
    </ul>
  </li>
                            
  <li class="dropdown-submenu">
    <a href="#">转载</a>
    <ul class="dropdown-menu">
            
  <li class="dropdown-submenu">
    <a href="#">cs231n-ch</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../cs231n-ch/ch-assignment2_google_cloud/">Google Cloud Tutorial Part 2 (with GPUs)</a>
</li>
            
<li >
    <a href="../../cs231n-ch/ch-aws-tutorial/">aws-tutorial</a>
</li>
            
<li >
    <a href="../../cs231n-ch/ch-classification/">classification</a>
</li>
            
<li >
    <a href="../../cs231n-ch/ch-convnet-tips/">convnet-tips</a>
</li>
            
<li >
    <a href="../../cs231n-ch/ch-convolutional-networks/">convolutional-networks</a>
</li>
            
<li >
    <a href="../../cs231n-ch/ch-google_cloud_tutorial/">Google Cloud Tutorial gce-tutorial</a>
</li>
            
<li >
    <a href="../../cs231n-ch/ch-ipython-tutorial/">IPython Tutorial ipython-tutorial</a>
</li>
            
<li >
    <a href="../../cs231n-ch/ch-linear-classify/">线性分类</a>
</li>
            
<li >
    <a href="../../cs231n-ch/ch-neural-networks-1/">neural-networks-1</a>
</li>
            
<li >
    <a href="../../cs231n-ch/ch-neural-networks-2/">neural-networks-2</a>
</li>
            
<li >
    <a href="../../cs231n-ch/ch-neural-networks-3/">neural-networks-3</a>
</li>
            
<li >
    <a href="../../cs231n-ch/ch-neural-networks-case-study/">neural-networks-case-study</a>
</li>
            
<li >
    <a href="../../cs231n-ch/ch-optimization-1/">optimization-1</a>
</li>
            
<li >
    <a href="../../cs231n-ch/ch-optimization-2/">optimization-2</a>
</li>
            
<li >
    <a href="../../cs231n-ch/ch-overview/">Overview of Computer Vision and Visual Recognition overview</a>
</li>
            
<li >
    <a href="../../cs231n-ch/ch-poster/">CS231n Poster Session poster-session</a>
</li>
            
<li >
    <a href="../../cs231n-ch/ch-python-numpy-tutorial/">Python Numpy Tutorial python-numpy-tutorial</a>
</li>
            
<li >
    <a href="../../cs231n-ch/ch-Readme/">ch Readme</a>
</li>
            
<li >
    <a href="../../cs231n-ch/ch-transfer-learning/">transfer-learning</a>
</li>
            
<li >
    <a href="../../cs231n-ch/ch-understanding-cnn/">understanding-cnn</a>
</li>
            
<li >
    <a href="../../cs231n-ch/terminal-tutorial/">Terminal.com Tutorial terminal-tutorial</a>
</li>
    </ul>
  </li>
            
  <li class="dropdown-submenu">
    <a href="#">cs231n-en</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../en-assignment2_google_cloud/">Google Cloud Tutorial Part 2 (with GPUs)</a>
</li>
            
<li >
    <a href="../en-aws-tutorial/">aws-tutorial</a>
</li>
            
<li >
    <a href="../en-classification/">classification</a>
</li>
            
<li >
    <a href="../en-convnet-tips/">convnet-tips</a>
</li>
            
<li >
    <a href="../en-convolutional-networks/">convolutional-networks</a>
</li>
            
<li >
    <a href="../en-google_cloud_tutorial/">Google Cloud Tutorial</a>
</li>
            
<li >
    <a href="../en-ipython-tutorial/">IPython Tutorial ipython-tutorial</a>
</li>
            
<li >
    <a href="../en-linear-classify/">linear-classify</a>
</li>
            
<li >
    <a href="../en-neural-networks-1/">neural-networks-1</a>
</li>
            
<li >
    <a href="../en-neural-networks-2/">neural-networks-2</a>
</li>
            
<li >
    <a href="../en-neural-networks-3/">neural-networks-3</a>
</li>
            
<li >
    <a href="../en-neural-networks-case-study/">neural-networks-case-study</a>
</li>
            
<li >
    <a href="../en-optimization-1/">optimization-1</a>
</li>
            
<li class="active">
    <a href="./">optimization-2</a>
</li>
            
<li >
    <a href="../en-overview/">En overview</a>
</li>
            
<li >
    <a href="../en-poster/">En poster</a>
</li>
            
<li >
    <a href="../en-python-numpy-tutorial/">En python numpy tutorial</a>
</li>
            
<li >
    <a href="../en-Readme/">en Readme</a>
</li>
            
<li >
    <a href="../en-terminal-tutorial/">En terminal tutorial</a>
</li>
            
<li >
    <a href="../en-transfer-learning/">En transfer learning</a>
</li>
            
<li >
    <a href="../en-understanding-cnn/">En understanding cnn</a>
</li>
    </ul>
  </li>
            
  <li class="dropdown-submenu">
    <a href="#">example</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../example/a-web-crawer-with-a-asyncio-coroutines-1/">A web crawer with a asyncio coroutines 1</a>
</li>
            
<li >
    <a href="../../example/a-web-crawer-with-a-asyncio-coroutines-2/">A web crawer with a asyncio coroutines 2</a>
</li>
            
<li >
    <a href="../../example/bias-variance/">Bias variance</a>
</li>
            
<li >
    <a href="../../example/bitcoin-explained-1/">Bitcoin explained 1</a>
</li>
            
<li >
    <a href="../../example/dive-into-gradient-decent/">Dive into gradient decent</a>
</li>
            
<li >
    <a href="../../example/ethereum-ultimate-guide/">Ethereum ultimate guide</a>
</li>
            
<li >
    <a href="../../example/fork-exec-source/">Fork exec source</a>
</li>
            
<li >
    <a href="../../example/latex语法/">Latex语法</a>
</li>
            
<li >
    <a href="../../example/latex语法1/">Latex语法1</a>
</li>
            
<li >
    <a href="../../example/nosql-in-python/">Nosql in python</a>
</li>
            
<li >
    <a href="../../example/perceptron/">Perceptron</a>
</li>
            
<li >
    <a href="../../example/quick-latex/">Quick latex</a>
</li>
            
<li >
    <a href="../../example/tendermint/">Tendermint</a>
</li>
    </ul>
  </li>
            
  <li class="dropdown-submenu">
    <a href="#">example2</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../example2/arrays-similar/">Arrays similar</a>
</li>
            
<li >
    <a href="../../example2/baidu-ife-1/">Baidu ife 1</a>
</li>
            
<li >
    <a href="../../example2/create-my-blog-with-jekyll/">Create my blog with jekyll</a>
</li>
            
<li >
    <a href="../../example2/front-end-tools/">Front end tools</a>
</li>
            
<li >
    <a href="../../example2/git-clone-not-master-branch/">Git clone not master branch</a>
</li>
            
<li >
    <a href="../../example2/History-API/">History API</a>
</li>
            
<li >
    <a href="../../example2/how-to-use-babel/">How to use babel</a>
</li>
            
<li >
    <a href="../../example2/how-to-write-a-count-down/">How to write a count down</a>
</li>
            
<li >
    <a href="../../example2/JavaScript-closure/">JavaScript closure</a>
</li>
            
<li >
    <a href="../../example2/JavaScript-function/">JavaScript function</a>
</li>
            
<li >
    <a href="../../example2/JavaScript-good-parts-note1/">JavaScript good parts note1</a>
</li>
            
<li >
    <a href="../../example2/JavaScript-good-parts-note2/">JavaScript good parts note2</a>
</li>
            
<li >
    <a href="../../example2/JavaScript-good-parts-note3/">JavaScript good parts note3</a>
</li>
            
<li >
    <a href="../../example2/JavaScript-Net/">JavaScript Net</a>
</li>
            
<li >
    <a href="../../example2/JavaScript-Object-Oriented/">JavaScript Object Oriented</a>
</li>
            
<li >
    <a href="../../example2/JavaScript-this/">JavaScript this</a>
</li>
            
<li >
    <a href="../../example2/jekyll-theme-version-2.0/">Jekyll theme version 2.0</a>
</li>
            
<li >
    <a href="../../example2/js-create-file-and-download/">Js create file and download</a>
</li>
            
<li >
    <a href="../../example2/low-IE-click-empty-block-bug/">low IE click empty block bug</a>
</li>
            
<li >
    <a href="../../example2/regular-expression-group/">Regular expression group</a>
</li>
            
<li >
    <a href="../../example2/scope/">Scope</a>
</li>
            
<li >
    <a href="../../example2/shuffle-algorithm/">Shuffle algorithm</a>
</li>
            
<li >
    <a href="../../example2/sublimeLinter/">sublimeLinter</a>
</li>
            
<li >
    <a href="../../example2/Syncing-a-fork/">Syncing a fork</a>
</li>
            
<li >
    <a href="../../example2/teach-girlfriend-html-css/">Teach girlfriend html css</a>
</li>
            
<li >
    <a href="../../example2/web-app/">Web app</a>
</li>
            
<li >
    <a href="../../example2/weinre/">Weinre</a>
</li>
    </ul>
  </li>
    </ul>
  </li>
                        </ul>
                    </li>
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">工具 <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                            
<li >
    <a href="../../../../工具/About/">About</a>
</li>
                            
<li >
    <a href="../../../../工具/Cheatsheet/">Cheatsheet</a>
</li>
                            
<li >
    <a href="../../../../工具/">Home</a>
</li>
                            
<li >
    <a href="../../../../工具/shell显示时间/">Shell显示时间</a>
</li>
                            
  <li class="dropdown-submenu">
    <a href="#">常用工具</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../../../工具/常用工具/10分钟上手Latex/">10分钟上手Latex</a>
</li>
            
<li >
    <a href="../../../../工具/常用工具/10分钟上手Pandas/">10分钟上手Pandas</a>
</li>
            
<li >
    <a href="../../../../工具/常用工具/10分钟上手Python/">10分钟上手Python</a>
</li>
    </ul>
  </li>
                        </ul>
                    </li>
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">文章 <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                            
<li >
    <a href="../../../../文章/">Home</a>
</li>
                        </ul>
                    </li>
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">日常 <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                            
<li >
    <a href="../../../../日常/">Home</a>
</li>
                            
  <li class="dropdown-submenu">
    <a href="#">test</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../../../日常/test/testtest/">Testtest</a>
</li>
    </ul>
  </li>
                        </ul>
                    </li>
                </ul>

            <ul class="nav navbar-nav navbar-right">
                <li>
                    <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                        <i class="fa fa-search"></i> Search
                    </a>
                </li>
                    <li >
                        <a rel="next" href="../en-optimization-1/">
                            <i class="fa fa-arrow-left"></i> Previous
                        </a>
                    </li>
                    <li >
                        <a rel="prev" href="../en-overview/">
                            Next <i class="fa fa-arrow-right"></i>
                        </a>
                    </li>
                    <li>
                        <a href="https://github.com/1007530194/Diary">1007530194/Diary</a>
                    </li>
            </ul>
        </div>
    </div>
</div>

        <div class="container">
                <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
        <li class="main active"><a href="#introduction">Introduction</a></li>
        <li class="main "><a href="#simple_expressions_and_interpretation_of_the_gradient">Simple expressions and interpretation of the gradient</a></li>
        <li class="main "><a href="#compound_expressions_with_chain_rule">Compound expressions with chain rule</a></li>
        <li class="main "><a href="#intuitive_understanding_of_backpropagation">Intuitive understanding of backpropagation</a></li>
        <li class="main "><a href="#modularity_sigmoid_example">Modularity: Sigmoid example</a></li>
        <li class="main "><a href="#backprop_in_practice_staged_computation">Backprop in practice: Staged computation</a></li>
        <li class="main "><a href="#patterns_in_backward_flow">Patterns in backward flow</a></li>
        <li class="main "><a href="#gradients_for_vectorized_operations">Gradients for vectorized operations</a></li>
        <li class="main "><a href="#summary">Summary</a></li>
        <li class="main "><a href="#references">References</a></li>
    </ul>
</div></div>
                <div class="col-md-9" role="main">

<p>Table of Contents:</p>
<ul>
<li><a href="#intro">Introduction</a></li>
<li><a href="#grad">Simple expressions, interpreting the gradient</a></li>
<li><a href="#backprop">Compound expressions, chain rule, backpropagation</a></li>
<li><a href="#intuitive">Intuitive understanding of backpropagation</a></li>
<li><a href="#sigmoid">Modularity: Sigmoid example</a></li>
<li><a href="#staged">Backprop in practice: Staged computation</a></li>
<li><a href="#patters">Patterns in backward flow</a></li>
<li><a href="#mat">Gradients for vectorized operations</a></li>
<li><a href="#summary">Summary</a></li>
</ul>
<p><a name='intro'></a></p>
<h3 id="introduction">Introduction<a class="headerlink" href="#introduction" title="Permanent link">&para;</a></h3>
<p><strong>Motivation</strong>. In this section we will develop expertise with an intuitive understanding of <strong>backpropagation</strong>, which is a way of computing gradients of expressions through recursive application of <strong>chain rule</strong>. Understanding of this process and its subtleties is critical for you to understand, and effectively develop, design and debug Neural Networks.</p>
<p><strong>Problem statement</strong>. The core problem studied in this section is as follows: We are given some function <span><span class="MathJax_Preview">f(x)</span><script type="math/tex">f(x)</script></span> where <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> is a vector of inputs and we are interested in computing the gradient of <span><span class="MathJax_Preview">f</span><script type="math/tex">f</script></span> at <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> (i.e. <span><span class="MathJax_Preview">\nabla f(x)</span><script type="math/tex">\nabla f(x)</script></span> ).</p>
<p><strong>Motivation</strong>. Recall that the primary reason we are interested in this problem is that in the specific case of Neural Networks, <span><span class="MathJax_Preview">f</span><script type="math/tex">f</script></span> will correspond to the loss function ( <span><span class="MathJax_Preview">L</span><script type="math/tex">L</script></span> ) and the inputs <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> will consist of the training data and the neural network weights. For example, the loss could be the SVM loss function and the inputs are both the training data <span><span class="MathJax_Preview">(x_i,y_i), i=1 \ldots N</span><script type="math/tex">(x_i,y_i), i=1 \ldots N</script></span> and the weights and biases <span><span class="MathJax_Preview">W,b</span><script type="math/tex">W,b</script></span>. Note that (as is usually the case in Machine Learning) we think of the training data as given and fixed, and of the weights as variables we have control over. Hence, even though we can easily use backpropagation to compute the gradient on the input examples <span><span class="MathJax_Preview">x_i</span><script type="math/tex">x_i</script></span>, in practice we usually only compute the gradient for the parameters (e.g. <span><span class="MathJax_Preview">W,b</span><script type="math/tex">W,b</script></span>) so that we can use it to perform a parameter update. However, as we will see later in the class the gradient on <span><span class="MathJax_Preview">x_i</span><script type="math/tex">x_i</script></span> can still be useful sometimes, for example for purposes of visualization and interpreting what the Neural Network might be doing.</p>
<p>If you are coming to this class and you're comfortable with deriving gradients with chain rule, we would still like to encourage you to at least skim this section, since it presents a rarely developed view of backpropagation as backward flow in real-valued circuits and any insights you'll gain may help you throughout the class.</p>
<p><a name='grad'></a></p>
<h3 id="simple_expressions_and_interpretation_of_the_gradient">Simple expressions and interpretation of the gradient<a class="headerlink" href="#simple_expressions_and_interpretation_of_the_gradient" title="Permanent link">&para;</a></h3>
<p>Lets start simple so that we can develop the notation and conventions for more complex expressions. Consider a simple multiplication function of two numbers <span><span class="MathJax_Preview">f(x,y) = x y</span><script type="math/tex">f(x,y) = x y</script></span>. It is a matter of simple calculus to derive the partial derivative for either input:</p>
<div>
<div class="MathJax_Preview">
f(x,y) = x y \hspace{0.5in} \rightarrow \hspace{0.5in} \frac{\partial f}{\partial x} = y \hspace{0.5in} \frac{\partial f}{\partial y} = x 
</div>
<script type="math/tex; mode=display">
f(x,y) = x y \hspace{0.5in} \rightarrow \hspace{0.5in} \frac{\partial f}{\partial x} = y \hspace{0.5in} \frac{\partial f}{\partial y} = x 
</script>
</div>
<p><strong>Interpretation</strong>. Keep in mind what the derivatives tell you: They indicate the rate of change of a function with respect to that variable surrounding an infinitesimally small region near a particular point:</p>
<div>
<div class="MathJax_Preview">
\frac{df(x)}{dx} = \lim_{h\ \to 0} \frac{f(x + h) - f(x)}{h}
</div>
<script type="math/tex; mode=display">
\frac{df(x)}{dx} = \lim_{h\ \to 0} \frac{f(x + h) - f(x)}{h}
</script>
</div>
<p>A technical note is that the division sign on the left-hand side is, unlike the division sign on the right-hand side, not a division. Instead, this notation indicates that the operator $  \frac{d}{dx} $ is being applied to the function <span><span class="MathJax_Preview">f</span><script type="math/tex">f</script></span>, and returns a different function (the derivative). A nice way to think about the expression above is that when <span><span class="MathJax_Preview">h</span><script type="math/tex">h</script></span> is very small, then the function is well-approximated by a straight line, and the derivative is its slope. In other words, the derivative on each variable tells you the sensitivity of the whole expression on its value. For example, if <span><span class="MathJax_Preview">x = 4, y = -3</span><script type="math/tex">x = 4, y = -3</script></span> then <span><span class="MathJax_Preview">f(x,y) = -12</span><script type="math/tex">f(x,y) = -12</script></span> and the derivative on <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> <span><span class="MathJax_Preview">\frac{\partial f}{\partial x} = -3</span><script type="math/tex">\frac{\partial f}{\partial x} = -3</script></span>. This tells us that if we were to increase the value of this variable by a tiny amount, the effect on the whole expression would be to decrease it (due to the negative sign), and by three times that amount. This can be seen by rearranging the above equation ( $ f(x + h) = f(x) + h \frac{df(x)}{dx} $ ). Analogously, since <span><span class="MathJax_Preview">\frac{\partial f}{\partial y} = 4</span><script type="math/tex">\frac{\partial f}{\partial y} = 4</script></span>, we expect that increasing the value of <span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span> by some very small amount <span><span class="MathJax_Preview">h</span><script type="math/tex">h</script></span> would also increase the output of the function (due to the positive sign), and by <span><span class="MathJax_Preview">4h</span><script type="math/tex">4h</script></span>.</p>
<blockquote>
<p>The derivative on each variable tells you the sensitivity of the whole expression on its value.</p>
</blockquote>
<p>As mentioned, the gradient <span><span class="MathJax_Preview">\nabla f</span><script type="math/tex">\nabla f</script></span> is the vector of partial derivatives, so we have that <span><span class="MathJax_Preview">\nabla f = [\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}] = [y, x]</span><script type="math/tex">\nabla f = [\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}] = [y, x]</script></span>. Even though the gradient is technically a vector, we will often use terms such as <em>"the gradient on x"</em> instead of the technically correct phrase <em>"the partial derivative on x"</em> for simplicity.</p>
<p>We can also derive the derivatives for the addition operation:</p>
<div>
<div class="MathJax_Preview">
f(x,y) = x + y \hspace{0.5in} \rightarrow \hspace{0.5in} \frac{\partial f}{\partial x} = 1 \hspace{0.5in} \frac{\partial f}{\partial y} = 1
</div>
<script type="math/tex; mode=display">
f(x,y) = x + y \hspace{0.5in} \rightarrow \hspace{0.5in} \frac{\partial f}{\partial x} = 1 \hspace{0.5in} \frac{\partial f}{\partial y} = 1
</script>
</div>
<p>that is, the derivative on both <span><span class="MathJax_Preview">x,y</span><script type="math/tex">x,y</script></span> is one regardless of what the values of <span><span class="MathJax_Preview">x,y</span><script type="math/tex">x,y</script></span> are. This makes sense, since increasing either <span><span class="MathJax_Preview">x,y</span><script type="math/tex">x,y</script></span> would increase the output of <span><span class="MathJax_Preview">f</span><script type="math/tex">f</script></span>, and the rate of that increase would be independent of what the actual values of <span><span class="MathJax_Preview">x,y</span><script type="math/tex">x,y</script></span> are (unlike the case of multiplication above). The last function we'll use quite a bit in the class is the <em>max</em> operation:</p>
<div>
<div class="MathJax_Preview">
f(x,y) = \max(x, y) \hspace{0.5in} \rightarrow \hspace{0.5in} \frac{\partial f}{\partial x} = \mathbb{1}(x &gt;= y) \hspace{0.5in} \frac{\partial f}{\partial y} = \mathbb{1}(y &gt;= x)
</div>
<script type="math/tex; mode=display">
f(x,y) = \max(x, y) \hspace{0.5in} \rightarrow \hspace{0.5in} \frac{\partial f}{\partial x} = \mathbb{1}(x >= y) \hspace{0.5in} \frac{\partial f}{\partial y} = \mathbb{1}(y >= x)
</script>
</div>
<p>That is, the (sub)gradient is 1 on the input that was larger and 0 on the other input. Intuitively, if the inputs are <span><span class="MathJax_Preview">x = 4,y = 2</span><script type="math/tex">x = 4,y = 2</script></span>, then the max is 4, and the function is not sensitive to the setting of <span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span>. That is, if we were to increase it by a tiny amount <span><span class="MathJax_Preview">h</span><script type="math/tex">h</script></span>, the function would keep outputting 4, and therefore the gradient is zero: there is no effect. Of course, if we were to change <span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span> by a large amount (e.g. larger than 2), then the value of <span><span class="MathJax_Preview">f</span><script type="math/tex">f</script></span> would change, but the derivatives tell us nothing about the effect of such large changes on the inputs of a function; They are only informative for tiny, infinitesimally small changes on the inputs, as indicated by the <span><span class="MathJax_Preview">\lim_{h \rightarrow 0}</span><script type="math/tex">\lim_{h \rightarrow 0}</script></span> in its definition.</p>
<p><a name='backprop'></a></p>
<h3 id="compound_expressions_with_chain_rule">Compound expressions with chain rule<a class="headerlink" href="#compound_expressions_with_chain_rule" title="Permanent link">&para;</a></h3>
<p>Lets now start to consider more complicated expressions that involve multiple composed functions, such as <span><span class="MathJax_Preview">f(x,y,z) = (x + y) z</span><script type="math/tex">f(x,y,z) = (x + y) z</script></span>. This expression is still simple enough to differentiate directly, but we'll take a particular approach to it that will be helpful with understanding the intuition behind backpropagation. In particular, note that this expression can be broken down into two expressions: <span><span class="MathJax_Preview">q = x + y</span><script type="math/tex">q = x + y</script></span> and <span><span class="MathJax_Preview">f = q z</span><script type="math/tex">f = q z</script></span>. Moreover, we know how to compute the derivatives of both expressions separately, as seen in the previous section. <span><span class="MathJax_Preview">f</span><script type="math/tex">f</script></span> is just multiplication of <span><span class="MathJax_Preview">q</span><script type="math/tex">q</script></span> and <span><span class="MathJax_Preview">z</span><script type="math/tex">z</script></span>, so <span><span class="MathJax_Preview">\frac{\partial f}{\partial q} = z, \frac{\partial f}{\partial z} = q</span><script type="math/tex">\frac{\partial f}{\partial q} = z, \frac{\partial f}{\partial z} = q</script></span>, and <span><span class="MathJax_Preview">q</span><script type="math/tex">q</script></span> is addition of <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> and <span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span> so $ \frac{\partial q}{\partial x} = 1, \frac{\partial q}{\partial y} = 1 $. However, we don't necessarily care about the gradient on the intermediate value <span><span class="MathJax_Preview">q</span><script type="math/tex">q</script></span> - the value of <span><span class="MathJax_Preview">\frac{\partial f}{\partial q}</span><script type="math/tex">\frac{\partial f}{\partial q}</script></span> is not useful. Instead, we are ultimately interested in the gradient of <span><span class="MathJax_Preview">f</span><script type="math/tex">f</script></span> with respect to its inputs <span><span class="MathJax_Preview">x,y,z</span><script type="math/tex">x,y,z</script></span>. The <strong>chain rule</strong> tells us that the correct way to "chain" these gradient expressions together is through multiplication. For example, $\frac{\partial f}{\partial x} = \frac{\partial f}{\partial q} \frac{\partial q}{\partial x} $. In practice this is simply a multiplication of the two numbers that hold the two gradients. Lets see this with an example:</p>
<div class="codehilite"><pre><span></span><span class="c1"># set some inputs</span>
<span class="n">x</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span><span class="p">;</span> <span class="n">y</span> <span class="o">=</span> <span class="mi">5</span><span class="p">;</span> <span class="n">z</span> <span class="o">=</span> <span class="o">-</span><span class="mi">4</span>

<span class="c1"># perform the forward pass</span>
<span class="n">q</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span> <span class="c1"># q becomes 3</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">q</span> <span class="o">*</span> <span class="n">z</span> <span class="c1"># f becomes -12</span>

<span class="c1"># perform the backward pass (backpropagation) in reverse order:</span>
<span class="c1"># first backprop through f = q * z</span>
<span class="n">dfdz</span> <span class="o">=</span> <span class="n">q</span> <span class="c1"># df/dz = q, so gradient on z becomes 3</span>
<span class="n">dfdq</span> <span class="o">=</span> <span class="n">z</span> <span class="c1"># df/dq = z, so gradient on q becomes -4</span>
<span class="c1"># now backprop through q = x + y</span>
<span class="n">dfdx</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">*</span> <span class="n">dfdq</span> <span class="c1"># dq/dx = 1. And the multiplication here is the chain rule!</span>
<span class="n">dfdy</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">*</span> <span class="n">dfdq</span> <span class="c1"># dq/dy = 1</span>
</pre></div>

<p>At the end we are left with the gradient in the variables <code>[dfdx,dfdy,dfdz]</code>, which tell us the sensitivity of the variables <code>x,y,z</code> on <code>f</code>!. This is the simplest example of backpropagation. Going forward, we will want to use a more concise notation so that we don't have to keep writing the <code>df</code> part. That is, for example instead of <code>dfdq</code> we would simply write <code>dq</code>, and always assume that the gradient is with respect to the final output.</p>
<p>This computation can also be nicely visualized with a circuit diagram:</p>
<div class="fig figleft fighighlight">
<svg width="420" height="220"><defs><marker id="arrowhead" refX="6" refY="2" markerWidth="6" markerHeight="4" orient="auto"><path d="M 0,0 V 4 L6,2 Z"></path></marker></defs><line x1="40" y1="30" x2="110" y2="30" stroke="black" stroke-width="1"></line><text x="45" y="24" font-size="16" fill="green">-2</text><text x="45" y="47" font-size="16" fill="red">-4</text><text x="35" y="24" font-size="16" text-anchor="end" fill="black">x</text><line x1="40" y1="100" x2="110" y2="100" stroke="black" stroke-width="1"></line><text x="45" y="94" font-size="16" fill="green">5</text><text x="45" y="117" font-size="16" fill="red">-4</text><text x="35" y="94" font-size="16" text-anchor="end" fill="black">y</text><line x1="40" y1="170" x2="110" y2="170" stroke="black" stroke-width="1"></line><text x="45" y="164" font-size="16" fill="green">-4</text><text x="45" y="187" font-size="16" fill="red">3</text><text x="35" y="164" font-size="16" text-anchor="end" fill="black">z</text><line x1="210" y1="65" x2="280" y2="65" stroke="black" stroke-width="1"></line><text x="215" y="59" font-size="16" fill="green">3</text><text x="215" y="82" font-size="16" fill="red">-4</text><text x="205" y="59" font-size="16" text-anchor="end" fill="black">q</text><circle cx="170" cy="65" fill="white" stroke="black" stroke-width="1" r="20"></circle><text x="170" y="70" font-size="20" fill="black" text-anchor="middle">+</text><line x1="110" y1="30" x2="150" y2="65" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"></line><line x1="110" y1="100" x2="150" y2="65" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"></line><line x1="190" y1="65" x2="210" y2="65" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"></line><line x1="380" y1="117" x2="450" y2="117" stroke="black" stroke-width="1"></line><text x="385" y="111" font-size="16" fill="green">-12</text><text x="385" y="134" font-size="16" fill="red">1</text><text x="375" y="111" font-size="16" text-anchor="end" fill="black">f</text><circle cx="340" cy="117" fill="white" stroke="black" stroke-width="1" r="20"></circle><text x="340" y="127" font-size="20" fill="black" text-anchor="middle">*</text><line x1="280" y1="65" x2="320" y2="117" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"></line><line x1="110" y1="170" x2="320" y2="117" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"></line><line x1="360" y1="117" x2="380" y2="117" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"></line></svg>

<div class="figcaption">
  The real-valued <i>"circuit"</i> on left shows the visual representation of the computation. The <b>forward pass</b> computes values from inputs to output (shown in green). The <b>backward pass</b> then performs backpropagation which starts at the end and recursively applies the chain rule to compute the gradients (shown in red) all the way to the inputs of the circuit. The gradients can be thought of as flowing backwards through the circuit.
</div>
<div style="clear:both;"></div>
</div>

<p><a name='intuitive'></a></p>
<h3 id="intuitive_understanding_of_backpropagation">Intuitive understanding of backpropagation<a class="headerlink" href="#intuitive_understanding_of_backpropagation" title="Permanent link">&para;</a></h3>
<p>Notice that backpropagation is a beautifully local process. Every gate in a circuit diagram gets some inputs and can right away compute two things: 1. its output value and 2. the <em>local</em> gradient of its inputs with respect to its output value. Notice that the gates can do this completely independently without being aware of any of the details of the full circuit that they are embedded in. However, once the forward pass is over, during backpropagation the gate will eventually learn about the gradient of its output value on the final output of the entire circuit. Chain rule says that the gate should take that gradient and multiply it into every gradient it normally computes for all of its inputs.</p>
<blockquote>
<p>This extra multiplication (for each input) due to the chain rule can turn a single and relatively useless gate into a cog in a complex circuit such as an entire neural network.</p>
</blockquote>
<p>Lets get an intuition for how this works by referring again to the example. The add gate received inputs [-2, 5] and computed output 3. Since the gate is computing the addition operation, its local gradient for both of its inputs is +1. The rest of the circuit computed the final value, which is -12. During the backward pass in which the chain rule is applied recursively backwards through the circuit, the add gate (which is an input to the multiply gate) learns that the gradient for its output was -4. If we anthropomorphize the circuit as wanting to output a higher value (which can help with intuition), then we can think of the circuit as "wanting" the output of the add gate to be lower (due to negative sign), and with a <em>force</em> of 4. To continue the recurrence and to chain the gradient, the add gate takes that gradient and multiplies it to all of the local gradients for its inputs (making the gradient on both <strong>x</strong> and <strong>y</strong> 1 * -4 = -4). Notice that this has the desired effect: If <strong>x,y</strong> were to decrease (responding to their negative gradient) then the add gate's output would decrease, which in turn makes the multiply gate's output increase.</p>
<p>Backpropagation can thus be thought of as gates communicating to each other (through the gradient signal) whether they want their outputs to increase or decrease (and how strongly), so as to make the final output value higher.</p>
<p><a name='sigmoid'></a></p>
<h3 id="modularity_sigmoid_example">Modularity: Sigmoid example<a class="headerlink" href="#modularity_sigmoid_example" title="Permanent link">&para;</a></h3>
<p>The gates we introduced above are relatively arbitrary. Any kind of differentiable function can act as a gate, and we can group multiple gates into a single gate, or decompose a function into multiple gates whenever it is convenient. Lets look at another expression that illustrates this point:</p>
<div>
<div class="MathJax_Preview">
f(w,x) = \frac{1}{1+e^{-(w_0x_0 + w_1x_1 + w_2)}}
</div>
<script type="math/tex; mode=display">
f(w,x) = \frac{1}{1+e^{-(w_0x_0 + w_1x_1 + w_2)}}
</script>
</div>
<p>as we will see later in the class, this expression describes a 2-dimensional neuron (with inputs <strong>x</strong> and weights <strong>w</strong>) that uses the <em>sigmoid activation</em> function. But for now lets think of this very simply as just a function from inputs <em>w,x</em> to a single number. The function is made up of multiple gates. In addition to the ones described already above (add, mul, max), there are four more:</p>
<div>
<div class="MathJax_Preview">
f(x) = \frac{1}{x} 
\hspace{1in} \rightarrow \hspace{1in} 
\frac{df}{dx} = -1/x^2 
\\\\
f_c(x) = c + x
\hspace{1in} \rightarrow \hspace{1in} 
\frac{df}{dx} = 1 
\\\\
f(x) = e^x
\hspace{1in} \rightarrow \hspace{1in} 
\frac{df}{dx} = e^x
\\\\
f_a(x) = ax
\hspace{1in} \rightarrow \hspace{1in} 
\frac{df}{dx} = a
</div>
<script type="math/tex; mode=display">
f(x) = \frac{1}{x} 
\hspace{1in} \rightarrow \hspace{1in} 
\frac{df}{dx} = -1/x^2 
\\\\
f_c(x) = c + x
\hspace{1in} \rightarrow \hspace{1in} 
\frac{df}{dx} = 1 
\\\\
f(x) = e^x
\hspace{1in} \rightarrow \hspace{1in} 
\frac{df}{dx} = e^x
\\\\
f_a(x) = ax
\hspace{1in} \rightarrow \hspace{1in} 
\frac{df}{dx} = a
</script>
</div>
<p>Where the functions <span><span class="MathJax_Preview">f_c, f_a</span><script type="math/tex">f_c, f_a</script></span> translate the input by a constant of <span><span class="MathJax_Preview">c</span><script type="math/tex">c</script></span> and scale the input by a constant of <span><span class="MathJax_Preview">a</span><script type="math/tex">a</script></span>, respectively. These are technically special cases of addition and multiplication, but we introduce them as (new) unary gates here since we do need the gradients for the constants. <span><span class="MathJax_Preview">c,a</span><script type="math/tex">c,a</script></span>. The full circuit then looks as follows:</p>
<div class="fig figleft fighighlight">
<svg width="799" height="306"><g transform="scale(0.8)"><defs><marker id="arrowhead" refX="6" refY="2" markerWidth="6" markerHeight="4" orient="auto"><path d="M 0,0 V 4 L6,2 Z"></path></marker></defs><line x1="50" y1="30" x2="90" y2="30" stroke="black" stroke-width="1"></line><text x="55" y="24" font-size="16" fill="green">2.00</text><text x="55" y="47" font-size="16" fill="red">-0.20</text><text x="45" y="24" font-size="16" text-anchor="end" fill="black">w0</text><line x1="50" y1="100" x2="90" y2="100" stroke="black" stroke-width="1"></line><text x="55" y="94" font-size="16" fill="green">-1.00</text><text x="55" y="117" font-size="16" fill="red">0.39</text><text x="45" y="94" font-size="16" text-anchor="end" fill="black">x0</text><line x1="50" y1="170" x2="90" y2="170" stroke="black" stroke-width="1"></line><text x="55" y="164" font-size="16" fill="green">-3.00</text><text x="55" y="187" font-size="16" fill="red">-0.39</text><text x="45" y="164" font-size="16" text-anchor="end" fill="black">w1</text><line x1="50" y1="240" x2="90" y2="240" stroke="black" stroke-width="1"></line><text x="55" y="234" font-size="16" fill="green">-2.00</text><text x="55" y="257" font-size="16" fill="red">-0.59</text><text x="45" y="234" font-size="16" text-anchor="end" fill="black">x1</text><line x1="50" y1="310" x2="90" y2="310" stroke="black" stroke-width="1"></line><text x="55" y="304" font-size="16" fill="green">-3.00</text><text x="55" y="327" font-size="16" fill="red">0.20</text><text x="45" y="304" font-size="16" text-anchor="end" fill="black">w2</text><line x1="170" y1="65" x2="210" y2="65" stroke="black" stroke-width="1"></line><text x="175" y="59" font-size="16" fill="green">-2.00</text><text x="175" y="82" font-size="16" fill="red">0.20</text><circle cx="130" cy="65" fill="white" stroke="black" stroke-width="1" r="20"></circle><text x="130" y="75" font-size="20" fill="black" text-anchor="middle">*</text><line x1="90" y1="30" x2="110" y2="65" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"></line><line x1="90" y1="100" x2="110" y2="65" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"></line><line x1="150" y1="65" x2="170" y2="65" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"></line><line x1="170" y1="205" x2="210" y2="205" stroke="black" stroke-width="1"></line><text x="175" y="199" font-size="16" fill="green">6.00</text><text x="175" y="222" font-size="16" fill="red">0.20</text><circle cx="130" cy="205" fill="white" stroke="black" stroke-width="1" r="20"></circle><text x="130" y="215" font-size="20" fill="black" text-anchor="middle">*</text><line x1="90" y1="170" x2="110" y2="205" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"></line><line x1="90" y1="240" x2="110" y2="205" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"></line><line x1="150" y1="205" x2="170" y2="205" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"></line><line x1="290" y1="135" x2="330" y2="135" stroke="black" stroke-width="1"></line><text x="295" y="129" font-size="16" fill="green">4.00</text><text x="295" y="152" font-size="16" fill="red">0.20</text><circle cx="250" cy="135" fill="white" stroke="black" stroke-width="1" r="20"></circle><text x="250" y="140" font-size="20" fill="black" text-anchor="middle">+</text><line x1="210" y1="65" x2="230" y2="135" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"></line><line x1="210" y1="205" x2="230" y2="135" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"></line><line x1="270" y1="135" x2="290" y2="135" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"></line><line x1="410" y1="222" x2="450" y2="222" stroke="black" stroke-width="1"></line><text x="415" y="216" font-size="16" fill="green">1.00</text><text x="415" y="239" font-size="16" fill="red">0.20</text><circle cx="370" cy="222" fill="white" stroke="black" stroke-width="1" r="20"></circle><text x="370" y="227" font-size="20" fill="black" text-anchor="middle">+</text><line x1="330" y1="135" x2="350" y2="222" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"></line><line x1="90" y1="310" x2="350" y2="222" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"></line><line x1="390" y1="222" x2="410" y2="222" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"></line><line x1="530" y1="222" x2="570" y2="222" stroke="black" stroke-width="1"></line><text x="535" y="216" font-size="16" fill="green">-1.00</text><text x="535" y="239" font-size="16" fill="red">-0.20</text><circle cx="490" cy="222" fill="white" stroke="black" stroke-width="1" r="20"></circle><text x="490" y="227" font-size="20" fill="black" text-anchor="middle">*-1</text><line x1="450" y1="222" x2="470" y2="222" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"></line><line x1="510" y1="222" x2="530" y2="222" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"></line><line x1="650" y1="222" x2="690" y2="222" stroke="black" stroke-width="1"></line><text x="655" y="216" font-size="16" fill="green">0.37</text><text x="655" y="239" font-size="16" fill="red">-0.53</text><circle cx="610" cy="222" fill="white" stroke="black" stroke-width="1" r="20"></circle><text x="610" y="227" font-size="20" fill="black" text-anchor="middle">exp</text><line x1="570" y1="222" x2="590" y2="222" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"></line><line x1="630" y1="222" x2="650" y2="222" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"></line><line x1="770" y1="222" x2="810" y2="222" stroke="black" stroke-width="1"></line><text x="775" y="216" font-size="16" fill="green">1.37</text><text x="775" y="239" font-size="16" fill="red">-0.53</text><circle cx="730" cy="222" fill="white" stroke="black" stroke-width="1" r="20"></circle><text x="730" y="227" font-size="20" fill="black" text-anchor="middle">+1</text><line x1="690" y1="222" x2="710" y2="222" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"></line><line x1="750" y1="222" x2="770" y2="222" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"></line><line x1="890" y1="222" x2="930" y2="222" stroke="black" stroke-width="1"></line><text x="895" y="216" font-size="16" fill="green">0.73</text><text x="895" y="239" font-size="16" fill="red">1.00</text><circle cx="850" cy="222" fill="white" stroke="black" stroke-width="1" r="20"></circle><text x="850" y="227" font-size="20" fill="black" text-anchor="middle">1/x</text><line x1="810" y1="222" x2="830" y2="222" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"></line><line x1="870" y1="222" x2="890" y2="222" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"></line></g></svg>
<div class="figcaption">
  Example circuit for a 2D neuron with a sigmoid activation function. The inputs are [x0,x1] and the (learnable) weights of the neuron are [w0,w1,w2]. As we will see later, the neuron computes a dot product with the input and then its activation is softly squashed by the sigmoid function to be in range from 0 to 1.
</div>
<div style="clear:both;"></div>
</div>

<p>In the example above, we see a long chain of function applications that operates on the result of the dot product between <strong>w,x</strong>. The function that these operations implement is called the <em>sigmoid function</em> <span><span class="MathJax_Preview">\sigma(x)</span><script type="math/tex">\sigma(x)</script></span>. It turns out that the derivative of the sigmoid function with respect to its input simplifies if you perform the derivation (after a fun tricky part where we add and subtract a 1 in the numerator):</p>
<div>
<div class="MathJax_Preview">
\sigma(x) = \frac{1}{1+e^{-x}} \\\\
\rightarrow \hspace{0.3in} \frac{d\sigma(x)}{dx} = \frac{e^{-x}}{(1+e^{-x})^2} = \left( \frac{1 + e^{-x} - 1}{1 + e^{-x}} \right) \left( \frac{1}{1+e^{-x}} \right) 
= \left( 1 - \sigma(x) \right) \sigma(x)
</div>
<script type="math/tex; mode=display">
\sigma(x) = \frac{1}{1+e^{-x}} \\\\
\rightarrow \hspace{0.3in} \frac{d\sigma(x)}{dx} = \frac{e^{-x}}{(1+e^{-x})^2} = \left( \frac{1 + e^{-x} - 1}{1 + e^{-x}} \right) \left( \frac{1}{1+e^{-x}} \right) 
= \left( 1 - \sigma(x) \right) \sigma(x)
</script>
</div>
<p>As we see, the gradient turns out to simplify and becomes surprisingly simple. For example, the sigmoid expression receives the input 1.0 and computes the output 0.73 during the forward pass. The derivation above shows that the <em>local</em> gradient would simply be (1 - 0.73) * 0.73 ~= 0.2, as the circuit computed before (see the image above), except this way it would be done with a single, simple and efficient expression (and with less numerical issues). Therefore, in any real practical application it would be very useful to group these operations into a single gate. Lets see the backprop for this neuron in code:</p>
<div class="codehilite"><pre><span></span><span class="n">w</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="p">]</span> <span class="c1"># assume some random weights and data</span>
<span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">]</span>

<span class="c1"># forward pass</span>
<span class="n">dot</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">w</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="n">f</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">dot</span><span class="p">))</span> <span class="c1"># sigmoid function</span>

<span class="c1"># backward pass through the neuron (backpropagation)</span>
<span class="n">ddot</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">f</span><span class="p">)</span> <span class="o">*</span> <span class="n">f</span> <span class="c1"># gradient on dot variable, using the sigmoid gradient derivation</span>
<span class="n">dx</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">ddot</span><span class="p">,</span> <span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">ddot</span><span class="p">]</span> <span class="c1"># backprop into x</span>
<span class="n">dw</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">ddot</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">ddot</span><span class="p">,</span> <span class="mf">1.0</span> <span class="o">*</span> <span class="n">ddot</span><span class="p">]</span> <span class="c1"># backprop into w</span>
<span class="c1"># we&#39;re done! we have the gradients on the inputs to the circuit</span>
</pre></div>

<p><strong>Implementation protip: staged backpropagation</strong>. As shown in the code above, in practice it is always helpful to break down the forward pass into stages that are easily backpropped through. For example here we created an intermediate variable <code>dot</code> which holds the output of the dot product between <code>w</code> and <code>x</code>. During backward pass we then successively compute (in reverse order) the corresponding variables (e.g. <code>ddot</code>, and ultimately <code>dw, dx</code>) that hold the gradients of those variables.</p>
<p>The point of this section is that the details of how the backpropagation is performed, and which parts of the forward function we think of as gates, is a matter of convenience. It helps to be aware of which parts of the expression have easy local gradients, so that they can be chained together with the least amount of code and effort. </p>
<p><a name='staged'></a></p>
<h3 id="backprop_in_practice_staged_computation">Backprop in practice: Staged computation<a class="headerlink" href="#backprop_in_practice_staged_computation" title="Permanent link">&para;</a></h3>
<p>Lets see this with another example. Suppose that we have a function of the form:</p>
<div>
<div class="MathJax_Preview">
f(x,y) = \frac{x + \sigma(y)}{\sigma(x) + (x+y)^2}
</div>
<script type="math/tex; mode=display">
f(x,y) = \frac{x + \sigma(y)}{\sigma(x) + (x+y)^2}
</script>
</div>
<p>To be clear, this function is completely useless and it's not clear why you would ever want to compute its gradient, except for the fact that it is a good example of backpropagation in practice. It is very important to stress that if you were to launch into performing the differentiation with respect to either <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> or <span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span>, you would end up with very large and complex expressions. However, it turns out that doing so is completely unnecessary because we don't need to have an explicit function written down that evaluates the gradient. We only have to know how to compute it. Here is how we would structure the forward pass of such expression:</p>
<div class="codehilite"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="mi">3</span> <span class="c1"># example values</span>
<span class="n">y</span> <span class="o">=</span> <span class="o">-</span><span class="mi">4</span>

<span class="c1"># forward pass</span>
<span class="n">sigy</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">y</span><span class="p">))</span> <span class="c1"># sigmoid in numerator   #(1)</span>
<span class="n">num</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">sigy</span> <span class="c1"># numerator                               #(2)</span>
<span class="n">sigx</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span> <span class="c1"># sigmoid in denominator #(3)</span>
<span class="n">xpy</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>                                              <span class="c1">#(4)</span>
<span class="n">xpysqr</span> <span class="o">=</span> <span class="n">xpy</span><span class="o">**</span><span class="mi">2</span>                                          <span class="c1">#(5)</span>
<span class="n">den</span> <span class="o">=</span> <span class="n">sigx</span> <span class="o">+</span> <span class="n">xpysqr</span> <span class="c1"># denominator                        #(6)</span>
<span class="n">invden</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">den</span>                                       <span class="c1">#(7)</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">num</span> <span class="o">*</span> <span class="n">invden</span> <span class="c1"># done!                                 #(8)</span>
</pre></div>

<p>Phew, by the end of the expression we have computed the forward pass. Notice that we have structured the code in such way that it contains multiple intermediate variables, each of which are only simple expressions for which we already know the local gradients. Therefore, computing the backprop pass is easy: We'll go backwards and for every variable along the way in the forward pass (<code>sigy, num, sigx, xpy, xpysqr, den, invden</code>) we will have the same variable, but one that begins with a <code>d</code>, which will hold the gradient of the output of the circuit with respect to that variable. Additionally, note that every single piece in our backprop will involve computing the local gradient of that expression, and chaining it with the gradient on that expression with a multiplication. For each row, we also highlight which part of the forward pass it refers to:</p>
<div class="codehilite"><pre><span></span><span class="c1"># backprop f = num * invden</span>
<span class="n">dnum</span> <span class="o">=</span> <span class="n">invden</span> <span class="c1"># gradient on numerator                             #(8)</span>
<span class="n">dinvden</span> <span class="o">=</span> <span class="n">num</span>                                                     <span class="c1">#(8)</span>
<span class="c1"># backprop invden = 1.0 / den </span>
<span class="n">dden</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">den</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span> <span class="o">*</span> <span class="n">dinvden</span>                                <span class="c1">#(7)</span>
<span class="c1"># backprop den = sigx + xpysqr</span>
<span class="n">dsigx</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">dden</span>                                                <span class="c1">#(6)</span>
<span class="n">dxpysqr</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">dden</span>                                              <span class="c1">#(6)</span>
<span class="c1"># backprop xpysqr = xpy**2</span>
<span class="n">dxpy</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">xpy</span><span class="p">)</span> <span class="o">*</span> <span class="n">dxpysqr</span>                                        <span class="c1">#(5)</span>
<span class="c1"># backprop xpy = x + y</span>
<span class="n">dx</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">dxpy</span>                                                   <span class="c1">#(4)</span>
<span class="n">dy</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">dxpy</span>                                                   <span class="c1">#(4)</span>
<span class="c1"># backprop sigx = 1.0 / (1 + math.exp(-x))</span>
<span class="n">dx</span> <span class="o">+=</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">sigx</span><span class="p">)</span> <span class="o">*</span> <span class="n">sigx</span><span class="p">)</span> <span class="o">*</span> <span class="n">dsigx</span> <span class="c1"># Notice += !! See notes below  #(3)</span>
<span class="c1"># backprop num = x + sigy</span>
<span class="n">dx</span> <span class="o">+=</span> <span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">dnum</span>                                                  <span class="c1">#(2)</span>
<span class="n">dsigy</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">dnum</span>                                                <span class="c1">#(2)</span>
<span class="c1"># backprop sigy = 1.0 / (1 + math.exp(-y))</span>
<span class="n">dy</span> <span class="o">+=</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">sigy</span><span class="p">)</span> <span class="o">*</span> <span class="n">sigy</span><span class="p">)</span> <span class="o">*</span> <span class="n">dsigy</span>                                 <span class="c1">#(1)</span>
<span class="c1"># done! phew</span>
</pre></div>

<p>Notice a few things:</p>
<p><strong>Cache forward pass variables</strong>. To compute the backward pass it is very helpful to have some of the variables that were used in the forward pass. In practice you want to structure your code so that you cache these variables, and so that they are available during backpropagation. If this is too difficult, it is possible (but wasteful) to recompute them.</p>
<p><strong>Gradients add up at forks</strong>. The forward expression involves the variables <strong>x,y</strong> multiple times, so when we perform backpropagation we must be careful to use <code>+=</code> instead of <code>=</code> to accumulate the gradient on these variables (otherwise we would overwrite it). This follows the <em>multivariable chain rule</em> in Calculus, which states that if a variable branches out to different parts of the circuit, then the gradients that flow back to it will add.</p>
<p><a name='patterns'></a></p>
<h3 id="patterns_in_backward_flow">Patterns in backward flow<a class="headerlink" href="#patterns_in_backward_flow" title="Permanent link">&para;</a></h3>
<p>It is interesting to note that in many cases the backward-flowing gradient can be interpreted on an intuitive level. For example, the three most commonly used gates in neural networks (<em>add,mul,max</em>), all have very simple interpretations in terms of how they act during backpropagation. Consider this example circuit:</p>
<div class="fig figleft fighighlight">
<svg width="460" height="290"><g transform="scale(1)"><defs><marker id="arrowhead" refX="6" refY="2" markerWidth="6" markerHeight="4" orient="auto"><path d="M 0,0 V 4 L6,2 Z"></path></marker></defs><line x1="50" y1="30" x2="90" y2="30" stroke="black" stroke-width="1"></line><text x="55" y="24" font-size="16" fill="green">3.00</text><text x="55" y="47" font-size="16" fill="red">-8.00</text><text x="45" y="24" font-size="16" text-anchor="end" fill="black">x</text><line x1="50" y1="100" x2="90" y2="100" stroke="black" stroke-width="1"></line><text x="55" y="94" font-size="16" fill="green">-4.00</text><text x="55" y="117" font-size="16" fill="red">6.00</text><text x="45" y="94" font-size="16" text-anchor="end" fill="black">y</text><line x1="50" y1="170" x2="90" y2="170" stroke="black" stroke-width="1"></line><text x="55" y="164" font-size="16" fill="green">2.00</text><text x="55" y="187" font-size="16" fill="red">2.00</text><text x="45" y="164" font-size="16" text-anchor="end" fill="black">z</text><line x1="50" y1="240" x2="90" y2="240" stroke="black" stroke-width="1"></line><text x="55" y="234" font-size="16" fill="green">-1.00</text><text x="55" y="257" font-size="16" fill="red">0.00</text><text x="45" y="234" font-size="16" text-anchor="end" fill="black">w</text><line x1="170" y1="65" x2="210" y2="65" stroke="black" stroke-width="1"></line><text x="175" y="59" font-size="16" fill="green">-12.00</text><text x="175" y="82" font-size="16" fill="red">2.00</text><circle cx="130" cy="65" fill="white" stroke="black" stroke-width="1" r="20"></circle><text x="130" y="75" font-size="20" fill="black" text-anchor="middle">*</text><line x1="90" y1="30" x2="110" y2="65" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"></line><line x1="90" y1="100" x2="110" y2="65" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"></line><line x1="150" y1="65" x2="170" y2="65" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"></line><line x1="170" y1="205" x2="210" y2="205" stroke="black" stroke-width="1"></line><text x="175" y="199" font-size="16" fill="green">2.00</text><text x="175" y="222" font-size="16" fill="red">2.00</text><circle cx="130" cy="205" fill="white" stroke="black" stroke-width="1" r="20"></circle><text x="130" y="210" font-size="20" fill="black" text-anchor="middle">max</text><line x1="90" y1="170" x2="110" y2="205" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"></line><line x1="90" y1="240" x2="110" y2="205" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"></line><line x1="150" y1="205" x2="170" y2="205" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"></line><line x1="290" y1="135" x2="330" y2="135" stroke="black" stroke-width="1"></line><text x="295" y="129" font-size="16" fill="green">-10.00</text><text x="295" y="152" font-size="16" fill="red">2.00</text><circle cx="250" cy="135" fill="white" stroke="black" stroke-width="1" r="20"></circle><text x="250" y="140" font-size="20" fill="black" text-anchor="middle">+</text><line x1="210" y1="65" x2="230" y2="135" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"></line><line x1="210" y1="205" x2="230" y2="135" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"></line><line x1="270" y1="135" x2="290" y2="135" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"></line><line x1="410" y1="135" x2="450" y2="135" stroke="black" stroke-width="1"></line><text x="415" y="129" font-size="16" fill="green">-20.00</text><text x="415" y="152" font-size="16" fill="red">1.00</text><circle cx="370" cy="135" fill="white" stroke="black" stroke-width="1" r="20"></circle><text x="370" y="140" font-size="20" fill="black" text-anchor="middle">*2</text><line x1="330" y1="135" x2="350" y2="135" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"></line><line x1="390" y1="135" x2="410" y2="135" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"></line></g></svg>
<div class="figcaption">
  An example circuit demonstrating the intuition behind the operations that backpropagation performs during the backward pass in order to compute the gradients on the inputs. Sum operation distributes gradients equally to all its inputs. Max operation routes the gradient to the higher input. Multiply gate takes the input activations, swaps them and multiplies by its gradient.
</div>
<div style="clear:both;"></div>
</div>

<p>Looking at the diagram above as an example, we can see that:</p>
<p>The <strong>add gate</strong> always takes the gradient on its output and distributes it equally to all of its inputs, regardless of what their values were during the forward pass. This follows from the fact that the local gradient for the add operation is simply +1.0, so the gradients on all inputs will exactly equal the gradients on the output because it will be multiplied by x1.0 (and remain unchanged). In the example circuit above, note that the + gate routed the gradient of 2.00 to both of its inputs, equally and unchanged.</p>
<p>The <strong>max gate</strong> routes the gradient. Unlike the add gate which distributed the gradient unchanged to all its inputs, the max gate distributes the gradient (unchanged) to exactly one of its inputs (the input that had the highest value during the forward pass). This is because the local gradient for a max gate is 1.0 for the highest value, and 0.0 for all other values. In the example circuit above, the max operation routed the gradient of 2.00 to the <strong>z</strong> variable, which had a higher value than <strong>w</strong>, and the gradient on <strong>w</strong> remains zero.</p>
<p>The <strong>multiply gate</strong> is a little less easy to interpret. Its local gradients are the input values (except switched), and this is multiplied by the gradient on its output during the chain rule. In the example above, the gradient on <strong>x</strong> is -8.00, which is -4.00 x 2.00. </p>
<p><em>Unintuitive effects and their consequences</em>. Notice that if one of the inputs to the multiply gate is very small and the other is very big, then the multiply gate will do something slightly unintuitive: it will assign a relatively huge gradient to the small input and a tiny gradient to the large input. Note that in linear classifiers where the weights are dot producted <span><span class="MathJax_Preview">w^Tx_i</span><script type="math/tex">w^Tx_i</script></span> (multiplied) with the inputs, this implies that the scale of the data has an effect on the magnitude of the gradient for the weights. For example, if you multiplied all input data examples <span><span class="MathJax_Preview">x_i</span><script type="math/tex">x_i</script></span> by 1000 during preprocessing, then the gradient on the weights will be 1000 times larger, and you'd have to lower the learning rate by that factor to compensate. This is why preprocessing matters a lot, sometimes in subtle ways! And having intuitive understanding for how the gradients flow can help you debug some of these cases.</p>
<p><a name='mat'></a></p>
<h3 id="gradients_for_vectorized_operations">Gradients for vectorized operations<a class="headerlink" href="#gradients_for_vectorized_operations" title="Permanent link">&para;</a></h3>
<p>The above sections were concerned with single variables, but all concepts extend in a straight-forward manner to matrix and vector operations. However, one must pay closer attention to dimensions and transpose operations.</p>
<p><strong>Matrix-Matrix multiply gradient</strong>. Possibly the most tricky operation is the matrix-matrix multiplication (which generalizes all matrix-vector and vector-vector) multiply operations:</p>
<div class="codehilite"><pre><span></span><span class="c1"># forward pass</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">D</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># now suppose we had the gradient on D from above in the circuit</span>
<span class="n">dD</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="o">*</span><span class="n">D</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1"># same shape as D</span>
<span class="n">dW</span> <span class="o">=</span> <span class="n">dD</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="c1">#.T gives the transpose of the matrix</span>
<span class="n">dX</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dD</span><span class="p">)</span>
</pre></div>

<p><em>Tip: use dimension analysis!</em> Note that you do not need to remember the expressions for <code>dW</code> and <code>dX</code>  because they are easy to re-derive based on dimensions. For instance, we know that the gradient on the weights <code>dW</code> must be of the same size as <code>W</code> after it is computed, and that it must depend on matrix multiplication of <code>X</code> and <code>dD</code> (as is the case when both <code>X,W</code> are single numbers and not matrices). There is always exactly one way of achieving this so that the dimensions work out. For example, <code>X</code> is of size [10 x 3] and <code>dD</code> of size [5 x 3], so if we want <code>dW</code> and <code>W</code> has shape [5 x 10], then the only way of achieving this is with <code>dD.dot(X.T)</code>, as shown above.</p>
<p><strong>Work with small, explicit examples</strong>. Some people may find it difficult at first to derive the gradient updates for some vectorized expressions. Our recommendation is to explicitly write out a minimal vectorized example, derive the gradient on paper and then generalize the pattern to its efficient, vectorized form.</p>
<p>Erik Learned-Miller has also written up a longer related document on taking matrix/vector derivatives which you might find helpful. <a href="http://cs231n.stanford.edu/vecDerivs.pdf">Find it here</a>.</p>
<p><a name='summary'></a></p>
<h3 id="summary">Summary<a class="headerlink" href="#summary" title="Permanent link">&para;</a></h3>
<ul>
<li>We developed intuition for what the gradients mean, how they flow backwards in the circuit, and how they communicate which part of the circuit should increase or decrease and with what force to make the final output higher.</li>
<li>We discussed the importance of <strong>staged computation</strong> for practical implementations of backpropagation. You always want to break up your function into modules for which you can easily derive local gradients, and then chain them with chain rule. Crucially, you almost never want to write out these expressions on paper and differentiate them symbolically in full, because you never need an explicit mathematical equation for the gradient of the input variables. Hence, decompose your expressions into stages such that you can differentiate every stage independently (the stages will be matrix vector multiplies, or max operations, or sum operations, etc.) and then backprop through the variables one step at a time.</li>
</ul>
<p>In the next section we will start to define Neural Networks, and backpropagation will allow us to efficiently compute the gradients on the connections of the neural network, with respect to a loss function. In other words, we're now ready to train Neural Nets, and the most conceptually difficult part of this class is behind us! ConvNets will then be a small step away.</p>
<h3 id="references">References<a class="headerlink" href="#references" title="Permanent link">&para;</a></h3>
<ul>
<li><a href="http://arxiv.org/abs/1502.05767">Automatic differentiation in machine learning: a survey</a></li>
</ul></div>
        </div>

        <footer class="col-md-12">
            <hr>
                <p>Copyright &copy; 2016 - 2018 Martin Donath</p>
            <p>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>var base_url = '../../../..';</script>
        <script src="../../../../js/base.js"></script>
        <script src="../../../../javascripts/extra.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
        <script src="../../../../search/require.js"></script>
        <script src="../../../../search/search.js"></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="Search Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Search</h4>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form role="form">
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="Keyboard Shortcuts Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Keyboard Shortcuts</h4>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td><kbd>&larr;</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td><kbd>&rarr;</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>


    </body>
</html>

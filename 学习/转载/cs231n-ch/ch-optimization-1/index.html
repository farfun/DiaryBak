



<!DOCTYPE html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
        <meta name="description" content="Nothing is impossible">
      
      
        <link rel="canonical" href="https://1007530194.github.io/Diary/学习/转载/cs231n-ch/ch-optimization-1/">
      
      
        <meta name="author" content="NiuLiangtao">
      
      
        <meta name="lang:clipboard.copy" content="复制">
      
        <meta name="lang:clipboard.copied" content="已复制">
      
        <meta name="lang:search.language" content="jp">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="没有找到符合条件的结果">
      
        <meta name="lang:search.result.one" content="找到 1 个符合条件的结果">
      
        <meta name="lang:search.result.other" content="# 个符合条件的结果">
      
        <meta name="lang:search.tokenizer" content="[\uff0c\u3002]+">
      
      <link rel="shortcut icon" href="../../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-0.17.3, mkdocs-material-2.7.0">
    
    
      
        <title>optimization-1</title>
      
    
    
      <link rel="stylesheet" href="../../../../assets/stylesheets/application.78aab2dc.css">
      
        <link rel="stylesheet" href="../../../../assets/stylesheets/application-palette.6079476c.css">
      
    
    
      <script src="../../../../assets/javascripts/modernizr.1aa3b519.js"></script>
    
    
      <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
      
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
      <link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons">
    
    
    
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448"
    viewBox="0 0 416 448" id="github">
  <path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19-18.125
        8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19 18.125-8.5
        18.125 8.5 10.75 19 3.125 20.5zM320 304q0 10-3.125 20.5t-10.75
        19-18.125 8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19
        18.125-8.5 18.125 8.5 10.75 19 3.125 20.5zM360
        304q0-30-17.25-51t-46.75-21q-10.25 0-48.75 5.25-17.75 2.75-39.25
        2.75t-39.25-2.75q-38-5.25-48.75-5.25-29.5 0-46.75 21t-17.25 51q0 22 8
        38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0
        37.25-1.75t35-7.375 30.5-15 20.25-25.75 8-38.375zM416 260q0 51.75-15.25
        82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5-41.75
        1.125q-19.5 0-35.5-0.75t-36.875-3.125-38.125-7.5-34.25-12.875-30.25-20.25-21.5-28.75q-15.5-30.75-15.5-82.75
        0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25
        30.875q36.75-8.75 77.25-8.75 37 0 70 8 26.25-20.5
        46.75-30.25t47.25-9.75q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34
        99.5z" />
</svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="drawer">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="search">
    <label class="md-overlay" data-md-component="overlay" for="drawer"></label>
    
      <a href="#introduction" tabindex="1" class="md-skip">
        跳转至
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="https://1007530194.github.io/Diary/" title="Blog of NiuLiangtao" class="md-header-nav__button md-logo">
          
            <i class="md-icon"></i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            
              <span class="md-header-nav__topic">
                Blog of NiuLiangtao
              </span>
              <span class="md-header-nav__topic">
                optimization-1
              </span>
            
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          
            <label class="md-icon md-icon--search md-header-nav__button" for="search"></label>
            
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            键入以开始搜索
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
          
        
      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            


  


  <a href="https://github.com/1007530194/Diary" title="前往 Github 仓库" class="md-source" data-md-source="github">
    
      <div class="md-source__icon">
        <svg viewBox="0 0 24 24" width="24" height="24">
          <use xlink:href="#github" width="24" height="24"></use>
        </svg>
      </div>
    
    <div class="md-source__repository">
      1007530194/Diary
    </div>
  </a>

          </div>
        </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
        

  

<nav class="md-tabs md-tabs--active" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../../.." title="Home" class="md-tabs__link">
          Home
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../../../书籍/" title="书籍" class="md-tabs__link">
          书籍
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../../" title="学习" class="md-tabs__link md-tabs__link--active">
          学习
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../../../工具/About/" title="工具" class="md-tabs__link">
          工具
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../../../推荐/个人理解的/" title="推荐" class="md-tabs__link">
          推荐
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../../../文章/" title="文章" class="md-tabs__link">
          文章
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../../../日常/AboutMe/" title="日常" class="md-tabs__link">
          日常
        </a>
      
    </li>
  

      
    </ul>
  </div>
</nav>
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="drawer">
    <span class="md-nav__button md-logo">
      
        <i class="md-icon"></i>
      
    </span>
    Blog of NiuLiangtao
  </label>
  
    <div class="md-nav__source">
      


  


  <a href="https://github.com/1007530194/Diary" title="前往 Github 仓库" class="md-source" data-md-source="github">
    
      <div class="md-source__icon">
        <svg viewBox="0 0 24 24" width="24" height="24">
          <use xlink:href="#github" width="24" height="24"></use>
        </svg>
      </div>
    
    <div class="md-source__repository">
      1007530194/Diary
    </div>
  </a>

    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href="../../../../ToDo/" title="Home" class="md-nav__link">
      Home
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2" type="checkbox" id="nav-2">
    
    <label class="md-nav__link" for="nav-2">
      Home
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-2">
        Home
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../.." title="Home" class="md-nav__link">
      Home
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../Nothing/" title="Nothing" class="md-nav__link">
      Nothing
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../ToDo/" title="要做的" class="md-nav__link">
      要做的
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-3" type="checkbox" id="nav-3">
    
    <label class="md-nav__link" for="nav-3">
      书籍
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-3">
        书籍
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../书籍/" title="Home" class="md-nav__link">
      Home
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-3-2" type="checkbox" id="nav-3-2">
    
    <label class="md-nav__link" for="nav-3-2">
      机器学习实战
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="2">
      <label class="md-nav__title" for="nav-3-2">
        机器学习实战
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../书籍/机器学习实战/00naive-bayes-discuss/" title="naive-bayes-discuss" class="md-nav__link">
      naive-bayes-discuss
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../书籍/机器学习实战/01.机器学习基础/" title="1.机器学习基础" class="md-nav__link">
      1.机器学习基础
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../书籍/机器学习实战/02.k-近邻算法/" title="2.k-近邻算法" class="md-nav__link">
      2.k-近邻算法
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../书籍/机器学习实战/03.决策树/" title="3.决策树" class="md-nav__link">
      3.决策树
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../书籍/机器学习实战/04.朴素贝叶斯/" title="4.朴素贝叶斯" class="md-nav__link">
      4.朴素贝叶斯
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../书籍/机器学习实战/05.Logistic回归/" title="5.Logistic回归" class="md-nav__link">
      5.Logistic回归
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../书籍/机器学习实战/06.0.支持向量机/" title="6.支持向量机" class="md-nav__link">
      6.支持向量机
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../书籍/机器学习实战/06.1.支持向量机的几个通俗理解/" title="6.1.支持向量机的几个通俗理解" class="md-nav__link">
      6.1.支持向量机的几个通俗理解
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../书籍/机器学习实战/07.集成方法-随机森林和AdaBoost/" title="7.集成方法-随机森林和AdaBoost" class="md-nav__link">
      7.集成方法-随机森林和AdaBoost
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../书籍/机器学习实战/08.预测数值型数据-回归/" title="8.预测数值型数据：回归" class="md-nav__link">
      8.预测数值型数据：回归
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../书籍/机器学习实战/09.树回归/" title="9.树回归" class="md-nav__link">
      9.树回归
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../书籍/机器学习实战/10.k-means聚类/" title="10.k-means聚类" class="md-nav__link">
      10.k-means聚类
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../书籍/机器学习实战/11.使用Apriori算法进行关联分析/" title="11.使用Apriori算法进行关联分析" class="md-nav__link">
      11.使用Apriori算法进行关联分析
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../书籍/机器学习实战/12.使用FP-growth算法来高效发现频繁项集/" title="12.使用FP-growth算法来高效发现频繁项集" class="md-nav__link">
      12.使用FP-growth算法来高效发现频繁项集
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../书籍/机器学习实战/13.利用PCA来简化数据/" title="13.利用PCA来简化数据" class="md-nav__link">
      13.利用PCA来简化数据
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../书籍/机器学习实战/14.利用SVD简化数据/" title="14.利用SVD简化数据" class="md-nav__link">
      14.利用SVD简化数据
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../书籍/机器学习实战/15.大数据与MapReduce/" title="15.大数据与MapReduce" class="md-nav__link">
      15.大数据与MapReduce
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../书籍/机器学习实战/16.推荐系统/" title="16.推荐系统" class="md-nav__link">
      16.推荐系统
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-3-3" type="checkbox" id="nav-3-3">
    
    <label class="md-nav__link" for="nav-3-3">
      深度学习
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="2">
      <label class="md-nav__title" for="nav-3-3">
        深度学习
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../书籍/深度学习/DeepLearning/" title="深度学习中文版" class="md-nav__link">
      深度学习中文版
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-4" type="checkbox" id="nav-4" checked>
    
    <label class="md-nav__link" for="nav-4">
      学习
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-4">
        学习
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../" title="Home" class="md-nav__link">
      Home
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-4-2" type="checkbox" id="nav-4-2">
    
    <label class="md-nav__link" for="nav-4-2">
      tensorflow
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="2">
      <label class="md-nav__title" for="nav-4-2">
        tensorflow
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../tensorflow/如何选择优化器 optimizer/" title="如何选择优化器 optimizer" class="md-nav__link">
      如何选择优化器 optimizer
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-4-3" type="checkbox" id="nav-4-3">
    
    <label class="md-nav__link" for="nav-4-3">
      深度学习
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="2">
      <label class="md-nav__title" for="nav-4-3">
        深度学习
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../深度学习/02激活函数/" title="02激活函数" class="md-nav__link">
      02激活函数
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../深度学习/04损失函数/" title="04损失函数" class="md-nav__link">
      04损失函数
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../深度学习/主成分分析/" title="主成分分析" class="md-nav__link">
      主成分分析
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../深度学习/最小二乘法/" title="最小二乘法" class="md-nav__link">
      最小二乘法
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../深度学习/矩阵求导/" title="前提及说明" class="md-nav__link">
      前提及说明
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-4-4" type="checkbox" id="nav-4-4" checked>
    
    <label class="md-nav__link" for="nav-4-4">
      转载
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="2">
      <label class="md-nav__title" for="nav-4-4">
        转载
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-4-4-1" type="checkbox" id="nav-4-4-1" checked>
    
    <label class="md-nav__link" for="nav-4-4-1">
      cs231n-ch
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="3">
      <label class="md-nav__title" for="nav-4-4-1">
        cs231n-ch
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../ch-assignment2_google_cloud/" title="Google Cloud Tutorial Part 2 (with GPUs)" class="md-nav__link">
      Google Cloud Tutorial Part 2 (with GPUs)
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../ch-aws-tutorial/" title="aws-tutorial" class="md-nav__link">
      aws-tutorial
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../ch-classification/" title="classification" class="md-nav__link">
      classification
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../ch-convnet-tips/" title="convnet-tips" class="md-nav__link">
      convnet-tips
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../ch-convolutional-networks/" title="convolutional-networks" class="md-nav__link">
      convolutional-networks
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../ch-google_cloud_tutorial/" title="Google Cloud Tutorial gce-tutorial" class="md-nav__link">
      Google Cloud Tutorial gce-tutorial
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../ch-ipython-tutorial/" title="IPython Tutorial ipython-tutorial" class="md-nav__link">
      IPython Tutorial ipython-tutorial
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../ch-linear-classify/" title="线性分类" class="md-nav__link">
      线性分类
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../ch-neural-networks-1/" title="neural-networks-1" class="md-nav__link">
      neural-networks-1
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../ch-neural-networks-2/" title="neural-networks-2" class="md-nav__link">
      neural-networks-2
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../ch-neural-networks-3/" title="neural-networks-3" class="md-nav__link">
      neural-networks-3
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../ch-neural-networks-case-study/" title="neural-networks-case-study" class="md-nav__link">
      neural-networks-case-study
    </a>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="toc">
    
    
      <label class="md-nav__link md-nav__link--active" for="toc">
        optimization-1
      </label>
    
    <a href="./" title="optimization-1" class="md-nav__link md-nav__link--active">
      optimization-1
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
  
    <label class="md-nav__title" for="toc">目录</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#introduction" title="Introduction" class="md-nav__link">
    Introduction
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#visualizing-the-loss-function" title="Visualizing the loss function" class="md-nav__link">
    Visualizing the loss function
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#optimization" title="Optimization" class="md-nav__link">
    Optimization
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#strategy-1-a-first-very-bad-idea-solution-random-search" title="Strategy #1: A first very bad idea solution: Random search" class="md-nav__link">
    Strategy #1: A first very bad idea solution: Random search
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#strategy-2-random-local-search" title="Strategy #2: Random Local Search" class="md-nav__link">
    Strategy #2: Random Local Search
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#strategy-3-following-the-gradient" title="Strategy #3: Following the Gradient" class="md-nav__link">
    Strategy #3: Following the Gradient
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#computing-the-gradient" title="Computing the gradient" class="md-nav__link">
    Computing the gradient
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#computing-the-gradient-numerically-with-finite-differences" title="Computing the gradient numerically with finite differences" class="md-nav__link">
    Computing the gradient numerically with finite differences
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#computing-the-gradient-analytically-with-calculus" title="Computing the gradient analytically with Calculus" class="md-nav__link">
    Computing the gradient analytically with Calculus
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#gradient-descent" title="Gradient Descent" class="md-nav__link">
    Gradient Descent
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#summary" title="Summary" class="md-nav__link">
    Summary
  </a>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
    
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../ch-optimization-2/" title="Ch optimization 2" class="md-nav__link">
      Ch optimization 2
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../ch-overview/" title="Ch overview" class="md-nav__link">
      Ch overview
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../ch-poster/" title="Ch poster" class="md-nav__link">
      Ch poster
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../ch-python-numpy-tutorial/" title="Ch python numpy tutorial" class="md-nav__link">
      Ch python numpy tutorial
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../ch-Readme/" title="ch Readme" class="md-nav__link">
      ch Readme
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../ch-transfer-learning/" title="Ch transfer learning" class="md-nav__link">
      Ch transfer learning
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../ch-understanding-cnn/" title="Ch understanding cnn" class="md-nav__link">
      Ch understanding cnn
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../terminal-tutorial/" title="Terminal tutorial" class="md-nav__link">
      Terminal tutorial
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-4-4-2" type="checkbox" id="nav-4-4-2">
    
    <label class="md-nav__link" for="nav-4-4-2">
      cs231n-en
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="3">
      <label class="md-nav__title" for="nav-4-4-2">
        cs231n-en
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../cs231n-en/en-assignment2_google_cloud/" title="En assignment2 google cloud" class="md-nav__link">
      En assignment2 google cloud
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../cs231n-en/en-aws-tutorial/" title="En aws tutorial" class="md-nav__link">
      En aws tutorial
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../cs231n-en/en-classification/" title="En classification" class="md-nav__link">
      En classification
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../cs231n-en/en-convnet-tips/" title="En convnet tips" class="md-nav__link">
      En convnet tips
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../cs231n-en/en-convolutional-networks/" title="En convolutional networks" class="md-nav__link">
      En convolutional networks
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../cs231n-en/en-google_cloud_tutorial/" title="En google cloud tutorial" class="md-nav__link">
      En google cloud tutorial
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../cs231n-en/en-ipython-tutorial/" title="En ipython tutorial" class="md-nav__link">
      En ipython tutorial
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../cs231n-en/en-linear-classify/" title="En linear classify" class="md-nav__link">
      En linear classify
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../cs231n-en/en-neural-networks-1/" title="En neural networks 1" class="md-nav__link">
      En neural networks 1
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../cs231n-en/en-neural-networks-2/" title="En neural networks 2" class="md-nav__link">
      En neural networks 2
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../cs231n-en/en-neural-networks-3/" title="En neural networks 3" class="md-nav__link">
      En neural networks 3
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../cs231n-en/en-neural-networks-case-study/" title="En neural networks case study" class="md-nav__link">
      En neural networks case study
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../cs231n-en/en-optimization-1/" title="En optimization 1" class="md-nav__link">
      En optimization 1
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../cs231n-en/en-optimization-2/" title="En optimization 2" class="md-nav__link">
      En optimization 2
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../cs231n-en/en-overview/" title="En overview" class="md-nav__link">
      En overview
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../cs231n-en/en-poster/" title="En poster" class="md-nav__link">
      En poster
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../cs231n-en/en-python-numpy-tutorial/" title="En python numpy tutorial" class="md-nav__link">
      En python numpy tutorial
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../cs231n-en/en-Readme/" title="en Readme" class="md-nav__link">
      en Readme
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../cs231n-en/en-terminal-tutorial/" title="En terminal tutorial" class="md-nav__link">
      En terminal tutorial
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../cs231n-en/en-transfer-learning/" title="En transfer learning" class="md-nav__link">
      En transfer learning
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../cs231n-en/en-understanding-cnn/" title="En understanding cnn" class="md-nav__link">
      En understanding cnn
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-4-4-3" type="checkbox" id="nav-4-4-3">
    
    <label class="md-nav__link" for="nav-4-4-3">
      example
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="3">
      <label class="md-nav__title" for="nav-4-4-3">
        example
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../example/a-web-crawer-with-a-asyncio-coroutines-1/" title="A web crawer with a asyncio coroutines 1" class="md-nav__link">
      A web crawer with a asyncio coroutines 1
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../example/a-web-crawer-with-a-asyncio-coroutines-2/" title="A web crawer with a asyncio coroutines 2" class="md-nav__link">
      A web crawer with a asyncio coroutines 2
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../example/bias-variance/" title="Bias variance" class="md-nav__link">
      Bias variance
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../example/bitcoin-explained-1/" title="Bitcoin explained 1" class="md-nav__link">
      Bitcoin explained 1
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../example/dive-into-gradient-decent/" title="Dive into gradient decent" class="md-nav__link">
      Dive into gradient decent
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../example/ethereum-ultimate-guide/" title="Ethereum ultimate guide" class="md-nav__link">
      Ethereum ultimate guide
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../example/fork-exec-source/" title="Fork exec source" class="md-nav__link">
      Fork exec source
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../example/latex语法/" title="Latex语法" class="md-nav__link">
      Latex语法
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../example/latex语法1/" title="Latex语法1" class="md-nav__link">
      Latex语法1
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../example/nosql-in-python/" title="Nosql in python" class="md-nav__link">
      Nosql in python
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../example/perceptron/" title="Perceptron" class="md-nav__link">
      Perceptron
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../example/quick-latex/" title="Quick latex" class="md-nav__link">
      Quick latex
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../example/tendermint/" title="Tendermint" class="md-nav__link">
      Tendermint
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-4-4-4" type="checkbox" id="nav-4-4-4">
    
    <label class="md-nav__link" for="nav-4-4-4">
      example2
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="3">
      <label class="md-nav__title" for="nav-4-4-4">
        example2
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../example2/arrays-similar/" title="Arrays similar" class="md-nav__link">
      Arrays similar
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../example2/baidu-ife-1-HTML&CSS/" title="baidu ife 1 HTML&CSS" class="md-nav__link">
      baidu ife 1 HTML&CSS
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../example2/baidu-ife-2-javascript/" title="Baidu ife 2 javascript" class="md-nav__link">
      Baidu ife 2 javascript
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../example2/create-my-blog-with-jekyll/" title="Create my blog with jekyll" class="md-nav__link">
      Create my blog with jekyll
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../example2/front-end-tools/" title="Front end tools" class="md-nav__link">
      Front end tools
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../example2/git-clone-not-master-branch/" title="Git clone not master branch" class="md-nav__link">
      Git clone not master branch
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../example2/History-API/" title="History API" class="md-nav__link">
      History API
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../example2/how-to-use-babel/" title="How to use babel" class="md-nav__link">
      How to use babel
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../example2/how-to-write-a-count-down/" title="How to write a count down" class="md-nav__link">
      How to write a count down
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../example2/JavaScript-closure/" title="JavaScript closure" class="md-nav__link">
      JavaScript closure
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../example2/JavaScript-function/" title="JavaScript function" class="md-nav__link">
      JavaScript function
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../example2/JavaScript-good-parts-note1/" title="JavaScript good parts note1" class="md-nav__link">
      JavaScript good parts note1
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../example2/JavaScript-good-parts-note2/" title="JavaScript good parts note2" class="md-nav__link">
      JavaScript good parts note2
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../example2/JavaScript-good-parts-note3/" title="JavaScript good parts note3" class="md-nav__link">
      JavaScript good parts note3
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../example2/JavaScript-Net/" title="JavaScript Net" class="md-nav__link">
      JavaScript Net
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../example2/JavaScript-Object-Oriented/" title="JavaScript Object Oriented" class="md-nav__link">
      JavaScript Object Oriented
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../example2/JavaScript-this/" title="JavaScript this" class="md-nav__link">
      JavaScript this
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../example2/jekyll-theme-version-2.0/" title="Jekyll theme version 2.0" class="md-nav__link">
      Jekyll theme version 2.0
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../example2/js-create-file-and-download/" title="Js create file and download" class="md-nav__link">
      Js create file and download
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../example2/low-IE-click-empty-block-bug/" title="low IE click empty block bug" class="md-nav__link">
      low IE click empty block bug
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../example2/regular-expression-group/" title="Regular expression group" class="md-nav__link">
      Regular expression group
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../example2/scope/" title="Scope" class="md-nav__link">
      Scope
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../example2/shuffle-algorithm/" title="Shuffle algorithm" class="md-nav__link">
      Shuffle algorithm
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../example2/sublimeLinter/" title="sublimeLinter" class="md-nav__link">
      sublimeLinter
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../example2/Syncing-a-fork/" title="Syncing a fork" class="md-nav__link">
      Syncing a fork
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../example2/teach-girlfriend-html-css/" title="Teach girlfriend html css" class="md-nav__link">
      Teach girlfriend html css
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../example2/web-app/" title="Web app" class="md-nav__link">
      Web app
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../example2/weinre/" title="Weinre" class="md-nav__link">
      Weinre
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
      </ul>
    </nav>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-5" type="checkbox" id="nav-5">
    
    <label class="md-nav__link" for="nav-5">
      工具
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-5">
        工具
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../工具/About/" title="About" class="md-nav__link">
      About
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../工具/Cheatsheet/" title="Cheatsheet" class="md-nav__link">
      Cheatsheet
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../工具/" title="Home" class="md-nav__link">
      Home
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../工具/shell显示时间/" title="Shell显示时间" class="md-nav__link">
      Shell显示时间
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-5-5" type="checkbox" id="nav-5-5">
    
    <label class="md-nav__link" for="nav-5-5">
      常用工具
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="2">
      <label class="md-nav__title" for="nav-5-5">
        常用工具
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../工具/常用工具/10分钟上手Latex/" title="10分钟上手Latex" class="md-nav__link">
      10分钟上手Latex
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../工具/常用工具/10分钟上手Pandas/" title="10分钟上手Pandas" class="md-nav__link">
      10分钟上手Pandas
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../工具/常用工具/10分钟上手Python/" title="10分钟上手Python" class="md-nav__link">
      10分钟上手Python
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-6" type="checkbox" id="nav-6">
    
    <label class="md-nav__link" for="nav-6">
      推荐
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-6">
        推荐
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../推荐/个人理解的/" title="个人理解的" class="md-nav__link">
      个人理解的
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-6-2" type="checkbox" id="nav-6-2">
    
    <label class="md-nav__link" for="nav-6-2">
      推荐框架
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="2">
      <label class="md-nav__title" for="nav-6-2">
        推荐框架
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../推荐/推荐框架/" title="Home" class="md-nav__link">
      Home
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../推荐/推荐框架/LibRec/" title="LibRec" class="md-nav__link">
      LibRec
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-6-2-3" type="checkbox" id="nav-6-2-3">
    
    <label class="md-nav__link" for="nav-6-2-3">
      LibRec介绍文档
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="3">
      <label class="md-nav__title" for="nav-6-2-3">
        LibRec介绍文档
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../推荐/推荐框架/LibRec介绍文档/AlgorithmList/" title="AlgorithmList" class="md-nav__link">
      AlgorithmList
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../推荐/推荐框架/LibRec介绍文档/CLIWalkthrough/" title="CLIWalkthrough" class="md-nav__link">
      CLIWalkthrough
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../推荐/推荐框架/LibRec介绍文档/ConfigurationList/" title="ConfigurationList" class="md-nav__link">
      ConfigurationList
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../推荐/推荐框架/LibRec介绍文档/Context/" title="Context" class="md-nav__link">
      Context
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../推荐/推荐框架/LibRec介绍文档/DataFileFormat/" title="DataFileFormat" class="md-nav__link">
      DataFileFormat
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../推荐/推荐框架/LibRec介绍文档/DataModel/" title="DataModel" class="md-nav__link">
      DataModel
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../推荐/推荐框架/LibRec介绍文档/Evaluator/" title="Evaluator" class="md-nav__link">
      Evaluator
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../推荐/推荐框架/LibRec介绍文档/FilmTrust/" title="FilmTrust" class="md-nav__link">
      FilmTrust
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../推荐/推荐框架/LibRec介绍文档/Filter/" title="Filter" class="md-nav__link">
      Filter
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../推荐/推荐框架/LibRec介绍文档/Introduction/" title="Introduction" class="md-nav__link">
      Introduction
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../推荐/推荐框架/LibRec介绍文档/LibrecDocument/" title="LibrecDocument" class="md-nav__link">
      LibrecDocument
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../推荐/推荐框架/LibRec介绍文档/Recommender/" title="Recommender" class="md-nav__link">
      Recommender
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../推荐/推荐框架/YouTube/" title="YouTube" class="md-nav__link">
      YouTube
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-7" type="checkbox" id="nav-7">
    
    <label class="md-nav__link" for="nav-7">
      文章
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-7">
        文章
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../文章/" title="Home" class="md-nav__link">
      Home
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-8" type="checkbox" id="nav-8">
    
    <label class="md-nav__link" for="nav-8">
      日常
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-8">
        日常
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../日常/AboutMe/" title="AboutMe" class="md-nav__link">
      AboutMe
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../日常/" title="Home" class="md-nav__link">
      Home
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-8-3" type="checkbox" id="nav-8-3">
    
    <label class="md-nav__link" for="nav-8-3">
      test
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="2">
      <label class="md-nav__title" for="nav-8-3">
        test
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../日常/test/testtest/" title="Testtest" class="md-nav__link">
      Testtest
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
      </ul>
    </nav>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
  
    <label class="md-nav__title" for="toc">目录</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#introduction" title="Introduction" class="md-nav__link">
    Introduction
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#visualizing-the-loss-function" title="Visualizing the loss function" class="md-nav__link">
    Visualizing the loss function
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#optimization" title="Optimization" class="md-nav__link">
    Optimization
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#strategy-1-a-first-very-bad-idea-solution-random-search" title="Strategy #1: A first very bad idea solution: Random search" class="md-nav__link">
    Strategy #1: A first very bad idea solution: Random search
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#strategy-2-random-local-search" title="Strategy #2: Random Local Search" class="md-nav__link">
    Strategy #2: Random Local Search
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#strategy-3-following-the-gradient" title="Strategy #3: Following the Gradient" class="md-nav__link">
    Strategy #3: Following the Gradient
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#computing-the-gradient" title="Computing the gradient" class="md-nav__link">
    Computing the gradient
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#computing-the-gradient-numerically-with-finite-differences" title="Computing the gradient numerically with finite differences" class="md-nav__link">
    Computing the gradient numerically with finite differences
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#computing-the-gradient-analytically-with-calculus" title="Computing the gradient analytically with Calculus" class="md-nav__link">
    Computing the gradient analytically with Calculus
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#gradient-descent" title="Gradient Descent" class="md-nav__link">
    Gradient Descent
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#summary" title="Summary" class="md-nav__link">
    Summary
  </a>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                
                  <h1>optimization-1</h1>
                
                <ul>
<li>content
{:toc}</li>
</ul>
<p>Table of Contents:</p>
<ul>
<li><a href="#intro">Introduction</a></li>
<li><a href="#vis">Visualizing the loss function</a></li>
<li><a href="#optimization">Optimization</a></li>
<li><a href="#opt1">Strategy #1: Random Search</a></li>
<li><a href="#opt2">Strategy #2: Random Local Search</a></li>
<li><a href="#opt3">Strategy #3: Following the gradient</a></li>
<li><a href="#gradcompute">Computing the gradient</a></li>
<li><a href="#numerical">Numerically with finite differences</a></li>
<li><a href="#analytic">Analytically with calculus</a></li>
<li><a href="#gd">Gradient descent</a></li>
<li><a href="#summary">Summary</a></li>
</ul>
<p><a name='intro'></a></p>
<h3 id="introduction">Introduction<a class="headerlink" href="#introduction" title="Permanent link">&para;</a></h3>
<p>In the previous section we introduced two key components in context of the image classification task:</p>
<ol>
<li>A (parameterized) <strong>score function</strong> mapping the raw image pixels to class scores (e.g. a linear function)</li>
<li>A <strong>loss function</strong> that measured the quality of a particular set of parameters based on how well the induced scores agreed with the ground truth labels in the training data. We saw that there are many ways and versions of this (e.g. Softmax/SVM).</li>
</ol>
<p>Concretely, recall that the linear function had the form $ f(x_i, W) =  W x_i $ and the SVM we developed was formulated as:</p>
<div>
<div class="MathJax_Preview">
L = \frac{1}{N} \sum_i \sum_{j\neq y_i} \left[ \max(0, f(x_i; W)_j - f(x_i; W)_{y_i} + 1) \right] + \alpha R(W)
</div>
<script type="math/tex; mode=display">
L = \frac{1}{N} \sum_i \sum_{j\neq y_i} \left[ \max(0, f(x_i; W)_j - f(x_i; W)_{y_i} + 1) \right] + \alpha R(W)
</script>
</div>
<p>We saw that a setting of the parameters <span><span class="MathJax_Preview">W</span><script type="math/tex">W</script></span> that produced predictions for examples <span><span class="MathJax_Preview">x_i</span><script type="math/tex">x_i</script></span> consistent with their ground truth labels <span><span class="MathJax_Preview">y_i</span><script type="math/tex">y_i</script></span> would also have a very low loss <span><span class="MathJax_Preview">L</span><script type="math/tex">L</script></span>. We are now going to introduce the third and last key component: <strong>optimization</strong>. Optimization is the process of finding the set of parameters <span><span class="MathJax_Preview">W</span><script type="math/tex">W</script></span> that minimize the loss function.</p>
<p><strong>Foreshadowing:</strong> Once we understand how these three core components interact, we will revisit the first component (the parameterized function mapping) and extend it to functions much more complicated than a linear mapping: First entire Neural Networks, and then Convolutional Neural Networks. The loss functions and the optimization process will remain relatively unchanged.</p>
<p><a name='vis'></a></p>
<h3 id="visualizing-the-loss-function">Visualizing the loss function<a class="headerlink" href="#visualizing-the-loss-function" title="Permanent link">&para;</a></h3>
<p>The loss functions we'll look at in this class are usually defined over very high-dimensional spaces (e.g. in CIFAR-10 a linear classifier weight matrix is of size [10 x 3073] for a total of 30,730 parameters), making them difficult to visualize. However, we can still gain some intuitions about one by slicing through the high-dimensional space along rays (1 dimension), or along planes (2 dimensions). For example, we can generate a random weight matrix <span><span class="MathJax_Preview">W</span><script type="math/tex">W</script></span> (which corresponds to a single point in the space), then march along a ray and record the loss function value along the way. That is, we can generate a random direction <span><span class="MathJax_Preview">W_1</span><script type="math/tex">W_1</script></span> and compute the loss along this direction by evaluating <span><span class="MathJax_Preview">L(W + a W_1)</span><script type="math/tex">L(W + a W_1)</script></span> for different values of <span><span class="MathJax_Preview">a</span><script type="math/tex">a</script></span>. This process generates a simple plot with the value of <span><span class="MathJax_Preview">a</span><script type="math/tex">a</script></span> as the x-axis and the value of the loss function as the y-axis. We can also carry out the same procedure with two dimensions by evaluating the loss $ L(W + a W_1 + b W_2) $ as we vary <span><span class="MathJax_Preview">a, b</span><script type="math/tex">a, b</script></span>. In a plot, <span><span class="MathJax_Preview">a, b</span><script type="math/tex">a, b</script></span> could then correspond to the x-axis and the y-axis, and the value of the loss function can be visualized with a color:</p>
<div class="fig figcenter fighighlight">
  <img src="/Dairy/assets/images/cs231n/svm1d.png">
  <img src="/Dairy/assets/images/cs231n/svm_one.jpg">
  <img src="/Dairy/assets/images/cs231n/svm_all.jpg">
  <div class="figcaption">
    Loss function landscape for the Multiclass SVM (without regularization) for one single example (left,middle) and for a hundred examples (right) in CIFAR-10. Left: one-dimensional loss by only varying <b>a</b>. Middle, Right: two-dimensional loss slice, Blue = low loss, Red = high loss. Notice the piecewise-linear structure of the loss function. The losses for multiple examples are combined with average, so the bowl shape on the right is the average of many piece-wise linear bowls (such as the one in the middle).
  </div>
</div>

<p>We can explain the piecewise-linear structure of the loss function by examining the math. For a single example we have:</p>
<div>
<div class="MathJax_Preview">
L_i = \sum_{j\neq y_i} \left[ \max(0, w_j^Tx_i - w_{y_i}^Tx_i + 1) \right]
</div>
<script type="math/tex; mode=display">
L_i = \sum_{j\neq y_i} \left[ \max(0, w_j^Tx_i - w_{y_i}^Tx_i + 1) \right]
</script>
</div>
<p>It is clear from the equation that the data loss for each example is a sum of (zero-thresholded due to the <span><span class="MathJax_Preview">\max(0,-)</span><script type="math/tex">\max(0,-)</script></span> function) linear functions of <span><span class="MathJax_Preview">W</span><script type="math/tex">W</script></span>. Moreover, each row of <span><span class="MathJax_Preview">W</span><script type="math/tex">W</script></span> (i.e. <span><span class="MathJax_Preview">w_j</span><script type="math/tex">w_j</script></span>) sometimes has a positive sign in front of it (when it corresponds to a wrong class for an example), and sometimes a negative sign (when it corresponds to the correct class for that example). To make this more explicit, consider a simple dataset that contains three 1-dimensional points and three classes. The full SVM loss (without regularization) becomes:</p>
<div>
<div class="MathJax_Preview">
\begin{align}
L_0 = &amp; \max(0, w_1^Tx_0 - w_0^Tx_0 + 1) + \max(0, w_2^Tx_0 - w_0^Tx_0 + 1) \\\\
L_1 = &amp; \max(0, w_0^Tx_1 - w_1^Tx_1 + 1) + \max(0, w_2^Tx_1 - w_1^Tx_1 + 1) \\\\
L_2 = &amp; \max(0, w_0^Tx_2 - w_2^Tx_2 + 1) + \max(0, w_1^Tx_2 - w_2^Tx_2 + 1) \\\\
L = &amp; (L_0 + L_1 + L_2)/3
\end{align}
</div>
<script type="math/tex; mode=display">
\begin{align}
L_0 = & \max(0, w_1^Tx_0 - w_0^Tx_0 + 1) + \max(0, w_2^Tx_0 - w_0^Tx_0 + 1) \\\\
L_1 = & \max(0, w_0^Tx_1 - w_1^Tx_1 + 1) + \max(0, w_2^Tx_1 - w_1^Tx_1 + 1) \\\\
L_2 = & \max(0, w_0^Tx_2 - w_2^Tx_2 + 1) + \max(0, w_1^Tx_2 - w_2^Tx_2 + 1) \\\\
L = & (L_0 + L_1 + L_2)/3
\end{align}
</script>
</div>
<p>Since these examples are 1-dimensional, the data <span><span class="MathJax_Preview">x_i</span><script type="math/tex">x_i</script></span> and weights <span><span class="MathJax_Preview">w_j</span><script type="math/tex">w_j</script></span> are numbers. Looking at, for instance, <span><span class="MathJax_Preview">w_0</span><script type="math/tex">w_0</script></span>, some terms above are linear functions of <span><span class="MathJax_Preview">w_0</span><script type="math/tex">w_0</script></span> and each is clamped at zero. We can visualize this as follows:</p>
<div class="fig figcenter fighighlight">
  <img src="/Dairy/assets/images/cs231n/svmbowl.png">
  <div class="figcaption">
    1-dimensional illustration of the data loss. The x-axis is a single weight and the y-axis is the loss. The data loss is a sum of multiple terms, each of which is either independent of a particular weight, or a linear function of it that is thresholded at zero. The full SVM data loss is a 30,730-dimensional version of this shape.
  </div>
</div>

<p>As an aside, you may have guessed from its bowl-shaped appearance that the SVM cost function is an example of a <a href="http://en.wikipedia.org/wiki/Convex_function">convex function</a> There is a large amount of literature devoted to efficiently minimizing these types of functions, and you can also take a Stanford class on the topic ( <a href="http://stanford.edu/~boyd/cvxbook/">convex optimization</a> ). Once we extend our score functions <span><span class="MathJax_Preview">f</span><script type="math/tex">f</script></span> to Neural Networks our objective functions will become non-convex, and the visualizations above will not feature bowls but complex, bumpy terrains.</p>
<p><em>Non-differentiable loss functions</em>. As a technical note, you can also see that the <em>kinks</em> in the loss function (due to the max operation) technically make the loss function non-differentiable because at these kinks the gradient is not defined. However, the <a href="http://en.wikipedia.org/wiki/Subderivative">subgradient</a> still exists and is commonly used instead. In this class will use the terms <em>subgradient</em> and <em>gradient</em> interchangeably.</p>
<p><a name='optimization'></a></p>
<h3 id="optimization">Optimization<a class="headerlink" href="#optimization" title="Permanent link">&para;</a></h3>
<p>To reiterate, the loss function lets us quantify the quality of any particular set of weights <strong>W</strong>. The goal of optimization is to find <strong>W</strong> that minimizes the loss function. We will now motivate and slowly develop an approach to optimizing the loss function. For those of you coming to this class with previous experience, this section might seem odd since the working example we'll use (the SVM loss) is a convex problem, but keep in mind that our goal is to eventually optimize Neural Networks where we can't easily use any of the tools developed in the Convex Optimization literature.</p>
<p><a name='opt1'></a></p>
<h4 id="strategy-1-a-first-very-bad-idea-solution-random-search">Strategy #1: A first very bad idea solution: Random search<a class="headerlink" href="#strategy-1-a-first-very-bad-idea-solution-random-search" title="Permanent link">&para;</a></h4>
<p>Since it is so simple to check how good a given set of parameters <strong>W</strong> is, the first (very bad) idea that may come to mind is to simply try out many different random weights and keep track of what works best. This procedure might look as follows:</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># assume X_train is the data where each column is an example (e.g. 3073 x 50,000)</span>
<span class="c1"># assume Y_train are the labels (e.g. 1D array of 50,000)</span>
<span class="c1"># assume the function L evaluates the loss function</span>

<span class="n">bestloss</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">)</span> <span class="c1"># Python assigns the highest possible float value</span>
<span class="k">for</span> <span class="n">num</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
  <span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3073</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.0001</span> <span class="c1"># generate random parameters</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="n">L</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span> <span class="c1"># get the loss over the entire training set</span>
  <span class="k">if</span> <span class="n">loss</span> <span class="o">&lt;</span> <span class="n">bestloss</span><span class="p">:</span> <span class="c1"># keep track of the best solution</span>
    <span class="n">bestloss</span> <span class="o">=</span> <span class="n">loss</span>
    <span class="n">bestW</span> <span class="o">=</span> <span class="n">W</span>
  <span class="k">print</span> <span class="s1">&#39;in attempt </span><span class="si">%d</span><span class="s1"> the loss was </span><span class="si">%f</span><span class="s1">, best </span><span class="si">%f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">num</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">bestloss</span><span class="p">)</span>

<span class="c1"># prints:</span>
<span class="c1"># in attempt 0 the loss was 9.401632, best 9.401632</span>
<span class="c1"># in attempt 1 the loss was 8.959668, best 8.959668</span>
<span class="c1"># in attempt 2 the loss was 9.044034, best 8.959668</span>
<span class="c1"># in attempt 3 the loss was 9.278948, best 8.959668</span>
<span class="c1"># in attempt 4 the loss was 8.857370, best 8.857370</span>
<span class="c1"># in attempt 5 the loss was 8.943151, best 8.857370</span>
<span class="c1"># in attempt 6 the loss was 8.605604, best 8.605604</span>
<span class="c1"># ... (trunctated: continues for 1000 lines)</span>
</pre></div>
</td></tr></table>

<p>In the code above, we see that we tried out several random weight vectors <strong>W</strong>, and some of them work better than others. We can take the best weights <strong>W</strong> found by this search and try it out on the test set:</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3
4
5
6
7</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Assume X_test is [3073 x 10000], Y_test [10000 x 1]</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">Wbest</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Xte_cols</span><span class="p">)</span> <span class="c1"># 10 x 10000, the class scores for all test examples</span>
<span class="c1"># find the index with max score in each column (the predicted class)</span>
<span class="n">Yte_predict</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
<span class="c1"># and calculate accuracy (fraction of predictions that are correct)</span>
<span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">Yte_predict</span> <span class="o">==</span> <span class="n">Yte</span><span class="p">)</span>
<span class="c1"># returns 0.1555</span>
</pre></div>
</td></tr></table>

<p>With the best <strong>W</strong> this gives an accuracy of about <strong>15.5%</strong>. Given that guessing classes completely at random achieves only 10%, that's not a very bad outcome for a such a brain-dead random search solution!</p>
<p><strong>Core idea: iterative refinement</strong>. Of course, it turns out that we can do much better. The core idea is that finding the best set of weights <strong>W</strong> is a very difficult or even impossible problem (especially once <strong>W</strong> contains weights for entire complex neural networks), but the problem of refining a specific set of weights <strong>W</strong> to be slightly better is significantly less difficult. In other words, our approach will be to start with a random <strong>W</strong> and then iteratively refine it, making it slightly better each time.</p>
<blockquote>
<p>Our strategy will be to start with random weights and iteratively refine them over time to get lower loss</p>
</blockquote>
<p><strong>Blindfolded hiker analogy.</strong> One analogy that you may find helpful going forward is to think of yourself as hiking on a hilly terrain with a blindfold on, and trying to reach the bottom. In the example of CIFAR-10, the hills are 30,730-dimensional, since the dimensions of <strong>W</strong> are 10 x 3073. At every point on the hill we achieve a particular loss (the height of the terrain).</p>
<p><a name='opt2'></a></p>
<h4 id="strategy-2-random-local-search">Strategy #2: Random Local Search<a class="headerlink" href="#strategy-2-random-local-search" title="Permanent link">&para;</a></h4>
<p>The first strategy you may think of is to try to extend one foot in a random direction and then take a step only if it leads downhill. Concretely, we will start out with a random <span><span class="MathJax_Preview">W</span><script type="math/tex">W</script></span>, generate random perturbations $ \delta W $ to it and if the loss at the perturbed <span><span class="MathJax_Preview">W + \delta W</span><script type="math/tex">W + \delta W</script></span> is lower, we will perform an update. The code for this procedure is as follows:</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3073</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.001</span> <span class="c1"># generate random starting W</span>
<span class="n">bestloss</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
  <span class="n">step_size</span> <span class="o">=</span> <span class="mf">0.0001</span>
  <span class="n">Wtry</span> <span class="o">=</span> <span class="n">W</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3073</span><span class="p">)</span> <span class="o">*</span> <span class="n">step_size</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="n">L</span><span class="p">(</span><span class="n">Xtr_cols</span><span class="p">,</span> <span class="n">Ytr</span><span class="p">,</span> <span class="n">Wtry</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">loss</span> <span class="o">&lt;</span> <span class="n">bestloss</span><span class="p">:</span>
    <span class="n">W</span> <span class="o">=</span> <span class="n">Wtry</span>
    <span class="n">bestloss</span> <span class="o">=</span> <span class="n">loss</span>
  <span class="k">print</span> <span class="s1">&#39;iter </span><span class="si">%d</span><span class="s1"> loss is </span><span class="si">%f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">bestloss</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<p>Using the same number of loss function evaluations as before (1000), this approach achieves test set classification accuracy of <strong>21.4%</strong>. This is better, but still wasteful and computationally expensive.</p>
<p><a name='opt3'></a></p>
<h4 id="strategy-3-following-the-gradient">Strategy #3: Following the Gradient<a class="headerlink" href="#strategy-3-following-the-gradient" title="Permanent link">&para;</a></h4>
<p>In the previous section we tried to find a direction in the weight-space that would improve our weight vector (and give us a lower loss). It turns out that there is no need to randomly search for a good direction: we can compute the <em>best</em> direction along which we should change our weight vector that is mathematically guaranteed to be the direction of the steepest descend (at least in the limit as the step size goes towards zero). This direction will be related to the <strong>gradient</strong> of the loss function. In our hiking analogy, this approach roughly corresponds to feeling the slope of the hill below our feet and stepping down the direction that feels steepest.</p>
<p>In one-dimensional functions, the slope is the instantaneous rate of change of the function at any point you might be interested in. The gradient is a generalization of slope for functions that don't take a single number but a vector of numbers. Additionally, the gradient is just a vector of slopes (more commonly referred to as <strong>derivatives</strong>) for each dimension in the input space. The mathematical expression for the derivative of a 1-D function with respect its input is:</p>
<div>
<div class="MathJax_Preview">
\frac{df(x)}{dx} = \lim_{h\ \to 0} \frac{f(x + h) - f(x)}{h}
</div>
<script type="math/tex; mode=display">
\frac{df(x)}{dx} = \lim_{h\ \to 0} \frac{f(x + h) - f(x)}{h}
</script>
</div>
<p>When the functions of interest take a vector of numbers instead of a single number, we call the derivatives <strong>partial derivatives</strong>, and the gradient is simply the vector of partial derivatives in each dimension.</p>
<p><a name='gradcompute'></a></p>
<h3 id="computing-the-gradient">Computing the gradient<a class="headerlink" href="#computing-the-gradient" title="Permanent link">&para;</a></h3>
<p>There are two ways to compute the gradient: A slow, approximate but easy way (<strong>numerical gradient</strong>), and a fast, exact but more error-prone way that requires calculus (<strong>analytic gradient</strong>). We will now present both.</p>
<p><a name='numerical'></a></p>
<h4 id="computing-the-gradient-numerically-with-finite-differences">Computing the gradient numerically with finite differences<a class="headerlink" href="#computing-the-gradient-numerically-with-finite-differences" title="Permanent link">&para;</a></h4>
<p>The formula given above allows us to compute the gradient numerically. Here is a generic function that takes a function <code>f</code>, a vector <code>x</code> to evaluate the gradient on, and returns the gradient of <code>f</code> at <code>x</code>:</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">eval_numerical_gradient</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot; </span>
<span class="sd">  a naive implementation of numerical gradient of f at x </span>
<span class="sd">  - f should be a function that takes a single argument</span>
<span class="sd">  - x is the point (numpy array) to evaluate the gradient at</span>
<span class="sd">  &quot;&quot;&quot;</span> 

  <span class="n">fx</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># evaluate function value at original point</span>
  <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
  <span class="n">h</span> <span class="o">=</span> <span class="mf">0.00001</span>

  <span class="c1"># iterate over all indexes in x</span>
  <span class="n">it</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">flags</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;multi_index&#39;</span><span class="p">],</span> <span class="n">op_flags</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;readwrite&#39;</span><span class="p">])</span>
  <span class="k">while</span> <span class="ow">not</span> <span class="n">it</span><span class="o">.</span><span class="n">finished</span><span class="p">:</span>

    <span class="c1"># evaluate function at x+h</span>
    <span class="n">ix</span> <span class="o">=</span> <span class="n">it</span><span class="o">.</span><span class="n">multi_index</span>
    <span class="n">old_value</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span>
    <span class="n">x</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span> <span class="o">=</span> <span class="n">old_value</span> <span class="o">+</span> <span class="n">h</span> <span class="c1"># increment by h</span>
    <span class="n">fxh</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># evalute f(x + h)</span>
    <span class="n">x</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span> <span class="o">=</span> <span class="n">old_value</span> <span class="c1"># restore to previous value (very important!)</span>

    <span class="c1"># compute the partial derivative</span>
    <span class="n">grad</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">fxh</span> <span class="o">-</span> <span class="n">fx</span><span class="p">)</span> <span class="o">/</span> <span class="n">h</span> <span class="c1"># the slope</span>
    <span class="n">it</span><span class="o">.</span><span class="n">iternext</span><span class="p">()</span> <span class="c1"># step to next dimension</span>

  <span class="k">return</span> <span class="n">grad</span>
</pre></div>
</td></tr></table>

<p>Following the gradient formula we gave above, the code above iterates over all dimensions one by one, makes a small change <code>h</code> along that dimension and calculates the partial derivative of the loss function along that dimension by seeing how much the function changed. The variable <code>grad</code> holds the full gradient in the end. </p>
<p><strong>Practical considerations</strong>. Note that in the mathematical formulation the gradient is defined in the limit as <strong>h</strong> goes towards zero, but in practice it is often sufficient to use a very small value (such as 1e-5 as seen in the example). Ideally, you want to use the smallest step size that does not lead to numerical issues. Additionally, in practice it often works better to compute the numeric gradient using the <strong>centered difference formula</strong>: $ [f(x+h) - f(x-h)] / 2 h $ . See <a href="http://en.wikipedia.org/wiki/Numerical_differentiation">wiki</a> for details.</p>
<p>We can use the function given above to compute the gradient at any point and for any function. Lets compute the gradient for the CIFAR-10 loss function at some random point in the weight space:</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3
4
5
6
7</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># to use the generic code above we want a function that takes a single argument</span>
<span class="c1"># (the weights in our case) so we close over X_train and Y_train</span>
<span class="k">def</span> <span class="nf">CIFAR10_loss_fun</span><span class="p">(</span><span class="n">W</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">L</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span>

<span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3073</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.001</span> <span class="c1"># random weight vector</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">eval_numerical_gradient</span><span class="p">(</span><span class="n">CIFAR10_loss_fun</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span> <span class="c1"># get the gradient</span>
</pre></div>
</td></tr></table>

<p>The gradient tells us the slope of the loss function along every dimension, which we can use to make an update:</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">loss_original</span> <span class="o">=</span> <span class="n">CIFAR10_loss_fun</span><span class="p">(</span><span class="n">W</span><span class="p">)</span> <span class="c1"># the original loss</span>
<span class="k">print</span> <span class="s1">&#39;original loss: </span><span class="si">%f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">loss_original</span><span class="p">,</span> <span class="p">)</span>

<span class="c1"># lets see the effect of multiple step sizes</span>
<span class="k">for</span> <span class="n">step_size_log</span> <span class="ow">in</span> <span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="o">-</span><span class="mi">9</span><span class="p">,</span> <span class="o">-</span><span class="mi">8</span><span class="p">,</span> <span class="o">-</span><span class="mi">7</span><span class="p">,</span> <span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
  <span class="n">step_size</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">**</span> <span class="n">step_size_log</span>
  <span class="n">W_new</span> <span class="o">=</span> <span class="n">W</span> <span class="o">-</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">df</span> <span class="c1"># new position in the weight space</span>
  <span class="n">loss_new</span> <span class="o">=</span> <span class="n">CIFAR10_loss_fun</span><span class="p">(</span><span class="n">W_new</span><span class="p">)</span>
  <span class="k">print</span> <span class="s1">&#39;for step size </span><span class="si">%f</span><span class="s1"> new loss: </span><span class="si">%f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">step_size</span><span class="p">,</span> <span class="n">loss_new</span><span class="p">)</span>

<span class="c1"># prints:</span>
<span class="c1"># original loss: 2.200718</span>
<span class="c1"># for step size 1.000000e-10 new loss: 2.200652</span>
<span class="c1"># for step size 1.000000e-09 new loss: 2.200057</span>
<span class="c1"># for step size 1.000000e-08 new loss: 2.194116</span>
<span class="c1"># for step size 1.000000e-07 new loss: 2.135493</span>
<span class="c1"># for step size 1.000000e-06 new loss: 1.647802</span>
<span class="c1"># for step size 1.000000e-05 new loss: 2.844355</span>
<span class="c1"># for step size 1.000000e-04 new loss: 25.558142</span>
<span class="c1"># for step size 1.000000e-03 new loss: 254.086573</span>
<span class="c1"># for step size 1.000000e-02 new loss: 2539.370888</span>
<span class="c1"># for step size 1.000000e-01 new loss: 25392.214036</span>
</pre></div>
</td></tr></table>

<p><strong>Update in negative gradient direction</strong>. In the code above, notice that to compute <code>W_new</code> we are making an update in the negative direction of the gradient <code>df</code> since we wish our loss function to decrease, not increase.</p>
<p><strong>Effect of step size</strong>. The gradient tells us the direction in which the function has the steepest rate of increase, but it does not tell us how far along this direction we should step. As we will see later in the course, choosing the step size (also called the <em>learning rate</em>) will become one of the most important (and most headache-inducing) hyperparameter settings in training a neural network. In our blindfolded hill-descent analogy, we feel the hill below our feet sloping in some direction, but the step length we should take is uncertain. If we shuffle our feet carefully we can expect to make consistent but very small progress (this corresponds to having a small step size). Conversely, we can choose to make a large, confident step in an attempt to descend faster, but this may not pay off. As you can see in the code example above, at some point taking a bigger step gives a higher loss as we "overstep".</p>
<div class="fig figleft fighighlight">
  <img src="/Dairy/assets/images/cs231n/stepsize.jpg">
  <div class="figcaption">
    Visualizing the effect of step size. We start at some particular spot W and evaluate the gradient (or rather its negative - the white arrow) which tells us the direction of the steepest decrease in the loss function. Small steps are likely to lead to consistent but slow progress. Large steps can lead to better progress but are more risky. Note that eventually, for a large step size we will overshoot and make the loss worse. The step size (or as we will later call it - the <b>learning rate</b>) will become one of the most important hyperparameters that we will have to carefully tune.
  </div>
  <div style="clear:both;"></div>
</div>

<p><strong>A problem of efficiency</strong>. You may have noticed that evaluating the numerical gradient has complexity linear in the number of parameters. In our example we had 30730 parameters in total and therefore had to perform 30,731 evaluations of the loss function to evaluate the gradient and to perform only a single parameter update. This problem only gets worse, since modern Neural Networks can easily have tens of millions of parameters. Clearly, this strategy is not scalable and we need something better.</p>
<p><a name='analytic'></a></p>
<h4 id="computing-the-gradient-analytically-with-calculus">Computing the gradient analytically with Calculus<a class="headerlink" href="#computing-the-gradient-analytically-with-calculus" title="Permanent link">&para;</a></h4>
<p>The numerical gradient is very simple to compute using the finite difference approximation, but the downside is that it is approximate (since we have to pick a small value of <em>h</em>, while the true gradient is defined as the limit as <em>h</em> goes to zero), and that it is very computationally expensive to compute. The second way to compute the gradient is analytically using Calculus, which allows us to derive a direct formula for the gradient (no approximations) that is also very fast to compute. However, unlike the numerical gradient it can be more error prone to implement, which is why in practice it is very common to compute the analytic gradient and compare it to the numerical gradient to check the correctness of your implementation. This is called a <strong>gradient check</strong>.</p>
<p>Lets use the example of the SVM loss function for a single datapoint:</p>
<div>
<div class="MathJax_Preview">
L_i = \sum_{j\neq y_i} \left[ \max(0, w_j^Tx_i - w_{y_i}^Tx_i + \Delta) \right]
</div>
<script type="math/tex; mode=display">
L_i = \sum_{j\neq y_i} \left[ \max(0, w_j^Tx_i - w_{y_i}^Tx_i + \Delta) \right]
</script>
</div>
<p>We can differentiate the function with respect to the weights. For example, taking the gradient with respect to <span><span class="MathJax_Preview">w_{y_i}</span><script type="math/tex">w_{y_i}</script></span> we obtain:</p>
<div>
<div class="MathJax_Preview">
\nabla_{w_{y_i}} L_i = - \left( \sum_{j\neq y_i} \mathbb{1}(w_j^Tx_i - w_{y_i}^Tx_i + \Delta &gt; 0) \right) x_i
</div>
<script type="math/tex; mode=display">
\nabla_{w_{y_i}} L_i = - \left( \sum_{j\neq y_i} \mathbb{1}(w_j^Tx_i - w_{y_i}^Tx_i + \Delta > 0) \right) x_i
</script>
</div>
<p>where <span><span class="MathJax_Preview">\mathbb{1}</span><script type="math/tex">\mathbb{1}</script></span> is the indicator function that is one if the condition inside is true or zero otherwise. While the expression may look scary when it is written out, when you're implementing this in code you'd simply count the number of classes that didn't meet the desired margin (and hence contributed to the loss function) and then the data vector <span><span class="MathJax_Preview">x_i</span><script type="math/tex">x_i</script></span> scaled by this number is the gradient. Notice that this is the gradient only with respect to the row of <span><span class="MathJax_Preview">W</span><script type="math/tex">W</script></span> that corresponds to the correct class. For the other rows where $j \neq y_i $ the gradient is:</p>
<div>
<div class="MathJax_Preview">
\nabla_{w_j} L_i = \mathbb{1}(w_j^Tx_i - w_{y_i}^Tx_i + \Delta &gt; 0) x_i
</div>
<script type="math/tex; mode=display">
\nabla_{w_j} L_i = \mathbb{1}(w_j^Tx_i - w_{y_i}^Tx_i + \Delta > 0) x_i
</script>
</div>
<p>Once you derive the expression for the gradient it is straight-forward to implement the expressions and use them to perform the gradient update. </p>
<p><a name='gd'></a></p>
<h3 id="gradient-descent">Gradient Descent<a class="headerlink" href="#gradient-descent" title="Permanent link">&para;</a></h3>
<p>Now that we can compute the gradient of the loss function, the procedure of repeatedly evaluating the gradient and then performing a parameter update is called <em>Gradient Descent</em>. Its <strong>vanilla</strong> version looks as follows:</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3
4
5</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Vanilla Gradient Descent</span>

<span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
  <span class="n">weights_grad</span> <span class="o">=</span> <span class="n">evaluate_gradient</span><span class="p">(</span><span class="n">loss_fun</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
  <span class="n">weights</span> <span class="o">+=</span> <span class="o">-</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">weights_grad</span> <span class="c1"># perform parameter update</span>
</pre></div>
</td></tr></table>

<p>This simple loop is at the core of all Neural Network libraries. There are other ways of performing the optimization (e.g. LBFGS), but Gradient Descent is currently by far the most common and established way of optimizing Neural Network loss functions. Throughout the class we will put some bells and whistles on the details of this loop (e.g. the exact details of the update equation), but the core idea of following the gradient until we're happy with the results will remain the same.</p>
<p><strong>Mini-batch gradient descent.</strong> In large-scale applications (such as the ILSVRC challenge), the training data can have on order of millions of examples. Hence, it seems wasteful to compute the full loss function over the entire training set in order to perform only a single parameter update. A very common approach to addressing this challenge is to compute the gradient over <strong>batches</strong> of the training data. For example, in current state of the art ConvNets, a typical batch contains 256 examples from the entire training set of 1.2 million. This batch is then used to perform a parameter update:</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3
4
5
6</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Vanilla Minibatch Gradient Descent</span>

<span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
  <span class="n">data_batch</span> <span class="o">=</span> <span class="n">sample_training_data</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span> <span class="c1"># sample 256 examples</span>
  <span class="n">weights_grad</span> <span class="o">=</span> <span class="n">evaluate_gradient</span><span class="p">(</span><span class="n">loss_fun</span><span class="p">,</span> <span class="n">data_batch</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
  <span class="n">weights</span> <span class="o">+=</span> <span class="o">-</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">weights_grad</span> <span class="c1"># perform parameter update</span>
</pre></div>
</td></tr></table>

<p>The reason this works well is that the examples in the training data are correlated. To see this, consider the extreme case where all 1.2 million images in ILSVRC are in fact made up of exact duplicates of only 1000 unique images (one for each class, or in other words 1200 identical copies of each image). Then it is clear that the gradients we would compute for all 1200 identical copies would all be the same, and when we average the data loss over all 1.2 million images we would get the exact same loss as if we only evaluated on a small subset of 1000. In practice of course, the dataset would not contain duplicate images, the gradient from a mini-batch is a good approximation of the gradient of the full objective. Therefore, much faster convergence can be achieved in practice by evaluating the mini-batch gradients to perform more frequent parameter updates.</p>
<p>The extreme case of this is a setting where the mini-batch contains only a single example. This process is called <strong>Stochastic Gradient Descent (SGD)</strong> (or also sometimes <strong>on-line</strong> gradient descent). This is relatively less common to see because in practice due to vectorized code optimizations it can be computationally much more efficient to evaluate the gradient for 100 examples, than the gradient for one example 100 times. Even though SGD technically refers to using a single example at a time to evaluate the gradient, you will hear people use the term SGD even when referring to mini-batch gradient descent (i.e. mentions of MGD for "Minibatch Gradient Descent", or BGD for "Batch gradient descent" are rare to see), where it is usually assumed that mini-batches are used. The size of the mini-batch is a hyperparameter but it is not very common to cross-validate it. It is usually based on memory constraints (if any), or set to some value, e.g. 32, 64 or 128. We use powers of 2 in practice because many vectorized operation implementations work faster when their inputs are sized in powers of 2.</p>
<p><a name='summary'></a></p>
<h3 id="summary">Summary<a class="headerlink" href="#summary" title="Permanent link">&para;</a></h3>
<div class="fig figcenter fighighlight">
  <img src="/Dairy/assets/images/cs231n/dataflow.jpeg">
  <div class="figcaption">
    Summary of the information flow. The dataset of pairs of <b>(x,y)</b> is given and fixed. The weights start out as random numbers and can change. During the forward pass the score function computes class scores, stored in vector <b>f</b>. The loss function contains two components: The data loss computes the compatibility between the scores <b>f</b> and the labels <b>y</b>. The regularization loss is only a function of the weights. During Gradient Descent, we compute the gradient on the weights (and optionally on data if we wish) and use them to perform a parameter update during Gradient Descent.
  </div>
</div>

<p>In this section,</p>
<ul>
<li>We developed the intuition of the loss function as a <strong>high-dimensional optimization landscape</strong> in which we are trying to reach the bottom. The working analogy we developed was that of a blindfolded hiker who wishes to reach the bottom. In particular, we saw that the SVM cost function is piece-wise linear and bowl-shaped.</li>
<li>We motivated the idea of optimizing the loss function with
<strong>iterative refinement</strong>, where we start with a random set of weights and refine them step by step until the loss is minimized. </li>
<li>We saw that the <strong>gradient</strong> of a function gives the steepest ascent direction and we discussed a simple but inefficient way of computing it numerically using the finite difference approximation (the finite difference being the value of <em>h</em> used in computing the numerical gradient).</li>
<li>We saw that the parameter update requires a tricky setting of the <strong>step size</strong> (or the <strong>learning rate</strong>) that must be set just right: if it is too low the progress is steady but slow. If it is too high the progress can be faster, but more risky. We will explore this tradeoff in much more detail in future sections.</li>
<li>We discussed the tradeoffs between computing the <strong>numerical</strong> and <strong>analytic</strong> gradient. The numerical gradient is simple but it is approximate and expensive to compute. The analytic gradient is exact, fast to compute but more error-prone since it requires the derivation of the gradient with math. Hence, in practice we always use the analytic gradient and then perform a <strong>gradient check</strong>, in which its implementation is compared to the numerical gradient.</li>
<li>We introduced the <strong>Gradient Descent</strong> algorithm which iteratively computes the gradient and performs a parameter update in loop.</li>
</ul>
<p><strong>Coming up:</strong> The core takeaway from this section is that the ability to compute the gradient of a loss function with respect to its weights (and have some intuitive understanding of it) is the most important skill needed to design, train and understand neural networks. In the next section we will develop proficiency in computing the gradient analytically using the chain rule, otherwise also referred to as <strong>backpropagation</strong>. This will allow us to efficiently optimize relatively arbitrary loss functions that express all kinds of Neural Networks, including Convolutional Neural Networks.</p>
                
                  
                
              
              
                
              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../ch-neural-networks-case-study/" title="neural-networks-case-study" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  后退
                </span>
                neural-networks-case-study
              </span>
            </div>
          </a>
        
        
          <a href="../ch-optimization-2/" title="Ch optimization 2" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  前进
                </span>
                Ch optimization 2
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            Copyright &copy; 2016 - 2018 Martin Donath
          </div>
        
        powered by
        <a href="http://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
        
  <div class="md-footer-social">
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    
      <a href="http://struct.cc" class="md-footer-social__link fa fa-globe"></a>
    
      <a href="https://github.com/1007530194" class="md-footer-social__link fa fa-github-alt"></a>
    
      <a href="https://twitter.com/1007530194" class="md-footer-social__link fa fa-twitter"></a>
    
      <a href="https://linkedin.com/in/1007530194" class="md-footer-social__link fa fa-linkedin"></a>
    
  </div>

      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../../../../assets/javascripts/application.189d7058.js"></script>
      
        
        
          
          <script src="../../../../assets/javascripts/lunr/lunr.stemmer.support.js"></script>
          
            
              
                <script src="../../../../assets/javascripts/lunr/tinyseg.js"></script>
              
              
                <script src="../../../../assets/javascripts/lunr/lunr.jp.js"></script>
              
            
          
          
        
      
      <script>app.initialize({version:"0.17.3",url:{base:"../../../.."}})</script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
    
    
      
        <script>
    !function (e, a, t, n, o, c, i) {
        e.GoogleAnalyticsObject = o, e.ga = e.ga || function () {
            (e.ga.q = e.ga.q || []).push(arguments)
        }, e.ga.l = 1 * new Date, c = a.createElement(t),
            i = a.getElementsByTagName(t)[0],
            c.async = 1,
            c.src = "https://www.google-analytics.com/analytics.js",
            i.parentNode.insertBefore(c, i)
    }(window, document, "script", 0, "ga"),
        ga("create", "UA-115879584-1", "auto"),
        ga("set", "anonymizeIp", !0), ga("send", "pageview");
    var links = document.getElementsByTagName("a");

    if (Array.prototype.map.call(links, function (e) {
            e.host != document.location.host && e.addEventListener("click", function () {
                var a = e.getAttribute("data-md-action") || "follow";
                ga("send", "event", "outbound", a, e.href)
            })
        }), document.forms.search) {
        var query = document.forms.search.query;
        query.addEventListener("blur", function () {
            if (this.value) {
                var e = document.location.pathname;
                ga("send", "pageview", e + "?q=" + this.value)
            }
        })
    }</script>
      
    
  </body>
</html>
<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        <meta name="author" content="NiuLiangtao">
        <link rel="canonical" href="https://1007530194.github.io/Diary/学习/tensorflow/如何选择优化器 optimizer/">
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>如何选择优化器 optimizer - Blog of NiuLiangtao</title>
        <link href="../../../css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome-4.5.0.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="../../../css/highlight.css">
        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
        <![endif]-->

        <script src="../../../js/jquery-1.10.2.min.js"></script>
        <script src="../../../js/bootstrap-3.0.3.min.js"></script>
        <script src="../../../js/highlight.pack.js"></script> 
    </head>

    <body>

        <div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">

        <!-- Collapsed navigation -->
        <div class="navbar-header">
            <!-- Expander button -->
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="../../../ToDo/">Blog of NiuLiangtao</a>
        </div>

        <!-- Expanded navigation -->
        <div class="navbar-collapse collapse">
                <!-- Main navigation -->
                <ul class="nav navbar-nav">
                    <li >
                        <a href="../../../ToDo/">Home</a>
                    </li>
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Home <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                            
<li >
    <a href="../../../AboutMe/">About</a>
</li>
                            
<li >
    <a href="../../..">Home</a>
</li>
                            
<li >
    <a href="../../../Nothing/">Nothing</a>
</li>
                            
<li >
    <a href="../../../ToDo/">要做的</a>
</li>
                        </ul>
                    </li>
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">书籍 <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                            
<li >
    <a href="../../../书籍/">Home</a>
</li>
                            
  <li class="dropdown-submenu">
    <a href="#">机器学习实战</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../../书籍/机器学习实战/00naive-bayes-discuss/">naive-bayes-discuss</a>
</li>
            
<li >
    <a href="../../../书籍/机器学习实战/01.机器学习基础/">1.机器学习基础</a>
</li>
            
<li >
    <a href="../../../书籍/机器学习实战/02.k-近邻算法/">2.k-近邻算法</a>
</li>
            
<li >
    <a href="../../../书籍/机器学习实战/03.决策树/">3.决策树</a>
</li>
            
<li >
    <a href="../../../书籍/机器学习实战/04.朴素贝叶斯/">4.朴素贝叶斯</a>
</li>
            
<li >
    <a href="../../../书籍/机器学习实战/05.Logistic回归/">5.Logistic回归</a>
</li>
            
<li >
    <a href="../../../书籍/机器学习实战/06.0.支持向量机/">6.支持向量机</a>
</li>
            
<li >
    <a href="../../../书籍/机器学习实战/06.1.支持向量机的几个通俗理解/">6.1.支持向量机的几个通俗理解</a>
</li>
            
<li >
    <a href="../../../书籍/机器学习实战/07.集成方法-随机森林和AdaBoost/">7.集成方法-随机森林和AdaBoost</a>
</li>
            
<li >
    <a href="../../../书籍/机器学习实战/08.预测数值型数据-回归/">8.预测数值型数据：回归</a>
</li>
            
<li >
    <a href="../../../书籍/机器学习实战/09.树回归/">9.树回归</a>
</li>
            
<li >
    <a href="../../../书籍/机器学习实战/10.k-means聚类/">10.k-means聚类</a>
</li>
            
<li >
    <a href="../../../书籍/机器学习实战/11.使用Apriori算法进行关联分析/">11.使用Apriori算法进行关联分析</a>
</li>
            
<li >
    <a href="../../../书籍/机器学习实战/12.使用FP-growth算法来高效发现频繁项集/">12.使用FP-growth算法来高效发现频繁项集</a>
</li>
            
<li >
    <a href="../../../书籍/机器学习实战/13.利用PCA来简化数据/">13.利用PCA来简化数据</a>
</li>
            
<li >
    <a href="../../../书籍/机器学习实战/14.利用SVD简化数据/">14.利用SVD简化数据</a>
</li>
            
<li >
    <a href="../../../书籍/机器学习实战/15.大数据与MapReduce/">15.大数据与MapReduce</a>
</li>
            
<li >
    <a href="../../../书籍/机器学习实战/16.推荐系统/">16.推荐系统</a>
</li>
    </ul>
  </li>
                            
  <li class="dropdown-submenu">
    <a href="#">深度学习</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../../书籍/深度学习/DeepLearning/">深度学习中文版</a>
</li>
    </ul>
  </li>
                        </ul>
                    </li>
                    <li class="dropdown active">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">学习 <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                            
<li >
    <a href="../../">Home</a>
</li>
                            
  <li class="dropdown-submenu">
    <a href="#">tensorflow</a>
    <ul class="dropdown-menu">
            
<li class="active">
    <a href="./">如何选择优化器 optimizer</a>
</li>
    </ul>
  </li>
                            
  <li class="dropdown-submenu">
    <a href="#">深度学习</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../深度学习/主成分分析-2/">主成分分析 2</a>
</li>
            
<li >
    <a href="../../深度学习/主成分分析/">主成分分析</a>
</li>
            
<li >
    <a href="../../深度学习/最小二乘法/">最小二乘法</a>
</li>
            
<li >
    <a href="../../深度学习/矩阵分解/">矩阵分解</a>
</li>
            
<li >
    <a href="../../深度学习/矩阵求导/">矩阵求导</a>
</li>
    </ul>
  </li>
                            
  <li class="dropdown-submenu">
    <a href="#">转载</a>
    <ul class="dropdown-menu">
            
  <li class="dropdown-submenu">
    <a href="#">cs231n-ch</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../转载/cs231n-ch/ch-assignment2_google_cloud/">Ch assignment2 google cloud</a>
</li>
            
<li >
    <a href="../../转载/cs231n-ch/ch-aws-tutorial/">Ch aws tutorial</a>
</li>
            
<li >
    <a href="../../转载/cs231n-ch/ch-classification/">Ch classification</a>
</li>
            
<li >
    <a href="../../转载/cs231n-ch/ch-convnet-tips/">Ch convnet tips</a>
</li>
            
<li >
    <a href="../../转载/cs231n-ch/ch-convolutional-networks/">Ch convolutional networks</a>
</li>
            
<li >
    <a href="../../转载/cs231n-ch/ch-google_cloud_tutorial/">Ch google cloud tutorial</a>
</li>
            
<li >
    <a href="../../转载/cs231n-ch/ch-ipython-tutorial/">Ch ipython tutorial</a>
</li>
            
<li >
    <a href="../../转载/cs231n-ch/ch-linear-classify/">Ch linear classify</a>
</li>
            
<li >
    <a href="../../转载/cs231n-ch/ch-neural-networks-1/">Ch neural networks 1</a>
</li>
            
<li >
    <a href="../../转载/cs231n-ch/ch-neural-networks-2/">Ch neural networks 2</a>
</li>
            
<li >
    <a href="../../转载/cs231n-ch/ch-neural-networks-3/">Ch neural networks 3</a>
</li>
            
<li >
    <a href="../../转载/cs231n-ch/ch-neural-networks-case-study/">Ch neural networks case study</a>
</li>
            
<li >
    <a href="../../转载/cs231n-ch/ch-optimization-1/">Ch optimization 1</a>
</li>
            
<li >
    <a href="../../转载/cs231n-ch/ch-optimization-2/">Ch optimization 2</a>
</li>
            
<li >
    <a href="../../转载/cs231n-ch/ch-overview/">Ch overview</a>
</li>
            
<li >
    <a href="../../转载/cs231n-ch/ch-poster/">Ch poster</a>
</li>
            
<li >
    <a href="../../转载/cs231n-ch/ch-python-numpy-tutorial/">Ch python numpy tutorial</a>
</li>
            
<li >
    <a href="../../转载/cs231n-ch/ch-Readme/">ch Readme</a>
</li>
            
<li >
    <a href="../../转载/cs231n-ch/ch-transfer-learning/">Ch transfer learning</a>
</li>
            
<li >
    <a href="../../转载/cs231n-ch/ch-understanding-cnn/">Ch understanding cnn</a>
</li>
            
<li >
    <a href="../../转载/cs231n-ch/terminal-tutorial/">Terminal tutorial</a>
</li>
    </ul>
  </li>
            
  <li class="dropdown-submenu">
    <a href="#">cs231n-en</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../转载/cs231n-en/en-assignment2_google_cloud/">En assignment2 google cloud</a>
</li>
            
<li >
    <a href="../../转载/cs231n-en/en-aws-tutorial/">En aws tutorial</a>
</li>
            
<li >
    <a href="../../转载/cs231n-en/en-classification/">En classification</a>
</li>
            
<li >
    <a href="../../转载/cs231n-en/en-convnet-tips/">En convnet tips</a>
</li>
            
<li >
    <a href="../../转载/cs231n-en/en-convolutional-networks/">En convolutional networks</a>
</li>
            
<li >
    <a href="../../转载/cs231n-en/en-google_cloud_tutorial/">En google cloud tutorial</a>
</li>
            
<li >
    <a href="../../转载/cs231n-en/en-ipython-tutorial/">En ipython tutorial</a>
</li>
            
<li >
    <a href="../../转载/cs231n-en/en-linear-classify/">En linear classify</a>
</li>
            
<li >
    <a href="../../转载/cs231n-en/en-neural-networks-1/">En neural networks 1</a>
</li>
            
<li >
    <a href="../../转载/cs231n-en/en-neural-networks-2/">En neural networks 2</a>
</li>
            
<li >
    <a href="../../转载/cs231n-en/en-neural-networks-3/">En neural networks 3</a>
</li>
            
<li >
    <a href="../../转载/cs231n-en/en-neural-networks-case-study/">En neural networks case study</a>
</li>
            
<li >
    <a href="../../转载/cs231n-en/en-optimization-1/">En optimization 1</a>
</li>
            
<li >
    <a href="../../转载/cs231n-en/en-optimization-2/">En optimization 2</a>
</li>
            
<li >
    <a href="../../转载/cs231n-en/en-overview/">En overview</a>
</li>
            
<li >
    <a href="../../转载/cs231n-en/en-poster/">En poster</a>
</li>
            
<li >
    <a href="../../转载/cs231n-en/en-python-numpy-tutorial/">En python numpy tutorial</a>
</li>
            
<li >
    <a href="../../转载/cs231n-en/en-Readme/">en Readme</a>
</li>
            
<li >
    <a href="../../转载/cs231n-en/en-terminal-tutorial/">En terminal tutorial</a>
</li>
            
<li >
    <a href="../../转载/cs231n-en/en-transfer-learning/">En transfer learning</a>
</li>
            
<li >
    <a href="../../转载/cs231n-en/en-understanding-cnn/">En understanding cnn</a>
</li>
    </ul>
  </li>
            
  <li class="dropdown-submenu">
    <a href="#">example</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../转载/example/a-web-crawer-with-a-asyncio-coroutines-1/">A web crawer with a asyncio coroutines 1</a>
</li>
            
<li >
    <a href="../../转载/example/a-web-crawer-with-a-asyncio-coroutines-2/">A web crawer with a asyncio coroutines 2</a>
</li>
            
<li >
    <a href="../../转载/example/bias-variance/">Bias variance</a>
</li>
            
<li >
    <a href="../../转载/example/bitcoin-explained-1/">Bitcoin explained 1</a>
</li>
            
<li >
    <a href="../../转载/example/dive-into-gradient-decent/">Dive into gradient decent</a>
</li>
            
<li >
    <a href="../../转载/example/ethereum-ultimate-guide/">Ethereum ultimate guide</a>
</li>
            
<li >
    <a href="../../转载/example/fork-exec-source/">Fork exec source</a>
</li>
            
<li >
    <a href="../../转载/example/latex语法/">Latex语法</a>
</li>
            
<li >
    <a href="../../转载/example/latex语法1/">Latex语法1</a>
</li>
            
<li >
    <a href="../../转载/example/nosql-in-python/">Nosql in python</a>
</li>
            
<li >
    <a href="../../转载/example/perceptron/">Perceptron</a>
</li>
            
<li >
    <a href="../../转载/example/quick-latex/">Quick latex</a>
</li>
            
<li >
    <a href="../../转载/example/tendermint/">Tendermint</a>
</li>
    </ul>
  </li>
            
  <li class="dropdown-submenu">
    <a href="#">example2</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../转载/example2/arrays-similar/">Arrays similar</a>
</li>
            
<li >
    <a href="../../转载/example2/baidu-ife-1/">Baidu ife 1</a>
</li>
            
<li >
    <a href="../../转载/example2/create-my-blog-with-jekyll/">Create my blog with jekyll</a>
</li>
            
<li >
    <a href="../../转载/example2/front-end-tools/">Front end tools</a>
</li>
            
<li >
    <a href="../../转载/example2/git-clone-not-master-branch/">Git clone not master branch</a>
</li>
            
<li >
    <a href="../../转载/example2/History-API/">History API</a>
</li>
            
<li >
    <a href="../../转载/example2/how-to-use-babel/">How to use babel</a>
</li>
            
<li >
    <a href="../../转载/example2/how-to-write-a-count-down/">How to write a count down</a>
</li>
            
<li >
    <a href="../../转载/example2/JavaScript-closure/">JavaScript closure</a>
</li>
            
<li >
    <a href="../../转载/example2/JavaScript-function/">JavaScript function</a>
</li>
            
<li >
    <a href="../../转载/example2/JavaScript-good-parts-note1/">JavaScript good parts note1</a>
</li>
            
<li >
    <a href="../../转载/example2/JavaScript-good-parts-note2/">JavaScript good parts note2</a>
</li>
            
<li >
    <a href="../../转载/example2/JavaScript-good-parts-note3/">JavaScript good parts note3</a>
</li>
            
<li >
    <a href="../../转载/example2/JavaScript-Net/">JavaScript Net</a>
</li>
            
<li >
    <a href="../../转载/example2/JavaScript-Object-Oriented/">JavaScript Object Oriented</a>
</li>
            
<li >
    <a href="../../转载/example2/JavaScript-this/">JavaScript this</a>
</li>
            
<li >
    <a href="../../转载/example2/jekyll-theme-version-2.0/">Jekyll theme version 2.0</a>
</li>
            
<li >
    <a href="../../转载/example2/js-create-file-and-download/">Js create file and download</a>
</li>
            
<li >
    <a href="../../转载/example2/low-IE-click-empty-block-bug/">low IE click empty block bug</a>
</li>
            
<li >
    <a href="../../转载/example2/regular-expression-group/">Regular expression group</a>
</li>
            
<li >
    <a href="../../转载/example2/scope/">Scope</a>
</li>
            
<li >
    <a href="../../转载/example2/shuffle-algorithm/">Shuffle algorithm</a>
</li>
            
<li >
    <a href="../../转载/example2/sublimeLinter/">sublimeLinter</a>
</li>
            
<li >
    <a href="../../转载/example2/Syncing-a-fork/">Syncing a fork</a>
</li>
            
<li >
    <a href="../../转载/example2/teach-girlfriend-html-css/">Teach girlfriend html css</a>
</li>
            
<li >
    <a href="../../转载/example2/web-app/">Web app</a>
</li>
            
<li >
    <a href="../../转载/example2/weinre/">Weinre</a>
</li>
    </ul>
  </li>
    </ul>
  </li>
                        </ul>
                    </li>
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">工具 <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                            
<li >
    <a href="../../../工具/About/">About</a>
</li>
                            
<li >
    <a href="../../../工具/Cheatsheet/">Cheatsheet</a>
</li>
                            
<li >
    <a href="../../../工具/">Home</a>
</li>
                            
<li >
    <a href="../../../工具/shell显示时间/">Shell显示时间</a>
</li>
                            
  <li class="dropdown-submenu">
    <a href="#">常用工具</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../../工具/常用工具/10分钟上手Latex/">10分钟上手Latex</a>
</li>
            
<li >
    <a href="../../../工具/常用工具/10分钟上手Pandas/">10分钟上手Pandas</a>
</li>
            
<li >
    <a href="../../../工具/常用工具/10分钟上手Python/">10分钟上手Python</a>
</li>
    </ul>
  </li>
                        </ul>
                    </li>
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">文章 <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                            
<li >
    <a href="../../../文章/">Home</a>
</li>
                        </ul>
                    </li>
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">日常 <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                            
<li >
    <a href="../../../日常/">Home</a>
</li>
                            
  <li class="dropdown-submenu">
    <a href="#">test</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../../日常/test/testtest/">Testtest</a>
</li>
    </ul>
  </li>
                        </ul>
                    </li>
                </ul>

            <ul class="nav navbar-nav navbar-right">
                <li>
                    <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                        <i class="fa fa-search"></i> Search
                    </a>
                </li>
                    <li >
                        <a rel="next" href="../../">
                            <i class="fa fa-arrow-left"></i> Previous
                        </a>
                    </li>
                    <li >
                        <a rel="prev" href="../../深度学习/主成分分析-2/">
                            Next <i class="fa fa-arrow-right"></i>
                        </a>
                    </li>
                    <li>
                        <a href="https://github.com/1007530194/Diary">1007530194/Diary</a>
                    </li>
            </ul>
        </div>
    </div>
</div>

        <div class="container">
                <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
        <li class="main active"><a href="#146">1. 优化器算法简述?</a></li>
            <li><a href="#146_batch_gradient_descent">1. Batch gradient descent</a></li>
            <li><a href="#246_stochastic_gradient_descent">2. Stochastic gradient descent</a></li>
            <li><a href="#346_mini-batch_gradient_descent">3. Mini-batch gradient descent</a></li>
            <li><a href="#446_momentum">4. Momentum</a></li>
            <li><a href="#546_nesterov_accelerated_gradient">5. Nesterov accelerated gradient</a></li>
            <li><a href="#646_adagrad">6. Adagrad</a></li>
            <li><a href="#746_adadelta">7. Adadelta</a></li>
            <li><a href="#846_rmsprop">8. RMSprop</a></li>
            <li><a href="#946_adam">9. Adam</a></li>
        <li class="main "><a href="#246">2. 效果比较?</a></li>
        <li class="main "><a href="#346">3. 如何选择？</a></li>
    </ul>
</div></div>
                <div class="col-md-9" role="main">

<p>在很多机器学习和深度学习的应用中，我们发现用的最多的优化器是 Adam，为什么呢？</p>
<p>下面是 TensorFlow 中的优化器， 
<a href="https://www.tensorflow.org/api_guides/python/train"><a href="https://www.tensorflow.org/api">https://www.tensorflow.org/api</a>_guides/python/train</a> 
<img alt="" src="../resources/28A308A65B43D79482C913803F349716.png" /></p>
<p>在 keras 中也有 SGD，RMSprop，Adagrad，Adadelta，Adam 等： 
<a href="https://keras.io/optimizers/">https://keras.io/optimizers/</a></p>
<p>我们可以发现除了常见的梯度下降，还有 Adadelta，Adagrad，RMSProp 等几种优化器，都是什么呢，又该怎么选择呢？</p>
<p>在 Sebastian Ruder 的这篇论文中给出了常用优化器的比较，今天来学习一下： 
<a href="https://arxiv.org/pdf/1609.04747.pdf">https://arxiv.org/pdf/1609.04747.pdf</a></p>
<p>本文将梳理：</p>
<ul>
<li>每个算法的梯度更新规则和缺点</li>
<li>为了应对这个不足而提出的下一个算法</li>
<li>超参数的一般设定值</li>
<li>几种算法的效果比较</li>
<li>选择哪种算法</li>
</ul>
<hr />
<h1 id="146">1. 优化器算法简述?<a class="headerlink" href="#146" title="Permanent link">&para;</a></h1>
<p>首先来看一下梯度下降最常见的三种变形 BGD，SGD，MBGD，这三种形式的区别就是取决于我们用多少数据来计算目标函数的梯度，这样的话自然就涉及到一个 trade－off，即参数更新的准确率和运行时间。</p>
<h2 id="146_batch_gradient_descent">1. Batch gradient descent<a class="headerlink" href="#146_batch_gradient_descent" title="Permanent link">&para;</a></h2>
<p>梯度更新规则: 
BGD 采用整个训练集的数据来计算 cost function 对参数的梯度：</p>
<div>
<div class="MathJax_Preview">
\theta = \theta - \eta \bullet \nabla_\theta \boldsymbol{J}(\theta)
</div>
<script type="math/tex; mode=display">
\theta = \theta - \eta \bullet \nabla_\theta \boldsymbol{J}(\theta)
</script>
</div>
<p>缺点: 
由于这种方法是在一次更新中，就对整个数据集计算梯度，所以计算起来非常慢，遇到很大量的数据集也会非常棘手，而且不能投入新数据实时更新模型</p>
<div class="codehilite"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_epochs</span><span class="p">):</span>
  <span class="n">params_grad</span> <span class="o">=</span> <span class="n">evaluate_gradient</span><span class="p">(</span><span class="n">loss_function</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
  <span class="n">params</span> <span class="o">=</span> <span class="n">params</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">params_grad</span>
</pre></div>

<p>我们会事先定义一个迭代次数 epoch，首先计算梯度向量 params_grad，然后沿着梯度的方向更新参数 params，learning rate 决定了我们每一步迈多大。</p>
<p>Batch gradient descent 对于凸函数可以收敛到全局极小值，对于非凸函数可以收敛到局部极小值。</p>
<hr />
<h2 id="246_stochastic_gradient_descent">2. Stochastic gradient descent<a class="headerlink" href="#246_stochastic_gradient_descent" title="Permanent link">&para;</a></h2>
<p>梯度更新规则: 
和 BGD 的一次用所有数据计算梯度相比，SGD 每次更新时对每个样本进行梯度更新，对于很大的数据集来说，可能会有相似的样本，这样 BGD 在计算梯度时会出现冗余，而 SGD 一次只进行一次更新，就没有冗余，而且比较快，并且可以新增样本。</p>
<div>
<div class="MathJax_Preview">
\theta = \theta - \eta \bullet \nabla_\theta 
 \boldsymbol{J}(\theta;x^{(i)};y^{(i)})
</div>
<script type="math/tex; mode=display">
\theta = \theta - \eta \bullet \nabla_\theta 
 \boldsymbol{J}(\theta;x^{(i)};y^{(i)})
</script>
</div>
<div class="codehilite"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_epochs</span><span class="p">):</span>
  <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">example</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
    <span class="n">params_grad</span> <span class="o">=</span> <span class="n">evaluate_gradient</span><span class="p">(</span><span class="n">loss_function</span><span class="p">,</span> <span class="n">example</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">params</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">params_grad</span>
</pre></div>

<p>看代码，可以看到区别，就是整体数据集是个循环，其中对每个样本进行一次参数更新。</p>
<p>缺点: 
但是 SGD 因为更新比较频繁，会造成 cost function 有严重的震荡。</p>
<p><img alt="" src="../resources/62F8D6CAC0120E34CAC75B59A467B1D8.png" /></p>
<p>BGD 可以收敛到局部极小值，当然 SGD 的震荡可能会跳到更好的局部极小值处。</p>
<p>当我们稍微减小 learning rate，SGD 和 BGD 的收敛性是一样的。</p>
<hr />
<h2 id="346_mini-batch_gradient_descent">3. Mini-batch gradient descent<a class="headerlink" href="#346_mini-batch_gradient_descent" title="Permanent link">&para;</a></h2>
<p>梯度更新规则: 
MBGD 每一次利用一小批样本，即 n 个样本进行计算，这样它可以降低参数更新时的方差，收敛更稳定，另一方面可以充分地利用深度学习库中高度优化的矩阵操作来进行更有效的梯度计算。</p>
<div>
<div class="MathJax_Preview">
\theta = \theta - \eta \bullet \nabla_\theta 
 \boldsymbol{J}(\theta;x^{(i:i+n)};y^{(i:i+n)})
</div>
<script type="math/tex; mode=display">
\theta = \theta - \eta \bullet \nabla_\theta 
 \boldsymbol{J}(\theta;x^{(i:i+n)};y^{(i:i+n)})
</script>
</div>
<p>和 SGD 的区别是每一次循环不是作用于每个样本，而是具有 n 个样本的批次</p>
<div class="codehilite"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_epochs</span><span class="p">):</span>
  <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">get_batches</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">50</span><span class="p">):</span>
    <span class="n">params_grad</span> <span class="o">=</span> <span class="n">evaluate_gradient</span><span class="p">(</span><span class="n">loss_function</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">params</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">params_grad</span>
</pre></div>

<p>超参数设定值: 
n 一般取值在 50～256</p>
<p>缺点: 
  1. learning rate 如果选择的太小，收敛速度会很慢，如果太大，loss function 就会在极小值处不停地震荡甚至偏离。（有一种措施是先设定大一点的学习率，当两次迭代之间的变化低于某个阈值后，就减小 learning rate，不过这个阈值的设定需要提前写好，这样的话就不能够适应数据集的特点。）
  2. 此外，这种方法是对所有参数更新时应用同样的 learning rate，如果我们的数据是稀疏的，我们更希望对出现频率低的特征进行大一点的更新。
  3. 另外，对于非凸函数，还要避免陷于局部极小值处，或者鞍点处，因为鞍点周围的error 是一样的，所有维度的梯度都接近于0，SGD 很容易被困在这里。</p>
<p>不过 Mini-batch gradient descent 不能保证很好的收敛性，鞍点就是：一个光滑函数的鞍点邻域的曲线，曲面，或超曲面，都位于这点的切线的不同边。 例如这个二维图形，像个马鞍：在x-轴方向往上曲，在y-轴方向往下曲，鞍点就是（0，0）</p>
<p><img alt="" src="../resources/BE69409D8ECB330D775F9E6F1E99A239.png" /></p>
<p>为了应对上面的三点挑战就有了下面这些算法。</p>
<hr />
<h2 id="446_momentum">4. Momentum<a class="headerlink" href="#446_momentum" title="Permanent link">&para;</a></h2>
<p>SGD 在 ravines 的情况下容易被困住， ravines 就是曲面的一个方向比另一个方向更陡，这时 SGD 会发生震荡而迟迟不能接近极小值：</p>
<p><img alt="" src="../resources/76BCC65FF39FB153D377EEF08FABEB16.png" /></p>
<p>梯度更新规则: 
Momentum 通过加入 <span><span class="MathJax_Preview">\gamma v_{t−1}</span><script type="math/tex">\gamma v_{t−1}</script></span> ，可以加速 SGD， 并且抑制震荡</p>
<div>
<div class="MathJax_Preview">
v_t=\gamma v_{t-1} + \eta \bullet \nabla_{\theta}\boldsymbol{J}(\theta)
\\\
\theta = \theta - v_t
</div>
<script type="math/tex; mode=display">
v_t=\gamma v_{t-1} + \eta \bullet \nabla_{\theta}\boldsymbol{J}(\theta)
\\\
\theta = \theta - v_t
</script>
</div>
<p>当我们将一个小球从山上滚下来时，没有阻力的话，它的动量会越来越大，但是如果遇到了阻力，速度就会变小。 
加入的这一项，可以使得梯度方向不变的维度上速度变快，梯度方向有所改变的维度上的更新速度变慢，这样就可以加快收敛并减小震荡。</p>
<p>超参数设定值: 
一般 <span><span class="MathJax_Preview">\gamma</span><script type="math/tex">\gamma</script></span> 取值 0.9 左右。</p>
<p>缺点: 
这种情况相当于小球从山上滚下来时是在盲目地沿着坡滚，如果它能具备一些先知，例如快要上坡时，就知道需要减速了的话，适应性会更好。</p>
<hr />
<h2 id="546_nesterov_accelerated_gradient">5. Nesterov accelerated gradient<a class="headerlink" href="#546_nesterov_accelerated_gradient" title="Permanent link">&para;</a></h2>
<p>梯度更新规则: 
用 <span><span class="MathJax_Preview">\theta−\gamma v_{t−1}</span><script type="math/tex">\theta−\gamma v_{t−1}</script></span> 来近似当做参数下一步会变成的值，则在计算梯度时，不是在当前位置，而是未来的位置上
$$
v_t = \gamma v_{t-1} + \eta \bullet \nabla_\theta \boldsymbol{J}(\theta-\gamma v_{t-1})
 \\
 \theta = \theta - v_t
$$</p>
<p>超参数设定值: 
<span><span class="MathJax_Preview">\gamma</span><script type="math/tex">\gamma</script></span> 仍然取值 0.9 左右。</p>
<p>效果比较: 
<img alt="" src="../resources/98F724EDF6695476F6B38F7F172B59F5.png" /></p>
<p>蓝色是 Momentum 的过程，会先计算当前的梯度，然后在更新后的累积梯度后会有一个大的跳跃。 
而 NAG 会先在前一步的累积梯度上(brown vector)有一个大的跳跃，然后衡量一下梯度做一下修正(red vector)，这种预期的更新可以避免我们走的太快。</p>
<p>NAG 可以使 RNN 在很多任务上有更好的表现。目前为止，我们可以做到，在更新梯度时顺应 loss function 的梯度来调整速度，并且对 SGD 进行加速。我们还希望可以根据参数的重要性而对不同的参数进行不同程度的更新。</p>
<hr />
<h2 id="646_adagrad">6. Adagrad<a class="headerlink" href="#646_adagrad" title="Permanent link">&para;</a></h2>
<p>这个算法就可以对低频的参数做较大的更新，对高频的做较小的更新，也因此，对于稀疏的数据它的表现很好，很好地提高了 SGD 的鲁棒性，例如识别 Youtube 视频里面的猫，训练 GloVe word embeddings，因为它们都是需要在低频的特征上有更大的更新。</p>
<p>梯度更新规则:</p>
<div>
<div class="MathJax_Preview">
 \theta_{t+1,i} = \theta_{t,i} -\frac{\eta}{\sqrt{G_{t,ii}+\varepsilon }} \bullet g_{t,i}
</div>
<script type="math/tex; mode=display">
 \theta_{t+1,i} = \theta_{t,i} -\frac{\eta}{\sqrt{G_{t,ii}+\varepsilon }} \bullet g_{t,i}
</script>
</div>
<p>其中 <span><span class="MathJax_Preview">g</span><script type="math/tex">g</script></span> 为：<span><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span> 时刻参数 <span><span class="MathJax_Preview">θ_i</span><script type="math/tex">θ_i</script></span> 的梯度</p>
<div>
<div class="MathJax_Preview">
g_{t,i} = \nabla_\theta \boldsymbol{J}(\theta_{i})
</div>
<script type="math/tex; mode=display">
g_{t,i} = \nabla_\theta \boldsymbol{J}(\theta_{i})
</script>
</div>
<p>如果是普通的 SGD， 那么 <span><span class="MathJax_Preview">θ_i</span><script type="math/tex">θ_i</script></span> 在每一时刻的梯度更新公式为：</p>
<div>
<div class="MathJax_Preview">
 \theta_{t+1,i} = \theta_{t,i} -\eta \bullet g_{t,i}
</div>
<script type="math/tex; mode=display">
 \theta_{t+1,i} = \theta_{t,i} -\eta \bullet g_{t,i}
</script>
</div>
<p>但这里的 learning rate <span><span class="MathJax_Preview">eta</span><script type="math/tex">eta</script></span> 也随 <span><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span> 和 <span><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span> 而变：</p>
<div>
<div class="MathJax_Preview">
 \theta_{t+1,i} = \theta_{t,i} -\frac{\eta}{\sqrt{G_{t,ii}+\varepsilon }} \bullet g_{t,i}
</div>
<script type="math/tex; mode=display">
 \theta_{t+1,i} = \theta_{t,i} -\frac{\eta}{\sqrt{G_{t,ii}+\varepsilon }} \bullet g_{t,i}
</script>
</div>
<p>其中 <span><span class="MathJax_Preview">G_t</span><script type="math/tex">G_t</script></span> 是个对角矩阵， <span><span class="MathJax_Preview">(i,i)</span><script type="math/tex">(i,i)</script></span> 元素就是 <span><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span> 时刻参数 <span><span class="MathJax_Preview">θ_i</span><script type="math/tex">θ_i</script></span> 的梯度平方和。</p>
<p>Adagrad 的优点是减少了学习率的手动调节</p>
<p>超参数设定值: 
一般 <span><span class="MathJax_Preview">\eta</span><script type="math/tex">\eta</script></span> 就取 0.01。</p>
<p>缺点: 
它的缺点是分母会不断积累，这样学习率就会收缩并最终会变得非常小。</p>
<hr />
<h2 id="746_adadelta">7. Adadelta<a class="headerlink" href="#746_adadelta" title="Permanent link">&para;</a></h2>
<p>这个算法是对 Adagrad 的改进，</p>
<p>和 Adagrad 相比，就是分母的 <span><span class="MathJax_Preview">G</span><script type="math/tex">G</script></span> 换成了过去的梯度平方的衰减平均值，</p>
<div>
<div class="MathJax_Preview">
 \Delta \theta_t =  -\frac{\eta}{\sqrt{E[g^2]_t+\varepsilon }} \bullet g_t
</div>
<script type="math/tex; mode=display">
 \Delta \theta_t =  -\frac{\eta}{\sqrt{E[g^2]_t+\varepsilon }} \bullet g_t
</script>
</div>
<p>这个分母相当于梯度的均方根 root mean squared (RMS) ，所以可以用 RMS 简写：</p>
<div>
<div class="MathJax_Preview">
 \Delta \theta_t =  -\frac{\eta}{RMS[g]_t} \bullet g_t
</div>
<script type="math/tex; mode=display">
 \Delta \theta_t =  -\frac{\eta}{RMS[g]_t} \bullet g_t
</script>
</div>
<p>其中 <span><span class="MathJax_Preview">E</span><script type="math/tex">E</script></span> 的计算公式如下，<span><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span> 时刻的依赖于前一时刻的平均和当前的梯度：</p>
<div>
<div class="MathJax_Preview">
 E[g^2]_t = \gamma E[g^2]_{t-1} + (1-\gamma)g_t^2
</div>
<script type="math/tex; mode=display">
 E[g^2]_t = \gamma E[g^2]_{t-1} + (1-\gamma)g_t^2
</script>
</div>
<p>梯度更新规则:</p>
<p>此外，还将学习率 <span><span class="MathJax_Preview">\eta</span><script type="math/tex">\eta</script></span> 换成了 <span><span class="MathJax_Preview">RMS[\Delta \theta]</span><script type="math/tex">RMS[\Delta \theta]</script></span>，这样的话，我们甚至都不需要提前设定学习率了：</p>
<div>
<div class="MathJax_Preview">
 \Delta \theta_t =  -\frac{RMS[\Delta \theta]_{t-1}}{RMS[g]_t} \bullet g_t
 \\\
 \theta_{t+1} = \theta_t + \Delta \theta_t
</div>
<script type="math/tex; mode=display">
 \Delta \theta_t =  -\frac{RMS[\Delta \theta]_{t-1}}{RMS[g]_t} \bullet g_t
 \\\
 \theta_{t+1} = \theta_t + \Delta \theta_t
</script>
</div>
<p>超参数设定值: 
<span><span class="MathJax_Preview">\gamma</span><script type="math/tex">\gamma</script></span> 一般设定为 0.9，</p>
<hr />
<h2 id="846_rmsprop">8. RMSprop<a class="headerlink" href="#846_rmsprop" title="Permanent link">&para;</a></h2>
<p>RMSprop 是 Geoff Hinton 提出的一种自适应学习率方法。
RMSprop 和 Adadelta 都是为了解决 Adagrad 学习率急剧下降问题的，</p>
<p>梯度更新规则: 
RMSprop 与 Adadelta 的第一种形式相同：
$$
 E[g^2]<em>t = \gamma E[g^2]</em>{t-1} + (1-\gamma)g_t^2
 \\
 \theta_{t+1} = \theta_t -\frac{\eta}{\sqrt{E[g^2]_t+\varepsilon }} g_t
$$</p>
<p>超参数设定值: 
Hinton 建议设定 <span><span class="MathJax_Preview">gamma</span><script type="math/tex">gamma</script></span> 为 0.9, 学习率 <span><span class="MathJax_Preview">\eta</span><script type="math/tex">\eta</script></span> 为 0.001。</p>
<hr />
<h2 id="946_adam">9. Adam<a class="headerlink" href="#946_adam" title="Permanent link">&para;</a></h2>
<p>这个算法是另一种计算每个参数的自适应学习率的方法。
除了像 Adadelta 和 RMSprop 一样存储了过去梯度的平方 <span><span class="MathJax_Preview">v_t</span><script type="math/tex">v_t</script></span> 的指数衰减平均值 ，也像 momentum 一样保持了过去梯度 <span><span class="MathJax_Preview">m_t</span><script type="math/tex">m_t</script></span> 的指数衰减平均值：</p>
<div>
<div class="MathJax_Preview">
m_t = \beta_1 m_{t-1} + (1-\beta_1)g_t
\\\
v_t = \beta_2 v_{t-1} + (1-\beta_2)g_t^2
</div>
<script type="math/tex; mode=display">
m_t = \beta_1 m_{t-1} + (1-\beta_1)g_t
\\\
v_t = \beta_2 v_{t-1} + (1-\beta_2)g_t^2
</script>
</div>
<p>如果 <span><span class="MathJax_Preview">m_t</span><script type="math/tex">m_t</script></span> 和 <span><span class="MathJax_Preview">v_t</span><script type="math/tex">v_t</script></span> 被初始化为 0 向量，那它们就会向 0 偏置，所以做了偏差校正， 
通过计算偏差校正后的 <span><span class="MathJax_Preview">m_t</span><script type="math/tex">m_t</script></span> 和 <span><span class="MathJax_Preview">v_t</span><script type="math/tex">v_t</script></span> 来抵消这些偏差：</p>
<div>
<div class="MathJax_Preview">
\hat{m}_t = \frac{m_t}{1-\beta^t_1}
\\\
\hat{v}_t = \frac{v_t}{1-\beta^t_2}
</div>
<script type="math/tex; mode=display">
\hat{m}_t = \frac{m_t}{1-\beta^t_1}
\\\
\hat{v}_t = \frac{v_t}{1-\beta^t_2}
</script>
</div>
<p>梯度更新规则:</p>
<div>
<div class="MathJax_Preview">
 \theta_{t+1} = \theta_t -\frac{\eta}{\sqrt{\hat{v}_t} + \varepsilon } \hat{m}_t
</div>
<script type="math/tex; mode=display">
 \theta_{t+1} = \theta_t -\frac{\eta}{\sqrt{\hat{v}_t} + \varepsilon } \hat{m}_t
</script>
</div>
<p>超参数设定值: 
建议 <span><span class="MathJax_Preview">\beta_1</span><script type="math/tex">\beta_1</script></span> ＝ 0.9，<span><span class="MathJax_Preview">\beta_2</span><script type="math/tex">\beta_2</script></span> ＝ 0.999，<span><span class="MathJax_Preview">\varepsilon</span><script type="math/tex">\varepsilon</script></span> ＝ 10e−8</p>
<p>实践表明，Adam 比其他适应性学习方法效果要好。</p>
<hr />
<h1 id="246">2. 效果比较?<a class="headerlink" href="#246" title="Permanent link">&para;</a></h1>
<p>下面看一下几种算法在鞍点和等高线上的表现：</p>
<p><img alt="SGD optimization on saddle point" src="../resources/4A3B4A39AB8E5C556359147B882B4788.gif" /></p>
<p><img alt="SGD optimization on loss surface contours" src="../resources/5D5166A3D3712E7C03AF74B1CCACBEAC.gif" /></p>
<p>上面两种情况都可以看出，Adagrad, Adadelta, RMSprop 几乎很快就找到了正确的方向并前进，收敛速度也相当快，而其它方法要么很慢，要么走了很多弯路才找到。</p>
<p>由图可知自适应学习率方法即 Adagrad, Adadelta, RMSprop, Adam 在这种情景下会更合适而且收敛性更好。</p>
<hr />
<h1 id="346">3. 如何选择？<a class="headerlink" href="#346" title="Permanent link">&para;</a></h1>
<p>如果数据是稀疏的，就用自适用方法，即 Adagrad, Adadelta, RMSprop, Adam。</p>
<p>RMSprop, Adadelta, Adam 在很多情况下的效果是相似的。</p>
<p>Adam 就是在 RMSprop 的基础上加了 bias-correction 和 momentum，</p>
<p>随着梯度变的稀疏，Adam 比 RMSprop 效果会好。</p>
<p>整体来讲，Adam 是最好的选择。</p>
<p>很多论文里都会用 SGD，没有 momentum 等。SGD 虽然能达到极小值，但是比其它算法用的时间长，而且可能会被困在鞍点。</p>
<p>如果需要更快的收敛，或者是训练更深更复杂的神经网络，需要用一种自适应的算法。</p>
<p>参考： 
<a href="http://sebastianruder.com/optimizing-gradient-descent/index.html#fn:24"><a href="http://sebastianruder.com/optimizing-gradient-descent/index.html">http://sebastianruder.com/optimizing-gradient-descent/index.html</a>#fn:24</a> 
<a href="http://www.redcedartech.com/pdfs/Select_Optimization_Method.pdf"><a href="http://www.redcedartech.com/pdfs/Select">http://www.redcedartech.com/pdfs/Select</a>_Optimization_Method.pdf</a> 
<a href="https://stats.stackexchange.com/questions/55247/how-to-choose-the-right-optimization-algorithm">https://stats.stackexchange.com/questions/55247/how-to-choose-the-right-optimization-algorithm</a></p></div>
        </div>

        <footer class="col-md-12">
            <hr>
                <p>Copyright &copy; 2016 - 2018 Martin Donath</p>
            <p>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>var base_url = '../../..';</script>
        <script src="../../../js/base.js"></script>
        <script src="../../../javascripts/extra.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
        <script src="../../../search/require.js"></script>
        <script src="../../../search/search.js"></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="Search Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Search</h4>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form role="form">
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="Keyboard Shortcuts Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Keyboard Shortcuts</h4>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td><kbd>&larr;</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td><kbd>&rarr;</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>


    </body>
</html>

<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../img/favicon.ico">
        <title>7.集成方法-随机森林和AdaBoost.md - DataLab</title>
        <link href="../../css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="../../css/font-awesome-4.5.0.css" rel="stylesheet">
        <link href="../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="../../css/highlight.css">
        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
        <![endif]-->

        <script src="../../js/jquery-1.10.2.min.js"></script>
        <script src="../../js/bootstrap-3.0.3.min.js"></script>
        <script src="../../js/highlight.pack.js"></script> 
    </head>

    <body>

        <div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">

        <!-- Collapsed navigation -->
        <div class="navbar-header">
            <!-- Expander button -->
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="../..">DataLab</a>
        </div>

        <!-- Expanded navigation -->
        <div class="navbar-collapse collapse">
                <!-- Main navigation -->
                <ul class="nav navbar-nav">
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Home <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                            
<li >
    <a href="../..">Home</a>
</li>
                        </ul>
                    </li>
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Home <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                            
<li >
    <a href="../../About/">About</a>
</li>
                            
<li >
    <a href="../../Cheatsheet/">Cheatsheet</a>
</li>
                            
<li >
    <a href="../../Index/">Index</a>
</li>
                        </ul>
                    </li>
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">常用 <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                            
<li >
    <a href="../../常用/2018-02-23-latex语法/">latex语法.md</a>
</li>
                        </ul>
                    </li>
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">常用工具 <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                            
<li >
    <a href="../../常用工具/2018-02-24-10分钟上手Pandas/">10分钟上手Pandas.md</a>
</li>
                        </ul>
                    </li>
                    <li class="dropdown active">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">机器学习实战 <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                            
<li >
    <a href="../2017-01-01-1.机器学习基础/">1.机器学习基础.md</a>
</li>
                            
<li >
    <a href="../2017-02-01-2.k-近邻算法/">2.k-近邻算法.md</a>
</li>
                            
<li >
    <a href="../2017-03-01-3.决策树/">3.决策树.md</a>
</li>
                            
<li >
    <a href="../2017-04-01-4.朴素贝叶斯/">4.朴素贝叶斯.md</a>
</li>
                            
<li >
    <a href="../2017-04-02-naive-bayes-discuss/">naive-bayes-discuss.md</a>
</li>
                            
<li >
    <a href="../2017-05-01-5.Logistic回归/">5.Logistic回归.md</a>
</li>
                            
<li >
    <a href="../2017-06-01-6.支持向量机/">6.支持向量机.md</a>
</li>
                            
<li >
    <a href="../2017-06-02-6.1.支持向量机的几个通俗理解/">6.1.支持向量机的几个通俗理解.md</a>
</li>
                            
<li class="active">
    <a href="./">7.集成方法-随机森林和AdaBoost.md</a>
</li>
                            
<li >
    <a href="../2017-09-01-9.树回归/">2017 09 01 9.树回归</a>
</li>
                            
<li >
    <a href="../2017-10-01-10.k-means聚类/">2017 10 01 10.k means聚类</a>
</li>
                            
<li >
    <a href="../2017-11-01-11.使用Apriori算法进行关联分析/">2017 11 01 11.使用Apriori算法进行关联分析</a>
</li>
                            
<li >
    <a href="../2017-12-01-12.使用FP-growth算法来高效发现频繁项集/">2017 12 01 12.使用FP growth算法来高效发现频繁项集</a>
</li>
                            
<li >
    <a href="../2018-01-01-13.利用PCA来简化数据/">2018 01 01 13.利用PCA来简化数据</a>
</li>
                            
<li >
    <a href="../2018-02-01-14.利用SVD简化数据/">2018 02 01 14.利用SVD简化数据</a>
</li>
                            
<li >
    <a href="../2018-03-01-15.大数据与MapReduce/">2018 03 01 15.大数据与MapReduce</a>
</li>
                            
<li >
    <a href="../2018-04-01-16.推荐系统/">2018 04 01 16.推荐系统</a>
</li>
                            
<li >
    <a href="../预测数值型数据,回归/">预测数值型数据,回归</a>
</li>
                        </ul>
                    </li>
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">深度学习 <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                            
<li >
    <a href="../../深度学习/2018-02-23-10分钟上手Pandas/">2018 02 23 10分钟上手Pandas</a>
</li>
                            
<li >
    <a href="../../深度学习/2018-02-23-主成分分析 (2)/">2018 02 23 主成分分析 (2)</a>
</li>
                            
<li >
    <a href="../../深度学习/2018-02-23-主成分分析/">2018 02 23 主成分分析</a>
</li>
                            
<li >
    <a href="../../深度学习/2018-02-23-最小二乘法/">2018 02 23 最小二乘法</a>
</li>
                            
<li >
    <a href="../../深度学习/2018-02-23-矩阵分解/">2018 02 23 矩阵分解</a>
</li>
                            
<li >
    <a href="../../深度学习/2018-02-23-矩阵求导/">2018 02 23 矩阵求导</a>
</li>
                        </ul>
                    </li>
                </ul>

            <ul class="nav navbar-nav navbar-right">
                <li>
                    <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                        <i class="fa fa-search"></i> Search
                    </a>
                </li>
                    <li >
                        <a rel="next" href="../2017-06-02-6.1.支持向量机的几个通俗理解/">
                            <i class="fa fa-arrow-left"></i> Previous
                        </a>
                    </li>
                    <li >
                        <a rel="prev" href="../2017-09-01-9.树回归/">
                            Next <i class="fa fa-arrow-right"></i>
                        </a>
                    </li>
            </ul>
        </div>
    </div>
</div>

        <div class="container">
                <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
        <li class="main active"><a href="#7-ensemble-method">第7章 集成方法 ensemble method</a></li>
            <li><a href="#ensemble-method-meta-algorithm">集成方法: ensemble method（元算法: meta algorithm） 概述</a></li>
            <li><a href="#_1">集成方法 场景</a></li>
            <li><a href="#_2">随机森林</a></li>
            <li><a href="#adaboost">AdaBoost</a></li>
    </ul>
</div></div>
                <div class="col-md-9" role="main">

<ul>
<li>content
{:toc}</li>
</ul>
<h1 id="7-ensemble-method">第7章 集成方法 ensemble method</h1>
<h2 id="ensemble-method-meta-algorithm">集成方法: ensemble method（元算法: meta algorithm） 概述</h2>
<ul>
<li>概念：是对其他算法进行组合的一种形式。</li>
<li>
<p>通俗来说： 当做重要决定时，大家可能都会考虑吸取多个专家而不只是一个人的意见。
    机器学习处理问题时又何尝不是如此？ 这就是集成方法背后的思想。</p>
</li>
<li>
<p>集成方法：  </p>
<ol>
<li>投票选举(bagging: 自举汇聚法 bootstrap aggregating): 是基于数据随机重抽样分类器构造的方法</li>
<li>再学习(boosting): 是基于所有分类器的加权求和的方法</li>
</ol>
</li>
</ul>
<h2 id="_1">集成方法 场景</h2>
<p>目前 bagging 方法最流行的版本是: 随机森林(random forest)<br/>
选男友：美女选择择偶对象的时候，会问几个闺蜜的建议，最后选择一个综合得分最高的一个作为男朋友</p>
<p>目前 boosting 方法最流行的版本是: AdaBoost<br/>
追女友：3个帅哥追同一个美女，第1个帅哥失败-&gt;(传授经验：姓名、家庭情况) 第2个帅哥失败-&gt;(传授经验：兴趣爱好、性格特点) 第3个帅哥成功</p>
<blockquote>
<p>bagging 和 boosting 区别是什么？</p>
</blockquote>
<ol>
<li>bagging 是一种与 boosting 很类似的技术, 所使用的多个分类器的类型（数据量和特征量）都是一致的。</li>
<li>bagging 是由不同的分类器（1.数据随机化 2.特征随机化）经过训练，综合得出的出现最多分类结果；boosting 是通过调整已有分类器错分的那些数据来获得新的分类器，得出目前最优的结果。</li>
<li>bagging 中的分类器权重是相等的；而 boosting 中的分类器加权求和，所以权重并不相等，每个权重代表的是其对应分类器在上一轮迭代中的成功度。</li>
</ol>
<h2 id="_2">随机森林</h2>
<h3 id="_3">随机森林 概述</h3>
<ul>
<li>随机森林指的是利用多棵树对样本进行训练并预测的一种分类器。</li>
<li>决策树相当于一个大师，通过自己在数据集中学到的知识用于新数据的分类。但是俗话说得好，一个诸葛亮，玩不过三个臭皮匠。随机森林就是希望构建多个臭皮匠，希望最终的分类效果能够超过单个大师的一种算法。</li>
</ul>
<h3 id="_4">随机森林 原理</h3>
<p>那随机森林具体如何构建呢？<br/>
有两个方面：<br/>
1. 数据的随机性化<br/> 
2. 待选特征的随机化<br/></p>
<p>使得随机森林中的决策树都能够彼此不同，提升系统的多样性，从而提升分类性能。</p>
<blockquote>
<p>数据的随机化：使得随机森林中的决策树更普遍化一点，适合更多的场景。</p>
</blockquote>
<p>（有放回的准确率在：70% 以上， 无放回的准确率在：60% 以上）
1. 采取有放回的抽样方式 构造子数据集，保证不同子集之间的数量级一样（不同子集／同一子集 之间的元素可以重复）
2. 利用子数据集来构建子决策树，将这个数据放到每个子决策树中，每个子决策树输出一个结果。
3. 然后统计子决策树的投票结果，得到最终的分类 就是 随机森林的输出结果。
4. 如下图，假设随机森林中有3棵子决策树，2棵子树的分类结果是A类，1棵子树的分类结果是B类，那么随机森林的分类结果就是A类。</p>
<p>![数据重抽样]({{ site.baseurl }}{{ site.images }}/MachineLearninginAction/7.RandomForest/数据重抽样.jpg)</p>
<blockquote>
<p>待选特征的随机化</p>
</blockquote>
<ol>
<li>子树从所有的待选特征中随机选取一定的特征。</li>
<li>在选取的特征中选取最优的特征。</li>
</ol>
<p>下图中，蓝色的方块代表所有可以被选择的特征，也就是目前的待选特征；黄色的方块是分裂特征。<br/>
左边是一棵决策树的特征选取过程，通过在待选特征中选取最优的分裂特征（别忘了前文提到的ID3算法，C4.5算法，CART算法等等），完成分裂。<br/>
右边是一个随机森林中的子树的特征选取过程。<br/></p>
<p>![特征重抽样]({{ site.baseurl }}{{ site.images }}/MachineLearninginAction/7.RandomForest/特征重抽样.jpg)</p>
<blockquote>
<p>随机森林 开发流程</p>
</blockquote>
<div class="highlight"><pre><span></span>收集数据：任何方法
准备数据：转换样本集
分析数据：任何方法
训练算法：通过数据随机化和特征随机化，进行多实例的分类评估
测试算法：计算错误率
使用算法：输入样本数据，然后运行 随机森林 算法判断输入数据分类属于哪个分类，最后对计算出的分类执行后续处理
</pre></div>

<blockquote>
<p>随机森林 算法特点</p>
</blockquote>
<div class="highlight"><pre><span></span>优点：几乎不需要输入准备、可实现隐式特征选择、训练速度非常快、其他模型很难超越、很难建立一个糟糕的随机森林模型、大量优秀、免费以及开源的实现。
缺点：劣势在于模型大小、是个很难去解释的黑盒子。
适用数据范围：数值型和标称型
</pre></div>

<h3 id="_5">项目案例: 声纳信号分类</h3>
<h4 id="_6">项目概述</h4>
<p>这是 Gorman 和 Sejnowski 在研究使用神经网络的声纳信号分类中使用的数据集。任务是训练一个模型来区分声纳信号。</p>
<h4 id="_7">开发流程</h4>
<div class="highlight"><pre><span></span>收集数据：提供的文本文件
准备数据：转换样本集
分析数据：手工检查数据
训练算法：在数据上，利用 random_forest() 函数进行优化评估，返回模型的综合分类结果
测试算法：在采用自定义 n_folds 份随机重抽样 进行测试评估，得出综合的预测评分
使用算法：若你感兴趣可以构建完整的应用程序，从案例进行封装，也可以参考我们的代码
</pre></div>

<blockquote>
<p>收集数据：提供的文本文件</p>
</blockquote>
<p>样本数据：sonar-all-data.txt</p>
<div class="highlight"><pre><span></span>0.02,0.0371,0.0428,0.0207,0.0954,0.0986,0.1539,0.1601,0.3109,0.2111,0.1609,0.1582,0.2238,0.0645,0.066,0.2273,0.31,0.2999,0.5078,0.4797,0.5783,0.5071,0.4328,0.555,0.6711,0.6415,0.7104,0.808,0.6791,0.3857,0.1307,0.2604,0.5121,0.7547,0.8537,0.8507,0.6692,0.6097,0.4943,0.2744,0.051,0.2834,0.2825,0.4256,0.2641,0.1386,0.1051,0.1343,0.0383,0.0324,0.0232,0.0027,0.0065,0.0159,0.0072,0.0167,0.018,0.0084,0.009,0.0032,R
0.0453,0.0523,0.0843,0.0689,0.1183,0.2583,0.2156,0.3481,0.3337,0.2872,0.4918,0.6552,0.6919,0.7797,0.7464,0.9444,1,0.8874,0.8024,0.7818,0.5212,0.4052,0.3957,0.3914,0.325,0.32,0.3271,0.2767,0.4423,0.2028,0.3788,0.2947,0.1984,0.2341,0.1306,0.4182,0.3835,0.1057,0.184,0.197,0.1674,0.0583,0.1401,0.1628,0.0621,0.0203,0.053,0.0742,0.0409,0.0061,0.0125,0.0084,0.0089,0.0048,0.0094,0.0191,0.014,0.0049,0.0052,0.0044,R
0.0262,0.0582,0.1099,0.1083,0.0974,0.228,0.2431,0.3771,0.5598,0.6194,0.6333,0.706,0.5544,0.532,0.6479,0.6931,0.6759,0.7551,0.8929,0.8619,0.7974,0.6737,0.4293,0.3648,0.5331,0.2413,0.507,0.8533,0.6036,0.8514,0.8512,0.5045,0.1862,0.2709,0.4232,0.3043,0.6116,0.6756,0.5375,0.4719,0.4647,0.2587,0.2129,0.2222,0.2111,0.0176,0.1348,0.0744,0.013,0.0106,0.0033,0.0232,0.0166,0.0095,0.018,0.0244,0.0316,0.0164,0.0095,0.0078,R
</pre></div>

<blockquote>
<p>准备数据：转换样本集</p>
</blockquote>
<div class="highlight"><pre><span></span><span class="c1"># 导入csv文件</span>
<span class="k">def</span> <span class="nf">loadDataSet</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fr</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">fr</span><span class="o">.</span><span class="n">readlines</span><span class="p">():</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">line</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="n">lineArr</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">featrue</span> <span class="ow">in</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;,&#39;</span><span class="p">):</span>
                <span class="c1"># strip()返回移除字符串头尾指定的字符生成的新字符串</span>
                <span class="n">str_f</span> <span class="o">=</span> <span class="n">featrue</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">str_f</span><span class="o">.</span><span class="n">isdigit</span><span class="p">():</span> <span class="c1"># 判断是否是数字</span>
                    <span class="c1"># 将数据集的第column列转换成float形式</span>
                    <span class="n">lineArr</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">str_f</span><span class="p">))</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># 添加分类标签</span>
                    <span class="n">lineArr</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">str_f</span><span class="p">)</span>
            <span class="n">dataset</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lineArr</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dataset</span>
</pre></div>

<blockquote>
<p>分析数据：手工检查数据</p>
<p>训练算法：在数据上，利用 random_forest() 函数进行优化评估，返回模型的综合分类结果</p>
</blockquote>
<ul>
<li>样本数据随机无放回抽样-用于交叉验证</li>
</ul>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">cross_validation_split</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">n_folds</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;cross_validation_split(将数据集进行抽重抽样 n_folds 份，数据可以重复重复抽取)</span>

<span class="sd">    Args:</span>
<span class="sd">        dataset     原始数据集</span>
<span class="sd">        n_folds     数据集dataset分成n_flods份</span>
<span class="sd">    Returns:</span>
<span class="sd">        dataset_split    list集合，存放的是：将数据集进行抽重抽样 n_folds 份，数据可以重复重复抽取</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">dataset_split</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
    <span class="n">dataset_copy</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>       <span class="c1"># 复制一份 dataset,防止 dataset 的内容改变</span>
    <span class="n">fold_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_folds</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_folds</span><span class="p">):</span>
        <span class="n">fold</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>                  <span class="c1"># 每次循环 fold 清零，防止重复导入 dataset_split</span>
        <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">fold</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">fold_size</span><span class="p">:</span>   <span class="c1"># 这里不能用 if，if 只是在第一次判断时起作用，while 执行循环，直到条件不成立</span>
            <span class="c1"># 有放回的随机采样，有一些样本被重复采样，从而在训练集中多次出现，有的则从未在训练集中出现，此则自助采样法。从而保证每棵决策树训练集的差异性            </span>
            <span class="n">index</span> <span class="o">=</span> <span class="n">randrange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset_copy</span><span class="p">))</span>
            <span class="c1"># 将对应索引 index 的内容从 dataset_copy 中导出，并将该内容从 dataset_copy 中删除。</span>
            <span class="c1"># pop() 函数用于移除列表中的一个元素（默认最后一个元素），并且返回该元素的值。</span>
            <span class="n">fold</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dataset_copy</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">index</span><span class="p">))</span>  <span class="c1"># 无放回的方式</span>
            <span class="c1"># fold.append(dataset_copy[index])  # 有放回的方式</span>
        <span class="n">dataset_split</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fold</span><span class="p">)</span>
    <span class="c1"># 由dataset分割出的n_folds个数据构成的列表，为了用于交叉验证</span>
    <span class="k">return</span> <span class="n">dataset_split</span>
</pre></div>

<ul>
<li>训练数据集随机化</li>
</ul>
<div class="highlight"><pre><span></span><span class="c1"># Create a random subsample from the dataset with replacement</span>
<span class="k">def</span> <span class="nf">subsample</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">ratio</span><span class="p">):</span>   <span class="c1"># 创建数据集的随机子样本</span>
    <span class="sd">&quot;&quot;&quot;random_forest(评估算法性能，返回模型得分)</span>

<span class="sd">    Args:</span>
<span class="sd">        dataset         训练数据集</span>
<span class="sd">        ratio           训练数据集的样本比例</span>
<span class="sd">    Returns:</span>
<span class="sd">        sample          随机抽样的训练样本</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">sample</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
    <span class="c1"># 训练样本的按比例抽样。</span>
    <span class="c1"># round() 方法返回浮点数x的四舍五入值。</span>
    <span class="n">n_sample</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span> <span class="o">*</span> <span class="n">ratio</span><span class="p">)</span>
    <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">n_sample</span><span class="p">:</span>
        <span class="c1"># 有放回的随机采样，有一些样本被重复采样，从而在训练集中多次出现，有的则从未在训练集中出现，此则自助采样法。从而保证每棵决策树训练集的差异性</span>
        <span class="n">index</span> <span class="o">=</span> <span class="n">randrange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">))</span>
        <span class="n">sample</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="n">index</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">sample</span>
</pre></div>

<ul>
<li>特征随机化</li>
</ul>
<div class="highlight"><pre><span></span><span class="c1"># 找出分割数据集的最优特征，得到最优的特征 index，特征值 row[index]，以及分割完的数据 groups（left, right）</span>
<span class="k">def</span> <span class="nf">get_split</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">n_features</span><span class="p">):</span>
    <span class="n">class_values</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">))</span>  <span class="c1"># class_values =[0, 1]</span>
    <span class="n">b_index</span><span class="p">,</span> <span class="n">b_value</span><span class="p">,</span> <span class="n">b_score</span><span class="p">,</span> <span class="n">b_groups</span> <span class="o">=</span> <span class="mi">999</span><span class="p">,</span> <span class="mi">999</span><span class="p">,</span> <span class="mi">999</span><span class="p">,</span> <span class="bp">None</span>
    <span class="n">features</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
    <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">n_features</span><span class="p">:</span>
        <span class="n">index</span> <span class="o">=</span> <span class="n">randrange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># 往 features 添加 n_features 个特征（ n_feature 等于特征数的根号），特征索引从 dataset 中随机取</span>
        <span class="k">if</span> <span class="n">index</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">features</span><span class="p">:</span>
            <span class="n">features</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">features</span><span class="p">:</span>                    <span class="c1"># 在 n_features 个特征中选出最优的特征索引，并没有遍历所有特征，从而保证了每课决策树的差异性</span>
        <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">:</span>
            <span class="n">groups</span> <span class="o">=</span> <span class="n">test_split</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">row</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="n">dataset</span><span class="p">)</span>  <span class="c1"># groups=(left, right), row[index] 遍历每一行 index 索引下的特征值作为分类值 value, 找出最优的分类特征和特征值</span>
            <span class="n">gini</span> <span class="o">=</span> <span class="n">gini_index</span><span class="p">(</span><span class="n">groups</span><span class="p">,</span> <span class="n">class_values</span><span class="p">)</span>
            <span class="c1"># 左右两边的数量越一样，说明数据区分度不高，gini系数越大</span>
            <span class="k">if</span> <span class="n">gini</span> <span class="o">&lt;</span> <span class="n">b_score</span><span class="p">:</span>
                <span class="n">b_index</span><span class="p">,</span> <span class="n">b_value</span><span class="p">,</span> <span class="n">b_score</span><span class="p">,</span> <span class="n">b_groups</span> <span class="o">=</span> <span class="n">index</span><span class="p">,</span> <span class="n">row</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="n">gini</span><span class="p">,</span> <span class="n">groups</span>  <span class="c1"># 最后得到最优的分类特征 b_index,分类特征值 b_value,分类结果 b_groups。b_value 为分错的代价成本</span>
    <span class="c1"># print b_score</span>
    <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;index&#39;</span><span class="p">:</span> <span class="n">b_index</span><span class="p">,</span> <span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="n">b_value</span><span class="p">,</span> <span class="s1">&#39;groups&#39;</span><span class="p">:</span> <span class="n">b_groups</span><span class="p">}</span>
</pre></div>

<ul>
<li>随机森林</li>
</ul>
<div class="highlight"><pre><span></span><span class="c1"># Random Forest Algorithm</span>
<span class="k">def</span> <span class="nf">random_forest</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">test</span><span class="p">,</span> <span class="n">max_depth</span><span class="p">,</span> <span class="n">min_size</span><span class="p">,</span> <span class="n">sample_size</span><span class="p">,</span> <span class="n">n_trees</span><span class="p">,</span> <span class="n">n_features</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;random_forest(评估算法性能，返回模型得分)</span>

<span class="sd">    Args:</span>
<span class="sd">        train           训练数据集</span>
<span class="sd">        test            测试数据集</span>
<span class="sd">        max_depth       决策树深度不能太深，不然容易导致过拟合</span>
<span class="sd">        min_size        叶子节点的大小</span>
<span class="sd">        sample_size     训练数据集的样本比例</span>
<span class="sd">        n_trees         决策树的个数</span>
<span class="sd">        n_features      选取的特征的个数</span>
<span class="sd">    Returns:</span>
<span class="sd">        predictions     每一行的预测结果，bagging 预测最后的分类结果</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">trees</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
    <span class="c1"># n_trees 表示决策树的数量</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_trees</span><span class="p">):</span>
        <span class="c1"># 随机抽样的训练样本， 随机采样保证了每棵决策树训练集的差异性</span>
        <span class="n">sample</span> <span class="o">=</span> <span class="n">subsample</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">sample_size</span><span class="p">)</span>
        <span class="c1"># 创建一个决策树</span>
        <span class="n">tree</span> <span class="o">=</span> <span class="n">build_tree</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">max_depth</span><span class="p">,</span> <span class="n">min_size</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>
        <span class="n">trees</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tree</span><span class="p">)</span>

    <span class="c1"># 每一行的预测结果，bagging 预测最后的分类结果</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="p">[</span><span class="n">bagging_predict</span><span class="p">(</span><span class="n">trees</span><span class="p">,</span> <span class="n">row</span><span class="p">)</span> <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">test</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">predictions</span>
</pre></div>

<blockquote>
<p>测试算法：在采用自定义 n_folds 份随机重抽样 进行测试评估，得出综合的预测评分。</p>
</blockquote>
<ul>
<li>计算随机森林的预测结果的正确率</li>
</ul>
<div class="highlight"><pre><span></span><span class="c1"># 评估算法性能，返回模型得分</span>
<span class="k">def</span> <span class="nf">evaluate_algorithm</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">algorithm</span><span class="p">,</span> <span class="n">n_folds</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;evaluate_algorithm(评估算法性能，返回模型得分)</span>

<span class="sd">    Args:</span>
<span class="sd">        dataset     原始数据集</span>
<span class="sd">        algorithm   使用的算法</span>
<span class="sd">        n_folds     数据的份数</span>
<span class="sd">        *args       其他的参数</span>
<span class="sd">    Returns:</span>
<span class="sd">        scores      模型得分</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># 将数据集进行随机抽样，分成 n_folds 份，数据无重复的抽取</span>
    <span class="n">folds</span> <span class="o">=</span> <span class="n">cross_validation_split</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">n_folds</span><span class="p">)</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
    <span class="c1"># 每次循环从 folds 从取出一个 fold 作为测试集，其余作为训练集，遍历整个 folds ，实现交叉验证</span>
    <span class="k">for</span> <span class="n">fold</span> <span class="ow">in</span> <span class="n">folds</span><span class="p">:</span>
        <span class="n">train_set</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">folds</span><span class="p">)</span>
        <span class="n">train_set</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">fold</span><span class="p">)</span>
        <span class="c1"># 将多个 fold 列表组合成一个 train_set 列表, 类似 union all</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        In [20]: l1=[[1, 2, &#39;a&#39;], [11, 22, &#39;b&#39;]]</span>
<span class="sd">        In [21]: l2=[[3, 4, &#39;c&#39;], [33, 44, &#39;d&#39;]]</span>
<span class="sd">        In [22]: l=[]</span>
<span class="sd">        In [23]: l.append(l1)</span>
<span class="sd">        In [24]: l.append(l2)</span>
<span class="sd">        In [25]: l</span>
<span class="sd">        Out[25]: [[[1, 2, &#39;a&#39;], [11, 22, &#39;b&#39;]], [[3, 4, &#39;c&#39;], [33, 44, &#39;d&#39;]]]</span>
<span class="sd">        In [26]: sum(l, [])</span>
<span class="sd">        Out[26]: [[1, 2, &#39;a&#39;], [11, 22, &#39;b&#39;], [3, 4, &#39;c&#39;], [33, 44, &#39;d&#39;]]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">train_set</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">train_set</span><span class="p">,</span> <span class="p">[])</span>
        <span class="n">test_set</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
        <span class="c1"># fold 表示从原始数据集 dataset 提取出来的测试集</span>
        <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">fold</span><span class="p">:</span>
            <span class="n">row_copy</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">row</span><span class="p">)</span>
            <span class="n">row_copy</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span> 
            <span class="n">test_set</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">row_copy</span><span class="p">)</span>
        <span class="n">predicted</span> <span class="o">=</span> <span class="n">algorithm</span><span class="p">(</span><span class="n">train_set</span><span class="p">,</span> <span class="n">test_set</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>
        <span class="n">actual</span> <span class="o">=</span> <span class="p">[</span><span class="n">row</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">fold</span><span class="p">]</span>

        <span class="c1"># 计算随机森林的预测结果的正确率</span>
        <span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_metric</span><span class="p">(</span><span class="n">actual</span><span class="p">,</span> <span class="n">predicted</span><span class="p">)</span>
        <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">accuracy</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">scores</span>
</pre></div>

<blockquote>
<p>使用算法：若你感兴趣可以构建完整的应用程序，从案例进行封装，也可以参考我们的代码</p>
</blockquote>
<p><a href="https://github.com/apachecn/MachineLearning/blob/master/src/python/7.RandomForest/randomForest.py">完整代码地址</a>: <a href="https://github.com/apachecn/MachineLearning/blob/master/src/python/7.RandomForest/randomForest.py">https://github.com/apachecn/MachineLearning/blob/master/src/python/7.RandomForest/randomForest.py</a></p>
<h2 id="adaboost">AdaBoost</h2>
<h3 id="adaboost-adaptive-boosting-boosting">AdaBoost (adaptive boosting: 自适应 boosting) 概述</h3>
<p><code>能否使用弱分类器和多个实例来构建一个强分类器？ 这是一个非常有趣的理论问题。</code></p>
<h3 id="adaboost_1">AdaBoost 原理</h3>
<blockquote>
<p>AdaBoost 工作原理</p>
</blockquote>
<p>![AdaBoost 工作原理]({{ site.baseurl }}{{ site.images }}/MachineLearninginAction/7.AdaBoost/adaboost_illustration.png "AdaBoost 工作原理")</p>
<blockquote>
<p>AdaBoost 开发流程</p>
</blockquote>
<div class="highlight"><pre><span></span>收集数据：可以使用任意方法
准备数据：依赖于所使用的弱分类器类型，本章使用的是单层决策树，这种分类器可以处理任何数据类型。
    当然也可以使用任意分类器作为弱分类器，第2章到第6章中的任一分类器都可以充当弱分类器。
    作为弱分类器，简单分类器的效果更好。
分析数据：可以使用任意方法。
训练算法：AdaBoost 的大部分时间都用在训练上，分类器将多次在同一数据集上训练弱分类器。
测试算法：计算分类的错误率。
使用算法：通SVM一样，AdaBoost 预测两个类别中的一个。如果想把它应用到多个类别的场景，那么就要像多类 SVM 中的做法一样对 AdaBoost 进行修改。
</pre></div>

<blockquote>
<p>AdaBoost 算法特点</p>
</blockquote>
<div class="highlight"><pre><span></span>* 优点：泛化（由具体的、个别的扩大为一般的）错误率低，易编码，可以应用在大部分分类器上，无参数调节。
* 缺点：对离群点敏感。
* 适用数据类型：数值型和标称型数据。
</pre></div>

<h3 id="_8">项目案例: 马疝病的预测</h3>
<blockquote>
<p>项目流程图</p>
</blockquote>
<p>![AdaBoost代码流程图]({{ site.baseurl }}{{ site.images }}/MachineLearninginAction/7.AdaBoost/adaboost_code-flow-chart.jpg "AdaBoost代码流程图")</p>
<p>基于单层决策树构建弱分类器
* 单层决策树(decision stump, 也称决策树桩)是一种简单的决策树。</p>
<h4 id="_9">项目概述</h4>
<p>预测患有疝气病的马的存活问题，这里的数据包括368个样本和28个特征，疝气病是描述马胃肠痛的术语，然而，这种病并不一定源自马的胃肠问题，其他问题也可能引发疝气病，该数据集中包含了医院检测马疝气病的一些指标，有的指标比较主观，有的指标难以测量，例如马的疼痛级别。另外，除了部分指标主观和难以测量之外，该数据还存在一个问题，数据集中有30%的值是缺失的。</p>
<h4 id="_10">开发流程</h4>
<div class="highlight"><pre><span></span>收集数据：提供的文本文件
准备数据：确保类别标签是+1和-1，而非1和0
分析数据：统计分析
训练算法：在数据上，利用 adaBoostTrainDS() 函数训练出一系列的分类器
测试算法：我们拥有两个数据集。在不采用随机抽样的方法下，我们就会对 AdaBoost 和 Logistic 回归的结果进行完全对等的比较
使用算法：观察该例子上的错误率。不过，也可以构建一个 Web 网站，让驯马师输入马的症状然后预测马是否会死去
</pre></div>

<blockquote>
<p>收集数据：提供的文本文件</p>
</blockquote>
<p>训练数据：horseColicTraining.txt<br/>
测试数据：horseColicTest.txt</p>
<div class="highlight"><pre><span></span>2.000000    1.000000    38.500000   66.000000   28.000000   3.000000    3.000000    0.000000    2.000000    5.000000    4.000000    4.000000    0.000000    0.000000    0.000000    3.000000    5.000000    45.000000   8.400000    0.000000    0.000000    -1.000000
1.000000    1.000000    39.200000   88.000000   20.000000   0.000000    0.000000    4.000000    1.000000    3.000000    4.000000    2.000000    0.000000    0.000000    0.000000    4.000000    2.000000    50.000000   85.000000   2.000000    2.000000    -1.000000
2.000000    1.000000    38.300000   40.000000   24.000000   1.000000    1.000000    3.000000    1.000000    3.000000    3.000000    1.000000    0.000000    0.000000    0.000000    1.000000    1.000000    33.000000   6.700000    0.000000    0.000000    1.000000
</pre></div>

<blockquote>
<p>准备数据：确保类别标签是+1和-1，而非1和0</p>
</blockquote>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">loadDataSet</span><span class="p">(</span><span class="n">fileName</span><span class="p">):</span>
    <span class="c1"># 获取 feature 的数量, 便于获取</span>
    <span class="n">numFeat</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="n">fileName</span><span class="p">)</span><span class="o">.</span><span class="n">readline</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">))</span>
    <span class="n">dataArr</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">labelArr</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">fr</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">fileName</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">fr</span><span class="o">.</span><span class="n">readlines</span><span class="p">():</span>
        <span class="n">lineArr</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">curLine</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">numFeat</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">lineArr</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">curLine</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
        <span class="n">dataArr</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lineArr</span><span class="p">)</span>
        <span class="n">labelArr</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">curLine</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
    <span class="k">return</span> <span class="n">dataArr</span><span class="p">,</span> <span class="n">labelArr</span>
</pre></div>

<blockquote>
<p>分析数据：统计分析</p>
</blockquote>
<p>过拟合(overfitting, 也称为过学习)
* 发现测试错误率在达到一个最小值之后有开始上升，这种现象称为过拟合。</p>
<p>![过拟合]({{ site.baseurl }}{{ site.images }}/MachineLearninginAction/7.AdaBoost/过拟合.png)</p>
<ul>
<li>通俗来说：就是把一些噪音数据也拟合进去的，如下图。</li>
</ul>
<p>![过拟合]({{ site.baseurl }}{{ site.images }}/MachineLearninginAction/7.AdaBoost/过拟合图解.png)</p>
<blockquote>
<p>训练算法：在数据上，利用 adaBoostTrainDS() 函数训练出一系列的分类器</p>
</blockquote>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">adaBoostTrainDS</span><span class="p">(</span><span class="n">dataArr</span><span class="p">,</span> <span class="n">labelArr</span><span class="p">,</span> <span class="n">numIt</span><span class="o">=</span><span class="mi">40</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;adaBoostTrainDS(adaBoost训练过程放大)</span>
<span class="sd">    Args:</span>
<span class="sd">        dataArr   特征标签集合</span>
<span class="sd">        labelArr  分类标签集合</span>
<span class="sd">        numIt     实例数</span>
<span class="sd">    Returns:</span>
<span class="sd">        weakClassArr  弱分类器的集合</span>
<span class="sd">        aggClassEst   预测的分类结果值</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">weakClassArr</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">shape</span><span class="p">(</span><span class="n">dataArr</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="c1"># 初始化 D，设置每个样本的权重值，平均分为m份</span>
    <span class="n">D</span> <span class="o">=</span> <span class="n">mat</span><span class="p">(</span><span class="n">ones</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">/</span><span class="n">m</span><span class="p">)</span>
    <span class="n">aggClassEst</span> <span class="o">=</span> <span class="n">mat</span><span class="p">(</span><span class="n">zeros</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">numIt</span><span class="p">):</span>
        <span class="c1"># 得到决策树的模型</span>
        <span class="n">bestStump</span><span class="p">,</span> <span class="n">error</span><span class="p">,</span> <span class="n">classEst</span> <span class="o">=</span> <span class="n">buildStump</span><span class="p">(</span><span class="n">dataArr</span><span class="p">,</span> <span class="n">labelArr</span><span class="p">,</span> <span class="n">D</span><span class="p">)</span>

        <span class="c1"># alpha目的主要是计算每一个分类器实例的权重(组合就是分类结果)</span>
        <span class="c1"># 计算每个分类器的alpha权重值</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="mf">0.5</span><span class="o">*</span><span class="n">log</span><span class="p">((</span><span class="mf">1.0</span><span class="o">-</span><span class="n">error</span><span class="p">)</span><span class="o">/</span><span class="nb">max</span><span class="p">(</span><span class="n">error</span><span class="p">,</span> <span class="mf">1e-16</span><span class="p">)))</span>
        <span class="n">bestStump</span><span class="p">[</span><span class="s1">&#39;alpha&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="c1"># store Stump Params in Array</span>
        <span class="n">weakClassArr</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">bestStump</span><span class="p">)</span>

        <span class="k">print</span> <span class="s2">&quot;alpha=</span><span class="si">%s</span><span class="s2">, classEst=</span><span class="si">%s</span><span class="s2">, bestStump=</span><span class="si">%s</span><span class="s2">, error=</span><span class="si">%s</span><span class="s2"> &quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">classEst</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">bestStump</span><span class="p">,</span> <span class="n">error</span><span class="p">)</span>
        <span class="c1"># 分类正确：乘积为1，不会影响结果，-1主要是下面求e的-alpha次方</span>
        <span class="c1"># 分类错误：乘积为 -1，结果会受影响，所以也乘以 -1</span>
        <span class="n">expon</span> <span class="o">=</span> <span class="n">multiply</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="o">*</span><span class="n">alpha</span><span class="o">*</span><span class="n">mat</span><span class="p">(</span><span class="n">labelArr</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">classEst</span><span class="p">)</span>
        <span class="k">print</span> <span class="s1">&#39;(-1取反)预测值expon=&#39;</span><span class="p">,</span> <span class="n">expon</span><span class="o">.</span><span class="n">T</span>
        <span class="c1"># 计算e的expon次方，然后计算得到一个综合的概率的值</span>
        <span class="c1"># 结果发现： 判断错误的样本，D对于的样本权重值会变大。</span>
        <span class="n">D</span> <span class="o">=</span> <span class="n">multiply</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">exp</span><span class="p">(</span><span class="n">expon</span><span class="p">))</span>
        <span class="n">D</span> <span class="o">=</span> <span class="n">D</span><span class="o">/</span><span class="n">D</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

        <span class="c1"># 预测的分类结果值，在上一轮结果的基础上，进行加和操作</span>
        <span class="k">print</span> <span class="s1">&#39;当前的分类结果：&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">*</span><span class="n">classEst</span><span class="o">.</span><span class="n">T</span>
        <span class="n">aggClassEst</span> <span class="o">+=</span> <span class="n">alpha</span><span class="o">*</span><span class="n">classEst</span>
        <span class="k">print</span> <span class="s2">&quot;叠加后的分类结果aggClassEst: &quot;</span><span class="p">,</span> <span class="n">aggClassEst</span><span class="o">.</span><span class="n">T</span>
        <span class="c1"># sign 判断正为1， 0为0， 负为-1，通过最终加和的权重值，判断符号。</span>
        <span class="c1"># 结果为：错误的样本标签集合，因为是 !=,那么结果就是0 正, 1 负</span>
        <span class="n">aggErrors</span> <span class="o">=</span> <span class="n">multiply</span><span class="p">(</span><span class="n">sign</span><span class="p">(</span><span class="n">aggClassEst</span><span class="p">)</span> <span class="o">!=</span> <span class="n">mat</span><span class="p">(</span><span class="n">labelArr</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">ones</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="n">errorRate</span> <span class="o">=</span> <span class="n">aggErrors</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="n">m</span>
        <span class="c1"># print &quot;total error=%s &quot; % (errorRate)</span>
        <span class="k">if</span> <span class="n">errorRate</span> <span class="o">==</span> <span class="mf">0.0</span><span class="p">:</span>
            <span class="k">break</span>
    <span class="k">return</span> <span class="n">weakClassArr</span><span class="p">,</span> <span class="n">aggClassEst</span>
</pre></div>

<div class="highlight"><pre><span></span>发现：
alpha （模型权重）目的主要是计算每一个分类器实例的权重(加和就是分类结果)
  分类的权重值：最大的值= alpha 的加和，最小值=-最大值
D （样本权重）的目的是为了计算错误概率： weightedError = D.T*errArr，求最佳分类器
  样本的权重值：如果一个值误判的几率越小，那么 D 的样本权重越小
</pre></div>

<p>![AdaBoost算法权重计算公式]({{ site.baseurl }}{{ site.images }}/MachineLearninginAction/7.AdaBoost/adaboost_alpha.png "AdaBoost算法权重计算公式")</p>
<blockquote>
<p>测试算法：我们拥有两个数据集。在不采用随机抽样的方法下，我们就会对 AdaBoost 和 Logistic 回归的结果进行完全对等的比较。</p>
</blockquote>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">adaClassify</span><span class="p">(</span><span class="n">datToClass</span><span class="p">,</span> <span class="n">classifierArr</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;adaClassify(ada分类测试)</span>
<span class="sd">    Args:</span>
<span class="sd">        datToClass     多个待分类的样例</span>
<span class="sd">        classifierArr  弱分类器的集合</span>
<span class="sd">    Returns:</span>
<span class="sd">        sign(aggClassEst) 分类结果</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># do stuff similar to last aggClassEst in adaBoostTrainDS</span>
    <span class="n">dataMat</span> <span class="o">=</span> <span class="n">mat</span><span class="p">(</span><span class="n">datToClass</span><span class="p">)</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">shape</span><span class="p">(</span><span class="n">dataMat</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">aggClassEst</span> <span class="o">=</span> <span class="n">mat</span><span class="p">(</span><span class="n">zeros</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>

    <span class="c1"># 循环 多个分类器</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">classifierArr</span><span class="p">)):</span>
        <span class="c1"># 前提： 我们已经知道了最佳的分类器的实例</span>
        <span class="c1"># 通过分类器来核算每一次的分类结果，然后通过alpha*每一次的结果 得到最后的权重加和的值。</span>
        <span class="n">classEst</span> <span class="o">=</span> <span class="n">stumpClassify</span><span class="p">(</span><span class="n">dataMat</span><span class="p">,</span> <span class="n">classifierArr</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;dim&#39;</span><span class="p">],</span> <span class="n">classifierArr</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;thresh&#39;</span><span class="p">],</span> <span class="n">classifierArr</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;ineq&#39;</span><span class="p">])</span>
        <span class="n">aggClassEst</span> <span class="o">+=</span> <span class="n">classifierArr</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;alpha&#39;</span><span class="p">]</span><span class="o">*</span><span class="n">classEst</span>
    <span class="k">return</span> <span class="n">sign</span><span class="p">(</span><span class="n">aggClassEst</span><span class="p">)</span>
</pre></div>

<blockquote>
<p>使用算法：观察该例子上的错误率。不过，也可以构建一个 Web 网站，让驯马师输入马的症状然后预测马是否会死去。</p>
</blockquote>
<div class="highlight"><pre><span></span><span class="c1"># 马疝病数据集</span>
<span class="c1"># 训练集合</span>
<span class="n">dataArr</span><span class="p">,</span> <span class="n">labelArr</span> <span class="o">=</span> <span class="n">loadDataSet</span><span class="p">(</span><span class="s2">&quot;input/7.AdaBoost/horseColicTraining2.txt&quot;</span><span class="p">)</span>
<span class="n">weakClassArr</span><span class="p">,</span> <span class="n">aggClassEst</span> <span class="o">=</span> <span class="n">adaBoostTrainDS</span><span class="p">(</span><span class="n">dataArr</span><span class="p">,</span> <span class="n">labelArr</span><span class="p">,</span> <span class="mi">40</span><span class="p">)</span>
<span class="k">print</span> <span class="n">weakClassArr</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">-----</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">aggClassEst</span><span class="o">.</span><span class="n">T</span>
<span class="c1"># 计算ROC下面的AUC的面积大小</span>
<span class="n">plotROC</span><span class="p">(</span><span class="n">aggClassEst</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">labelArr</span><span class="p">)</span>
<span class="c1"># 测试集合</span>
<span class="n">dataArrTest</span><span class="p">,</span> <span class="n">labelArrTest</span> <span class="o">=</span> <span class="n">loadDataSet</span><span class="p">(</span><span class="s2">&quot;input/7.AdaBoost/horseColicTest2.txt&quot;</span><span class="p">)</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">shape</span><span class="p">(</span><span class="n">dataArrTest</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">predicting10</span> <span class="o">=</span> <span class="n">adaClassify</span><span class="p">(</span><span class="n">dataArrTest</span><span class="p">,</span> <span class="n">weakClassArr</span><span class="p">)</span>
<span class="n">errArr</span> <span class="o">=</span> <span class="n">mat</span><span class="p">(</span><span class="n">ones</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="c1"># 测试：计算总样本数，错误样本数，错误率</span>
<span class="k">print</span> <span class="n">m</span><span class="p">,</span> <span class="n">errArr</span><span class="p">[</span><span class="n">predicting10</span> <span class="o">!=</span> <span class="n">mat</span><span class="p">(</span><span class="n">labelArrTest</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span> <span class="n">errArr</span><span class="p">[</span><span class="n">predicting10</span> <span class="o">!=</span> <span class="n">mat</span><span class="p">(</span><span class="n">labelArrTest</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="n">m</span>
</pre></div>

<p><a href="https://github.com/apachecn/MachineLearning/blob/master/src/python/7.AdaBoost/adaboost.py">完整代码地址</a>: <a href="https://github.com/apachecn/MachineLearning/blob/master/src/python/7.AdaBoost/adaboost.py">https://github.com/apachecn/MachineLearning/blob/master/src/python/7.AdaBoost/adaboost.py</a></p>
<h4 id="_11">要点补充</h4>
<blockquote>
<p>非均衡现象：</p>
</blockquote>
<p><code>在分类器训练时，正例数目和反例数目不相等（相差很大）</code></p>
<ul>
<li>判断马是否能继续生存(不可误杀)</li>
<li>过滤垃圾邮件(不可漏判)</li>
<li>不能放过传染病的人</li>
<li>不能随便认为别人犯罪</li>
</ul>
<blockquote>
<p>ROC 评估方法</p>
</blockquote>
<ul>
<li>ROC 曲线: 最佳的分类器应该尽可能地处于左上角</li>
</ul>
<p>![ROC曲线]({{ site.baseurl }}{{ site.images }}/MachineLearninginAction/7.AdaBoost/ROC曲线.png)</p>
<ul>
<li>对不同的 ROC 曲线进行比较的一个指标是曲线下的面积(Area Unser the Curve, AUC). </li>
<li>AUC 给出的是分类器的平均性能值，当然它并不能完全代替对整条曲线的观察。</li>
<li>一个完美分类器的 AUC 为1，而随机猜测的 AUC 则为0.5。</li>
</ul>
<blockquote>
<p>代价函数</p>
</blockquote>
<ul>
<li>基于代价函数的分类器决策控制：<code>TP*(-5)+FN*1+FP*50+TN*0</code></li>
</ul>
<p>![代价函数]({{ site.baseurl }}{{ site.images }}/MachineLearninginAction/7.AdaBoost/代价函数.png)</p>
<blockquote>
<p>抽样</p>
</blockquote>
<ul>
<li>欠抽样(undersampling)或者过抽样(oversampling)<ul>
<li>欠抽样: 意味着删除样例</li>
<li>过抽样: 意味着复制样例(重复使用)</li>
</ul>
</li>
</ul>
<hr />
<ul>
<li><strong>作者：<a href="http://www.apache.wiki/display/~jiangzhonglian">片刻</a></strong></li>
<li><a href="https://github.com/apachecn/MachineLearning">GitHub地址</a>: <a href="https://github.com/apachecn/MachineLearning">https://github.com/apachecn/MachineLearning</a></li>
<li><strong>版权声明：欢迎转载学习 =&gt; 请标注信息来源于 <a href="http://www.apachecn.org/">ApacheCN</a></strong></li>
</ul></div>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>var base_url = '../..';</script>
        <script src="../../js/base.js"></script>
        <script src="../../javascripts/extra.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
        <script src="../../search/require.js"></script>
        <script src="../../search/search.js"></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="Search Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Search</h4>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form role="form">
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="Keyboard Shortcuts Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Keyboard Shortcuts</h4>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td><kbd>&larr;</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td><kbd>&rarr;</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>


    </body>
</html>

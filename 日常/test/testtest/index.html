<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        <meta name="author" content="NiuLiangtao">
        <link rel="canonical" href="https://1007530194.github.io/Diary/日常/test/testtest/">
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>test - Blog of NiuLiangtao</title>
        <link href="../../../css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome-4.5.0.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="../../../css/highlight.css">
        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
        <![endif]-->

        <script src="../../../js/jquery-1.10.2.min.js"></script>
        <script src="../../../js/bootstrap-3.0.3.min.js"></script>
        <script src="../../../js/highlight.pack.js"></script> 
    </head>

    <body>

        <div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">

        <!-- Collapsed navigation -->
        <div class="navbar-header">
            <!-- Expander button -->
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="../../../ToDo/">Blog of NiuLiangtao</a>
        </div>

        <!-- Expanded navigation -->
        <div class="navbar-collapse collapse">
                <!-- Main navigation -->
                <ul class="nav navbar-nav">
                    <li >
                        <a href="../../../ToDo/">Home</a>
                    </li>
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Home <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                            
<li >
    <a href="../../../AboutMe/">About</a>
</li>
                            
<li >
    <a href="../../..">Home</a>
</li>
                            
<li >
    <a href="../../../Nothing/">Nothing</a>
</li>
                            
<li >
    <a href="../../../ToDo/">要做的</a>
</li>
                        </ul>
                    </li>
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">书籍 <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                            
<li >
    <a href="../../../书籍/">Home</a>
</li>
                            
  <li class="dropdown-submenu">
    <a href="#">机器学习实战</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../../书籍/机器学习实战/00naive-bayes-discuss/">naive-bayes-discuss</a>
</li>
            
<li >
    <a href="../../../书籍/机器学习实战/01.机器学习基础/">1.机器学习基础</a>
</li>
            
<li >
    <a href="../../../书籍/机器学习实战/02.k-近邻算法/">2.k-近邻算法</a>
</li>
            
<li >
    <a href="../../../书籍/机器学习实战/03.决策树/">3.决策树</a>
</li>
            
<li >
    <a href="../../../书籍/机器学习实战/04.朴素贝叶斯/">4.朴素贝叶斯</a>
</li>
            
<li >
    <a href="../../../书籍/机器学习实战/05.Logistic回归/">5.Logistic回归</a>
</li>
            
<li >
    <a href="../../../书籍/机器学习实战/06.0.支持向量机/">6.支持向量机</a>
</li>
            
<li >
    <a href="../../../书籍/机器学习实战/06.1.支持向量机的几个通俗理解/">6.1.支持向量机的几个通俗理解</a>
</li>
            
<li >
    <a href="../../../书籍/机器学习实战/07.集成方法-随机森林和AdaBoost/">7.集成方法-随机森林和AdaBoost</a>
</li>
            
<li >
    <a href="../../../书籍/机器学习实战/08.预测数值型数据-回归/">8.预测数值型数据：回归</a>
</li>
            
<li >
    <a href="../../../书籍/机器学习实战/09.树回归/">9.树回归</a>
</li>
            
<li >
    <a href="../../../书籍/机器学习实战/10.k-means聚类/">10.k-means聚类</a>
</li>
            
<li >
    <a href="../../../书籍/机器学习实战/11.使用Apriori算法进行关联分析/">11.使用Apriori算法进行关联分析</a>
</li>
            
<li >
    <a href="../../../书籍/机器学习实战/12.使用FP-growth算法来高效发现频繁项集/">12.使用FP-growth算法来高效发现频繁项集</a>
</li>
            
<li >
    <a href="../../../书籍/机器学习实战/13.利用PCA来简化数据/">13.利用PCA来简化数据</a>
</li>
            
<li >
    <a href="../../../书籍/机器学习实战/14.利用SVD简化数据/">14.利用SVD简化数据</a>
</li>
            
<li >
    <a href="../../../书籍/机器学习实战/15.大数据与MapReduce/">15.大数据与MapReduce</a>
</li>
            
<li >
    <a href="../../../书籍/机器学习实战/16.推荐系统/">16.推荐系统</a>
</li>
    </ul>
  </li>
                            
  <li class="dropdown-submenu">
    <a href="#">深度学习</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../../书籍/深度学习/DeepLearning/">深度学习中文版</a>
</li>
    </ul>
  </li>
                        </ul>
                    </li>
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">学习 <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                            
<li >
    <a href="../../../学习/">Home</a>
</li>
                            
  <li class="dropdown-submenu">
    <a href="#">tensorflow</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../../学习/tensorflow/如何选择优化器 optimizer/">如何选择优化器 optimizer</a>
</li>
    </ul>
  </li>
                            
  <li class="dropdown-submenu">
    <a href="#">深度学习</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../../学习/深度学习/主成分分析-2/">主成分分析 (2)</a>
</li>
            
<li >
    <a href="../../../学习/深度学习/主成分分析/">主成分分析</a>
</li>
            
<li >
    <a href="../../../学习/深度学习/最小二乘法/">最小二乘法</a>
</li>
            
<li >
    <a href="../../../学习/深度学习/矩阵分解/">矩阵分解</a>
</li>
            
<li >
    <a href="../../../学习/深度学习/矩阵求导/">矩阵求导</a>
</li>
    </ul>
  </li>
                            
  <li class="dropdown-submenu">
    <a href="#">转载</a>
    <ul class="dropdown-menu">
            
  <li class="dropdown-submenu">
    <a href="#">cs231n-ch</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../../学习/转载/cs231n-ch/ch-assignment2_google_cloud/">Google Cloud Tutorial Part 2 (with GPUs)</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-ch/ch-aws-tutorial/">aws-tutorial</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-ch/ch-classification/">classification</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-ch/ch-convnet-tips/">convnet-tips</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-ch/ch-convolutional-networks/">convolutional-networks</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-ch/ch-google_cloud_tutorial/">Google Cloud Tutorial gce-tutorial</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-ch/ch-ipython-tutorial/">IPython Tutorial ipython-tutorial</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-ch/ch-linear-classify/">线性分类</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-ch/ch-neural-networks-1/">neural-networks-1</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-ch/ch-neural-networks-2/">neural-networks-2</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-ch/ch-neural-networks-3/">neural-networks-3</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-ch/ch-neural-networks-case-study/">neural-networks-case-study</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-ch/ch-optimization-1/">optimization-1</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-ch/ch-optimization-2/">optimization-2</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-ch/ch-overview/">Overview of Computer Vision and Visual Recognition overview</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-ch/ch-poster/">CS231n Poster Session poster-session</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-ch/ch-python-numpy-tutorial/">Python Numpy Tutorial python-numpy-tutorial</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-ch/ch-Readme/">ch Readme</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-ch/ch-transfer-learning/">transfer-learning</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-ch/ch-understanding-cnn/">understanding-cnn</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-ch/terminal-tutorial/">Terminal.com Tutorial terminal-tutorial</a>
</li>
    </ul>
  </li>
            
  <li class="dropdown-submenu">
    <a href="#">cs231n-en</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../../学习/转载/cs231n-en/en-assignment2_google_cloud/">Google Cloud Tutorial Part 2 (with GPUs)</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-en/en-aws-tutorial/">aws-tutorial</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-en/en-classification/">classification</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-en/en-convnet-tips/">convnet-tips</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-en/en-convolutional-networks/">convolutional-networks</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-en/en-google_cloud_tutorial/">Google Cloud Tutorial</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-en/en-ipython-tutorial/">IPython Tutorial ipython-tutorial</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-en/en-linear-classify/">linear-classify</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-en/en-neural-networks-1/">neural-networks-1</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-en/en-neural-networks-2/">neural-networks-2</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-en/en-neural-networks-3/">neural-networks-3</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-en/en-neural-networks-case-study/">neural-networks-case-study</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-en/en-optimization-1/">optimization-1</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-en/en-optimization-2/">optimization-2</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-en/en-overview/">Overview of Computer Vision and Visual Recognition overview</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-en/en-poster/">CS231n Poster Session poster-session</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-en/en-python-numpy-tutorial/">Python Numpy Tutorial python-numpy-tutorial</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-en/en-Readme/">en Readme</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-en/en-terminal-tutorial/">Terminal.com Tutorial terminal-tutorial</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-en/en-transfer-learning/">transfer-learning</a>
</li>
            
<li >
    <a href="../../../学习/转载/cs231n-en/en-understanding-cnn/">understanding-cnn</a>
</li>
    </ul>
  </li>
            
  <li class="dropdown-submenu">
    <a href="#">example</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../../学习/转载/example/a-web-crawer-with-a-asyncio-coroutines-1/">A web crawer with a asyncio coroutines 1</a>
</li>
            
<li >
    <a href="../../../学习/转载/example/a-web-crawer-with-a-asyncio-coroutines-2/">A web crawer with a asyncio coroutines 2</a>
</li>
            
<li >
    <a href="../../../学习/转载/example/bias-variance/">Bias variance</a>
</li>
            
<li >
    <a href="../../../学习/转载/example/bitcoin-explained-1/">用表情符号解释比特币 (1)</a>
</li>
            
<li >
    <a href="../../../学习/转载/example/dive-into-gradient-decent/">理解梯度下降</a>
</li>
            
<li >
    <a href="../../../学习/转载/example/ethereum-ultimate-guide/">Ethereum ultimate guide</a>
</li>
            
<li >
    <a href="../../../学习/转载/example/fork-exec-source/">Fork exec source</a>
</li>
            
<li >
    <a href="../../../学习/转载/example/latex语法/">Latex语法</a>
</li>
            
<li >
    <a href="../../../学习/转载/example/latex语法1/">latex语法</a>
</li>
            
<li >
    <a href="../../../学习/转载/example/nosql-in-python/">Nosql in python</a>
</li>
            
<li >
    <a href="../../../学习/转载/example/perceptron/">"感知器"</a>
</li>
            
<li >
    <a href="../../../学习/转载/example/quick-latex/">Quick latex</a>
</li>
            
<li >
    <a href="../../../学习/转载/example/tendermint/">Tendermint</a>
</li>
    </ul>
  </li>
            
  <li class="dropdown-submenu">
    <a href="#">example2</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../../学习/转载/example2/arrays-similar/">"判断两个数组是否相似 (arraysSimilar)"</a>
</li>
            
<li >
    <a href="../../../学习/转载/example2/baidu-ife-1/">"百度Web前端技术学院(1)-HTML, CSS基础"</a>
</li>
            
<li >
    <a href="../../../学习/转载/example2/create-my-blog-with-jekyll/">"Jekyll 搭建静态博客"</a>
</li>
            
<li >
    <a href="../../../学习/转载/example2/front-end-tools/">"前端的一些资料和工具"</a>
</li>
            
<li >
    <a href="../../../学习/转载/example2/git-clone-not-master-branch/">"Git 如何 clone 非 master 分支的代码"</a>
</li>
            
<li >
    <a href="../../../学习/转载/example2/History-API/">"前端处理动态 url 和 pushStatus 的使用"</a>
</li>
            
<li >
    <a href="../../../学习/转载/example2/how-to-use-babel/">"如何使用 babel"</a>
</li>
            
<li >
    <a href="../../../学习/转载/example2/how-to-write-a-count-down/">"前端如何写一个精确的倒计时"</a>
</li>
            
<li >
    <a href="../../../学习/转载/example2/JavaScript-closure/">"JavaScript 中的闭包"</a>
</li>
            
<li >
    <a href="../../../学习/转载/example2/JavaScript-function/">"JavaScript 函数"</a>
</li>
            
<li >
    <a href="../../../学习/转载/example2/JavaScript-good-parts-note1/">"JavaScript 语言精粹笔记1-语法、对象、函数"</a>
</li>
            
<li >
    <a href="../../../学习/转载/example2/JavaScript-good-parts-note2/">"JavaScript 语言精粹笔记2-继承、数组、正则表达式"</a>
</li>
            
<li >
    <a href="../../../学习/转载/example2/JavaScript-good-parts-note3/">"JavaScript 语言精粹笔记3-方法、毒瘤等"</a>
</li>
            
<li >
    <a href="../../../学习/转载/example2/JavaScript-Net/">"JavaScript 阶段总结"</a>
</li>
            
<li >
    <a href="../../../学习/转载/example2/JavaScript-Object-Oriented/">"JavaScript 面向对象"</a>
</li>
            
<li >
    <a href="../../../学习/转载/example2/JavaScript-this/">"JavaScript 中的 this"</a>
</li>
            
<li >
    <a href="../../../学习/转载/example2/jekyll-theme-version-2.0/">"对这个 jekyll 博客主题的改版和重构"</a>
</li>
            
<li >
    <a href="../../../学习/转载/example2/js-create-file-and-download/">"使用 JavaScript 创建并下载文件"</a>
</li>
            
<li >
    <a href="../../../学习/转载/example2/low-IE-click-empty-block-bug/">"在低版本 IE 中点击空 block 元素的问题"</a>
</li>
            
<li >
    <a href="../../../学习/转载/example2/regular-expression-group/">"浅谈正则表达式中的分组和引用"</a>
</li>
            
<li >
    <a href="../../../学习/转载/example2/scope/">"JavaScript 作用域和作用域链"</a>
</li>
            
<li >
    <a href="../../../学习/转载/example2/shuffle-algorithm/">"Fisher–Yates shuffle 洗牌算法"</a>
</li>
            
<li >
    <a href="../../../学习/转载/example2/sublimeLinter/">"代码校验工具 SublimeLinter 的安装与使用"</a>
</li>
            
<li >
    <a href="../../../学习/转载/example2/Syncing-a-fork/">"同步一个 fork"</a>
</li>
            
<li >
    <a href="../../../学习/转载/example2/teach-girlfriend-html-css/">"Teach Girlfriend to make a Web Pages like Zhihu"</a>
</li>
            
<li >
    <a href="../../../学习/转载/example2/web-app/">"Web App 相关技术"</a>
</li>
            
<li >
    <a href="../../../学习/转载/example2/weinre/">"Weinre --WebApp 调试工具"</a>
</li>
    </ul>
  </li>
    </ul>
  </li>
                        </ul>
                    </li>
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">工具 <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                            
<li >
    <a href="../../../工具/About/">About</a>
</li>
                            
<li >
    <a href="../../../工具/Cheatsheet/">Cheatsheet</a>
</li>
                            
<li >
    <a href="../../../工具/">Home</a>
</li>
                            
<li >
    <a href="../../../工具/shell显示时间/">Shell显示时间</a>
</li>
                            
  <li class="dropdown-submenu">
    <a href="#">常用工具</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../../工具/常用工具/10分钟上手Latex/">10分钟上手Latex</a>
</li>
            
<li >
    <a href="../../../工具/常用工具/10分钟上手Pandas/">10分钟上手Pandas</a>
</li>
            
<li >
    <a href="../../../工具/常用工具/10分钟上手Python/">10分钟上手Python</a>
</li>
    </ul>
  </li>
                        </ul>
                    </li>
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">文章 <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                            
<li >
    <a href="../../../文章/">Home</a>
</li>
                        </ul>
                    </li>
                    <li class="dropdown active">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">日常 <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                            
<li >
    <a href="../../">Home</a>
</li>
                            
  <li class="dropdown-submenu">
    <a href="#">test</a>
    <ul class="dropdown-menu">
            
<li class="active">
    <a href="./">test</a>
</li>
    </ul>
  </li>
                        </ul>
                    </li>
                </ul>

            <ul class="nav navbar-nav navbar-right">
                <li>
                    <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                        <i class="fa fa-search"></i> Search
                    </a>
                </li>
                    <li >
                        <a rel="next" href="../../">
                            <i class="fa fa-arrow-left"></i> Previous
                        </a>
                    </li>
                    <li class="disabled">
                        <a rel="prev" >
                            Next <i class="fa fa-arrow-right"></i>
                        </a>
                    </li>
                    <li>
                        <a href="https://github.com/1007530194/Diary">1007530194/Diary</a>
                    </li>
            </ul>
        </div>
    </div>
</div>

        <div class="container">
                <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
        <li class="main active"><a href="#learning">Learning</a></li>
            <li><a href="#gradient_checks">Gradient Checks</a></li>
            <li><a href="#before_learning_sanity_checks_tipstricks">Before learning: sanity checks Tips/Tricks</a></li>
            <li><a href="#babysitting_the_learning_process">Babysitting the learning process</a></li>
            <li><a href="#parameter_updates">Parameter updates</a></li>
    </ul>
</div></div>
                <div class="col-md-9" role="main">

<h2 id="learning">Learning<a class="headerlink" href="#learning" title="Permanent link">&para;</a></h2>
<p>In the previous sections we've discussed the static parts of a Neural Networks: how we can set up the network connectivity, the data, and the loss function. This section is devoted to the dynamics, or in other words, the process of learning the parameters and finding good hyperparameters.</p>
<p><a name='gradcheck'></a></p>
<h3 id="gradient_checks">Gradient Checks<a class="headerlink" href="#gradient_checks" title="Permanent link">&para;</a></h3>
<p>In theory, performing a gradient check is as simple as comparing the analytic gradient to the numerical gradient. In practice, the process is much more involved and error prone. Here are some tips, tricks, and issues to watch out for:</p>
<p><strong>Use the centered formula</strong>. The formula you may have seen for the finite difference approximation when evaluating the numerical gradient looks as follows:</p>
<div>
<div class="MathJax_Preview">
\frac{df(x)}{dx} = \frac{f(x + h) - f(x)}{h} \hspace{0.1in} \text{(bad, do not use)}
</div>
<script type="math/tex; mode=display">
\frac{df(x)}{dx} = \frac{f(x + h) - f(x)}{h} \hspace{0.1in} \text{(bad, do not use)}
</script>
</div>
<p>where <span><span class="MathJax_Preview">h</span><script type="math/tex">h</script></span> is a very small number, in practice approximately 1e-5 or so. In practice, it turns out that it is much better to use the <em>centered</em> difference formula of the form:</p>
<div>
<div class="MathJax_Preview">
\frac{df(x)}{dx} = \frac{f(x + h) - f(x - h)}{2h} \hspace{0.1in} \text{(use instead)}
</div>
<script type="math/tex; mode=display">
\frac{df(x)}{dx} = \frac{f(x + h) - f(x - h)}{2h} \hspace{0.1in} \text{(use instead)}
</script>
</div>
<p>This requires you to evaluate the loss function twice to check every single dimension of the gradient (so it is about 2 times as expensive), but the gradient approximation turns out to be much more precise. To see this, you can use Taylor expansion of <span><span class="MathJax_Preview">f(x+h)</span><script type="math/tex">f(x+h)</script></span> and <span><span class="MathJax_Preview">f(x-h)</span><script type="math/tex">f(x-h)</script></span> and verify that the first formula has an error on order of <span><span class="MathJax_Preview">O(h)</span><script type="math/tex">O(h)</script></span>, while the second formula only has error terms on order of <span><span class="MathJax_Preview">O(h^2)</span><script type="math/tex">O(h^2)</script></span> (i.e. it is a second order approximation).</p>
<p><strong>Use relative error for the comparison</strong>. What are the details of comparing the numerical gradient <span><span class="MathJax_Preview">f'_n</span><script type="math/tex">f'_n</script></span> and analytic gradient <span><span class="MathJax_Preview">f'_a</span><script type="math/tex">f'_a</script></span>? That is, how do we know if the two are not compatible? You might be temped to keep track of the difference $\mid f'_a - f'_n \mid $ or its square and define the gradient check as failed if that difference is above a threshold. However, this is problematic. For example, consider the case where their difference is 1e-4. This seems like a very appropriate difference if the two gradients are about 1.0, so we'd consider the two gradients to match. But if the gradients were both on order of 1e-5 or lower, then we'd consider 1e-4 to be a huge difference and likely a failure. Hence, it is always more appropriate to consider the <em>relative error</em>:</p>
<div>
<div class="MathJax_Preview">
\frac{\mid f'_a - f'_n \mid}{\max(\mid f'_a \mid, \mid f'_n \mid)}
</div>
<script type="math/tex; mode=display">
\frac{\mid f'_a - f'_n \mid}{\max(\mid f'_a \mid, \mid f'_n \mid)}
</script>
</div>
<p>which considers their ratio of the differences to the ratio of the absolute values of both gradients. Notice that normally the relative error formula only includes one of the two terms (either one), but I prefer to max (or add) both to make it symmetric and to prevent dividing by zero in the case where one of the two is zero (which can often happen, especially with ReLUs). However, one must explicitly keep track of the case where both are zero and pass the gradient check in that edge case. In practice:</p>
<ul>
<li>relative error &gt; 1e-2 usually means the gradient is probably wrong</li>
<li>1e-2 &gt; relative error &gt; 1e-4 should make you feel uncomfortable</li>
<li>1e-4 &gt; relative error is usually okay for objectives with kinks. But if there are no kinks (e.g. use of tanh nonlinearities and softmax), then 1e-4 is too high.</li>
<li>1e-7 and less you should be happy.</li>
</ul>
<p>Also keep in mind that the deeper the network, the higher the relative errors will be. So if you are gradient checking the input data for a 10-layer network, a relative error of 1e-2 might be okay because the errors build up on the way. Conversely, an error of 1e-2 for a single differentiable function likely indicates incorrect gradient.</p>
<p><strong>Use double precision</strong>. A common pitfall is using single precision floating point to compute gradient check. It is often that case that you might get high relative errors (as high as 1e-2) even with a correct gradient implementation. In my experience I've sometimes seen my relative errors plummet from 1e-2 to 1e-8 by switching to double precision.</p>
<p><strong>Stick around active range of floating point</strong>. It's a good idea to read through <a href="http://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html">"What Every Computer Scientist Should Know About Floating-Point Arithmetic"</a>, as it may demystify your errors and enable you to write more careful code. For example, in neural nets it can be common to normalize the loss function over the batch. However, if your gradients per datapoint are very small, then <em>additionally</em> dividing them by the number of data points is starting to give very small numbers, which in turn will lead to more numerical issues. This is why I like to always print the raw numerical/analytic gradient, and make sure that the numbers you are comparing are not extremely small (e.g. roughly 1e-10 and smaller in absolute value is worrying). If they are you may want to temporarily scale your loss function up by a constant to bring them to a "nicer" range where floats are more dense - ideally on the order of 1.0, where your float exponent is 0.</p>
<p><strong>Kinks in the objective</strong>. One source of inaccuracy to be aware of during gradient checking is the problem of <em>kinks</em>. Kinks refer to non-differentiable parts of an objective function, introduced by functions such as ReLU (<span><span class="MathJax_Preview">max(0,x)</span><script type="math/tex">max(0,x)</script></span>), or the SVM loss, Maxout neurons, etc. Consider gradient checking the ReLU function at <span><span class="MathJax_Preview">x = -1e6</span><script type="math/tex">x = -1e6</script></span>. Since <span><span class="MathJax_Preview">x &lt; 0</span><script type="math/tex">x < 0</script></span>, the analytic gradient at this point is exactly zero. However, the numerical gradient would suddenly compute a non-zero gradient because <span><span class="MathJax_Preview">f(x+h)</span><script type="math/tex">f(x+h)</script></span> might cross over the kink (e.g. if <span><span class="MathJax_Preview">h &gt; 1e-6</span><script type="math/tex">h > 1e-6</script></span>) and introduce a non-zero contribution. You might think that this is a pathological case, but in fact this case can be very common. For example, an SVM for CIFAR-10 contains up to 450,000 <span><span class="MathJax_Preview">max(0,x)</span><script type="math/tex">max(0,x)</script></span> terms because there are 50,000 examples and each example yields 9 terms to the objective. Moreover, a Neural Network with an SVM classifier will contain many more kinks due to ReLUs.</p>
<p>Note that it is possible to know if a kink was crossed in the evaluation of the loss. This can be done by keeping track of the identities of all "winners" in a function of form <span><span class="MathJax_Preview">max(x,y)</span><script type="math/tex">max(x,y)</script></span>; That is, was x or y higher during the forward pass. If the identity of at least one winner changes when evaluating <span><span class="MathJax_Preview">f(x+h)</span><script type="math/tex">f(x+h)</script></span> and then <span><span class="MathJax_Preview">f(x-h)</span><script type="math/tex">f(x-h)</script></span>, then a kink was crossed and the numerical gradient will not be exact.</p>
<p><strong>Use only few datapoints</strong>. One fix to the above problem of kinks is to use fewer datapoints, since loss functions that contain kinks (e.g. due to use of ReLUs or margin losses etc.) will have fewer kinks with fewer datapoints, so it is less likely for you to cross one when you perform the finite different approximation. Moreover, if your gradcheck for only ~2 or 3 datapoints then you would almost certainly gradcheck for an entire batch. Using very few datapoints also makes your gradient check faster and more efficient.</p>
<p><strong>Be careful with the step size h</strong>. It is not necessarily the case that smaller is better, because when <span><span class="MathJax_Preview">h</span><script type="math/tex">h</script></span> is much smaller, you may start running into numerical precision problems. Sometimes when the gradient doesn't check, it is possible that you change <span><span class="MathJax_Preview">h</span><script type="math/tex">h</script></span> to be 1e-4 or 1e-6 and suddenly the gradient will be correct. This <a href="http://en.wikipedia.org/wiki/Numerical_differentiation">wikipedia article</a> contains a chart that plots the value of <strong>h</strong> on the x-axis and the numerical gradient error on the y-axis.</p>
<p><strong>Gradcheck during a "characteristic" mode of operation</strong>. It is important to realize that a gradient check is performed at a particular (and usually random), single point in the space of parameters. Even if the gradient check succeeds at that point, it is not immediately certain that the gradient is correctly implemented globally. Additionally, a random initialization might not be the most "characteristic" point in the space of parameters and may in fact introduce pathological situations where the gradient seems to be correctly implemented but isn't. For instance, an SVM with very small weight initialization will assign almost exactly zero scores to all datapoints and the gradients will exhibit a particular pattern across all datapoints. An incorrect implementation of the gradient could still produce this pattern and not generalize to a more characteristic mode of operation where some scores are larger than others. Therefore, to be safe it is best to use a short <strong>burn-in</strong> time during which the network is allowed to learn and perform the gradient check after the loss starts to go down. The danger of performing it at the first iteration is that this could introduce pathological edge cases and mask an incorrect implementation of the gradient.</p>
<p><strong>Don't let the regularization overwhelm the data</strong>. It is often the case that a loss function is a sum of the data loss and the regularization loss (e.g. L2 penalty on weights). One danger to be aware of is that the regularization loss may overwhelm the data loss, in which case the gradients will be primarily coming from the regularization term (which usually has a much simpler gradient expression). This can mask an incorrect implementation of the data loss gradient. Therefore, it is recommended to turn off regularization and check the data loss alone first, and then the regularization term second and independently. One way to perform the latter is to hack the code to remove the data loss contribution. Another way is to increase the regularization strength so as to ensure that its effect is non-negligible in the gradient check, and that an incorrect implementation would be spotted.</p>
<p><strong>Remember to turn off dropout/augmentations</strong>. When performing gradient check, remember to turn off any non-deterministic effects in the network, such as dropout, random data augmentations, etc. Otherwise these can clearly introduce huge errors when estimating the numerical gradient. The downside of turning off these effects is that you wouldn't be gradient checking them (e.g. it might be that dropout isn't backpropagated correctly). Therefore, a better solution might be to force a particular random seed before evaluating both <span><span class="MathJax_Preview">f(x+h)</span><script type="math/tex">f(x+h)</script></span> and <span><span class="MathJax_Preview">f(x-h)</span><script type="math/tex">f(x-h)</script></span>, and when evaluating the analytic gradient.</p>
<p><strong>Check only few dimensions</strong>. In practice the gradients can have sizes of million parameters. In these cases it is only practical to check some of the dimensions of the gradient and assume that the others are correct. <strong>Be careful</strong>: One issue to be careful with is to make sure to gradient check a few dimensions for every separate parameter. In some applications, people combine the parameters into a single large parameter vector for convenience. In these cases, for example, the biases could only take up a tiny number of parameters from the whole vector, so it is important to not sample at random but to take this into account and check that all parameters receive the correct gradients.</p>
<p><a name='sanitycheck'></a></p>
<h3 id="before_learning_sanity_checks_tipstricks">Before learning: sanity checks Tips/Tricks<a class="headerlink" href="#before_learning_sanity_checks_tipstricks" title="Permanent link">&para;</a></h3>
<p>Here are a few sanity checks you might consider running before you plunge into expensive optimization:</p>
<ul>
<li><strong>Look for correct loss at chance performance.</strong> Make sure you're getting the loss you expect when you initialize with small parameters. It's best to first check the data loss alone (so set regularization strength to zero). For example, for CIFAR-10 with a Softmax classifier we would expect the initial loss to be 2.302, because we expect a diffuse probability of 0.1 for each class (since there are 10 classes), and Softmax loss is the negative log probability of the correct class so: -ln(0.1) = 2.302. For The Weston Watkins SVM, we expect all desired margins to be violated (since all scores are approximately zero), and hence expect a loss of 9 (since margin is 1 for each wrong class). If you're not seeing these losses there might be issue with initialization.</li>
<li>As a second sanity check, increasing the regularization strength should increase the loss</li>
<li><strong>Overfit a tiny subset of data</strong>. Lastly and most importantly, before training on the full dataset try to train on a tiny portion (e.g. 20 examples) of your data and make sure you can achieve zero cost. For this experiment it's also best to set regularization to zero, otherwise this can prevent you from getting zero cost. Unless you pass this sanity check with a small dataset it is not worth proceeding to the full dataset. Note that it may happen that you can overfit very small dataset but still have an incorrect implementation. For instance, if your datapoints' features are random due to some bug, then it will be possible to overfit your small training set but you will never notice any generalization when you fold it your full dataset.</li>
</ul>
<p><a name='baby'></a></p>
<h3 id="babysitting_the_learning_process">Babysitting the learning process<a class="headerlink" href="#babysitting_the_learning_process" title="Permanent link">&para;</a></h3>
<p>There are multiple useful quantities you should monitor during training of a neural network. These plots are the window into the training process and should be utilized to get intuitions about different hyperparameter settings and how they should be changed for more efficient learning. </p>
<p>The x-axis of the plots below are always in units of epochs, which measure how many times every example has been seen during training in expectation (e.g. one epoch means that every example has been seen once). It is preferable to track epochs rather than iterations since the number of iterations depends on the arbitrary setting of batch size.</p>
<p><a name='loss'></a></p>
<h4 id="loss_function">Loss function<a class="headerlink" href="#loss_function" title="Permanent link">&para;</a></h4>
<p>The first quantity that is useful to track during training is the loss, as it is evaluated on the individual batches during the forward pass. Below is a cartoon diagram showing the loss over time, and especially what the shape might tell you about the learning rate:</p>
<div class="fig figcenter fighighlight">
  <img src="/Dairy/assets/images/cs231n/nn3/learningrates.jpeg" width="49%">
  <img src="/Dairy/assets/images/cs231n/nn3/loss.jpeg" width="49%">
  <div class="figcaption">
    <b>Left:</b> A cartoon depicting the effects of different learning rates. With low learning rates the improvements will be linear. With high learning rates they will start to look more exponential. Higher learning rates will decay the loss faster, but they get stuck at worse values of loss (green line). This is because there is too much "energy" in the optimization and the parameters are bouncing around chaotically, unable to settle in a nice spot in the optimization landscape. <b>Right:</b> An example of a typical loss function over time, while training a small network on CIFAR-10 dataset. This loss function looks reasonable (it might indicate a slightly too small learning rate based on its speed of decay, but it's hard to say), and also indicates that the batch size might be a little too low (since the cost is a little too noisy).
  </div>
</div>

<p>The amount of "wiggle" in the loss is related to the batch size. When the batch size is 1, the wiggle will be relatively high. When the batch size is the full dataset, the wiggle will be minimal because every gradient update should be improving the loss function monotonically (unless the learning rate is set too high).</p>
<p>Some people prefer to plot their loss functions in the log domain. Since learning progress generally takes an exponential form shape, the plot appears as a slightly more interpretable straight line, rather than a hockey stick. Additionally, if multiple cross-validated models are plotted on the same loss graph, the differences between them become more apparent.</p>
<p>Sometimes loss functions can look funny <a href="http://lossfunctions.tumblr.com/">lossfunctions.tumblr.com</a>.</p>
<p><a name='accuracy'></a></p>
<h4 id="trainval_accuracy">Train/Val accuracy<a class="headerlink" href="#trainval_accuracy" title="Permanent link">&para;</a></h4>
<p>The second important quantity to track while training a classifier is the validation/training accuracy. This plot can give you valuable insights into the amount of overfitting in your model:</p>
<div class="fig figleft fighighlight">
  <img src="/Dairy/assets/images/cs231n/nn3/accuracies.jpeg">
  <div class="figcaption">
    The gap between the training and validation accuracy indicates the amount of overfitting. Two possible cases are shown in the diagram on the left. The blue validation error curve shows very small validation accuracy compared to the training accuracy, indicating strong overfitting (note, it's possible for the validation accuracy to even start to go down after some point). When you see this in practice you probably want to increase regularization (stronger L2 weight penalty, more dropout, etc.) or collect more data. The other possible case is when the validation accuracy tracks the training accuracy fairly well. This case indicates that your model capacity is not high enough: make the model larger by increasing the number of parameters.
  </div>
  <div style="clear:both"></div>
</div>

<p><a name='ratio'></a></p>
<h4 id="ratio_of_weightsupdates">Ratio of weights:updates<a class="headerlink" href="#ratio_of_weightsupdates" title="Permanent link">&para;</a></h4>
<p>The last quantity you might want to track is the ratio of the update magnitudes to the value magnitudes. Note: <em>updates</em>, not the raw gradients (e.g. in vanilla sgd this would be the gradient multiplied by the learning rate). You might want to evaluate and track this ratio for every set of parameters independently. A rough heuristic is that this ratio should be somewhere around 1e-3. If it is lower than this then the learning rate might be too low. If it is higher then the learning rate is likely too high. Here is a specific example:</p>
<div class="codehilite"><pre><span></span><span class="c1"># assume parameter vector W and its gradient vector dW</span>
<span class="n">param_scale</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span>
<span class="n">update</span> <span class="o">=</span> <span class="o">-</span><span class="n">learning_rate</span><span class="o">*</span><span class="n">dW</span> <span class="c1"># simple SGD update</span>
<span class="n">update_scale</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">update</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span>
<span class="n">W</span> <span class="o">+=</span> <span class="n">update</span> <span class="c1"># the actual update</span>
<span class="k">print</span> <span class="n">update_scale</span> <span class="o">/</span> <span class="n">param_scale</span> <span class="c1"># want ~1e-3</span>
</pre></div>

<p>Instead of tracking the min or the max, some people prefer to compute and track the norm of the gradients and their updates instead. These metrics are usually correlated and often give approximately the same results.</p>
<p><a name='distr'></a></p>
<h4 id="activation_gradient_distributions_per_layer">Activation / Gradient distributions per layer<a class="headerlink" href="#activation_gradient_distributions_per_layer" title="Permanent link">&para;</a></h4>
<p>An incorrect initialization can slow down or even completely stall the learning process. Luckily, this issue can be diagnosed relatively easily. One way to do so is to plot activation/gradient histograms for all layers of the network. Intuitively, it is not a good sign to see any strange distributions - e.g. with tanh neurons we would like to see a distribution of neuron activations between the full range of [-1,1], instead of seeing all neurons outputting zero, or all neurons being completely saturated at either -1 or 1.
<a name='vis'></a></p>
<h4 id="first-layer_visualizations">First-layer Visualizations<a class="headerlink" href="#first-layer_visualizations" title="Permanent link">&para;</a></h4>
<p>Lastly, when one is working with image pixels it can be helpful and satisfying to plot the first-layer features visually:</p>
<div class="fig figcenter fighighlight">
  <img src="/Dairy/assets/images/cs231n/nn3/weights.jpeg" width="43%" style="margin-right:10px;">
  <img src="/Dairy/assets/images/cs231n/nn3/cnnweights.jpg" width="49%">
  <div class="figcaption">
    Examples of visualized weights for the first layer of a neural network. <b>Left</b>: Noisy features indicate could be a symptom: Unconverged network, improperly set learning rate, very low weight regularization penalty. <b>Right:</b> Nice, smooth, clean and diverse features are a good indication that the training is proceeding well.
  </div>
</div>

<p><a name='update'></a></p>
<h3 id="parameter_updates">Parameter updates<a class="headerlink" href="#parameter_updates" title="Permanent link">&para;</a></h3>
<p>Once the analytic gradient is computed with backpropagation, the gradients are used to perform a parameter update. There are several approaches for performing the update, which we discuss next.</p>
<p>We note that optimization for deep networks is currently a very active area of research. In this section we highlight some established and common techniques you may see in practice, briefly describe their intuition, but leave a detailed analysis outside of the scope of the class. We provide some further pointers for an interested reader.</p>
<p><a name='sgd'></a></p>
<p>GD and bells and whistles</p>
<p><strong>Vanilla update</strong>. The simplest form of update is to change the parameters along the negative gradient direction (since the gradient indicates the direction of increase, but we usually wish to minimize a loss function). Assuming a vector of parameters <code>x</code> and the gradient <code>dx</code>, the simplest update has the form:</p></div>
        </div>

        <footer class="col-md-12">
            <hr>
                <p>Copyright &copy; 2016 - 2018 Martin Donath</p>
            <p>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>var base_url = '../../..';</script>
        <script src="../../../js/base.js"></script>
        <script src="../../../javascripts/extra.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
        <script src="../../../search/require.js"></script>
        <script src="../../../search/search.js"></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="Search Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Search</h4>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form role="form">
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="Keyboard Shortcuts Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Keyboard Shortcuts</h4>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td><kbd>&larr;</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td><kbd>&rarr;</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>


    </body>
</html>
